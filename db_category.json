[
    {
        "title": "Towards a General Framework for ML-based Self-tuning Databases",
        "authors": "['Thomas Schmied', 'Diego Didona', 'Andreas Döring', 'Thomas Parnell', 'Nikolas Ioannou']",
        "date": "April 2021",
        "source": "EuroMLSys '21: Proceedings of the 1st Workshop on Machine Learning and Systems",
        "abstract": "Machine learning (ML) methods have recently emerged as an effective way to perform automated parameter tuning of databases. State-of-the-art approaches include Bayesian optimization (BO) and reinforcement learning (RL). In this work, we describe our experience when applying these methods to a database not yet studied in this context: FoundationDB. Firstly, we describe the challenges we faced, such as unknown valid ranges of configuration parameters and combinations of parameter values that result in invalid runs, and how we mitigated them. While these issues are typically overlooked, we argue that they are a crucial barrier to the adoption of ML self-tuning techniques in databases, and thus deserve more attention from the research community. Secondly, we present experimental results obtained when tuning FoundationDB using ML methods. Unlike prior work in this domain, we also compare with the simplest of baselines: random search. Our results show that, while BO and RL methods can improve the throughput of FoundationDB by up to 38%, random search is a highly competitive baseline, finding a configuration that is only 4% worse than the, vastly more complex, ML methods. We conclude that future work in this area may want to focus more on randomized, model-free optimization algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3437984.3458830",
        "category": "Databases"
    },
    {
        "title": "Determining a consistent experimental setup for benchmarking and optimizing databases",
        "authors": "['Moisés Silva-Muñoz', 'Gonzalo Calderon', 'Alberto Franzin', 'Hugues Bersini']",
        "date": "July 2021",
        "source": "GECCO '21: Proceedings of the Genetic and Evolutionary Computation Conference Companion",
        "abstract": "The evaluation of the performance of an IT system is a fundamental operation in its benchmarking and optimization. However, despite the general consensus on the importance of this task, little guidance is usually provided to practitioners who need to benchmark their IT system. In particular, many works in the area of database optimization do not provide an adequate amount of information on the setup used in their experiments and analyses. In this work we report an experimental procedure that, through a sequence of experiments, analyzes the impact of various choices in the design of a database benchmark, leading to the individuation of an experimental setup that balances the consistency of the results with the time needed to obtain them. We show that the minimal experimental setup we obtain is representative also of heavier scenarios, which make it possible for the results of optimization tasks to scale.",
        "link": "https://dl.acm.org/doi/10.1145/3449726.3463180",
        "category": "Databases"
    },
    {
        "title": "The Curse of Correlations for Robust Fingerprinting of Relational Databases",
        "authors": "['Tianxi Ji', 'Emre Yilmaz', 'Erman Ayday', 'Pan Li']",
        "date": "October 2021",
        "source": "RAID '21: Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses",
        "abstract": "Database fingerprinting have been widely adopted to prevent unauthorized sharing of data and identify the source of data leakages. Although existing schemes are robust against common attacks, like random bit flipping and subset attack, their robustness degrades significantly if attackers utilize the inherent correlations among database entries. In this paper, we first demonstrate the vulnerability of existing database fingerprinting schemes by identifying different correlation attacks: column-wise correlation attack, row-wise correlation attack, and the integration of them. To provide robust fingerprinting against the identified correlation attacks, we then develop mitigation techniques, which can work as post-processing steps for any off-the-shelf database fingerprinting schemes. The proposed mitigation techniques also preserve the utility of the fingerprinted database considering different utility metrics. We empirically investigate the impact of the identified correlation attacks and the performance of mitigation techniques using real-world relational databases. Our results show (i) high success rates of the identified correlation attacks against existing fingerprinting schemes (e.g., the integrated correlation attack can distort 64.8% fingerprint bits by just modifying 14.2% entries in a fingerprinted database), and (ii) high robustness of the proposed mitigation techniques (e.g., with the mitigation techniques, the integrated correlation attack can only distort 3% fingerprint bits).",
        "link": "https://dl.acm.org/doi/10.1145/3471621.3471853",
        "category": "Databases"
    },
    {
        "title": "Tuple-Independent Representations of Infinite Probabilistic Databases",
        "authors": "['Nofar Carmeli', 'Martin Grohe', 'Peter Lindner', 'Christoph Standke']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Probabilistic databases (PDBs) are probability spaces over database instances. They provide a framework for handling uncertainty in databases, as occurs due to data integration, noisy data, data from unreliable sources or randomized processes. Most of the existing theory literature investigated finite, tuple-independent PDBs (TI-PDBs) where the occurrences of tuples are independent events. Only recently, Grohe and Lindner (PODS '19) introduced independence assumptions for PDBs beyond the finite domain assumption. In the finite, a major argument for discussing the theoretical properties of TI-PDBs is that they can be used to represent any finite PDB via views. This is no longer the case once the number of tuples is countably infinite. In this paper, we systematically study the representability of infinite PDBs in terms of TI-PDBs and the related block-independent disjoint PDBs. The central question is which infinite PDBs are representable as first-order views over tuple-independent PDBs. We give a necessary condition for the representability of PDBs and provide a sufficient criterion for representability in terms of the probability distribution of a PDB. With various examples, we explore the limits of our criteria. We show that conditioning on first order properties yields no additional power in terms of expressivity. Finally, we discuss the relation between purely logical and arithmetic reasons for (non-)representability.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458315",
        "category": "Databases"
    },
    {
        "title": "Probabilistic Databases under Updates: Boolean Query Evaluation and Ranked Enumeration",
        "authors": "['Christoph Berkholz', 'Maximilian Merz']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "We consider tuple-independent probabilistic databases in a dynamic setting, where tuples can be inserted or deleted. In this context we are interested in efficient data structures for maintaining the query result of Boolean as well as non-Boolean queries. For Boolean queries, we show how the known lifted inference rules can be made dynamic, so that they support single-tuple updates with only a constant number of arithmetic operations. As a consequence, we obtain that the probability of every safe UCQ can be maintained with constant update time. For non-Boolean queries, our task is to enumerate all result tuples ranked by their probability. We develop lifted inference rules for non-Boolean queries, and, based on these rules, provide a dynamic data structure that allows both log-time updates and ranked enumeration with logarithmic delay. As an application, we identify a fragment of non-repeating conjunctive queries that supports log-time updates as well as log-delay ranked enumeration. This characterisation is tight under the OMv-conjecture.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458326",
        "category": "Databases"
    },
    {
        "title": "Subarray Skyline Query Processing in Array Databases",
        "authors": "['Dalsu Choi', 'Hyunsik Yoon', 'Yon Dohn Chung']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "With the generation of large-scale spatial data in various fields, array databases that represent space as an array have become one of the means of managing spatial data. Each cell in an array tends to interact with one another; therefore, instead of considering a single cell, considering a concept of subarray is required in some applications. In addition, each cell has several attribute values to indicate its features. Based on the two observations, we propose a new type of query, subarray skyline, that provides a way to find meaningful subarrays or filter less meaningful subarrays considering attributes. We also introduce an efficient query processing method, ReSKY, in centralized and distributed settings. Through extensive experiments using an array database and real datasets, we show that ReSKY has better performance than the existing techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468799",
        "category": "Databases"
    },
    {
        "title": "The Complexity of Counting Problems Over Incomplete Databases",
        "authors": "['Marcelo Arenas', 'Pablo Barceló', 'Mikaël Monet']",
        "date": "None",
        "source": "ACM Transactions on Computational Logic",
        "abstract": "We study the complexity of various fundamental counting problems that arise in the context of incomplete databases, i.e., relational databases that can contain unknown values in the form of labeled nulls. Specifically, we assume that the domains of these unknown values are finite and, for a Boolean query q, we consider the following two problems: Given as input an incomplete database D, (a) return the number of completions of D that satisfy q; or (b) return the number of valuations of the nulls of D yielding a completion that satisfies q. We obtain dichotomies between #P-hardness and polynomial-time computability for these problems when q is a self-join–free conjunctive query and study the impact on the complexity of the following two restrictions: (1) every null occurs at most once in D (what is called Codd tables); and (2) the domain of each null is the same. Roughly speaking, we show that counting completions is much harder than counting valuations: For instance, while the latter is always in #P, we prove that the former is not in #P under some widely believed theoretical complexity assumption. Moreover, we find that both (1) and (2) can reduce the complexity of our problems. We also study the approximability of these problems and show that, while counting valuations always has a fully polynomial-time randomized approximation scheme (FPRAS), in most cases counting completions does not. Finally, we consider more expressive query languages and situate our problems with respect to known complexity classes.",
        "link": "https://dl.acm.org/doi/10.1145/3461642",
        "category": "Databases"
    },
    {
        "title": "Properties of Inconsistency Measures for Databases",
        "authors": "['Ester Livshits', 'Rina Kochirgan', 'Segev Tsur', 'Ihab F. Ilyas', 'Benny Kimelfeld', 'Sudeepa Roy']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "How should we quantify the inconsistency of a database that violates integrity constraints? Proper measures are important for various tasks, such as progress indication and action prioritization in cleaning systems, and reliability estimation for new datasets. To choose an appropriate inconsistency measure, it is important to identify the desired properties in the application and understand which of these is guaranteed or at least expected in practice. For example, in some use cases, the inconsistency should reduce if constraints are eliminated; in others, it should be stable and avoid jitters and jumps in reaction to small changes in the database. We embark on a systematic investigation of properties for database inconsistency measures. We investigate a collection of basic measures that have been proposed in the past in both the Knowledge Representation and Database communities, analyze their theoretical properties, and empirically observe their behavior in an experimental study. We also demonstrate how the framework can lead to new inconsistency measures by introducing a new measure that, in contrast to the rest, satisfies all of the properties we consider and can be computed in polynomial time.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457310",
        "category": "Databases"
    },
    {
        "title": "Blockchains vs. Distributed Databases: Dichotomy and Fusion",
        "authors": "['Pingcheng Ruan', 'Tien Tuan Anh Dinh', 'Dumitrel Loghin', 'Meihui Zhang', 'Gang Chen', 'Qian Lin', 'Beng Chin Ooi']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Blockchain has come a long way - a system that was initially proposed specifically for cryptocurrencies is now being adapted and adopted as a general-purpose transactional system. As blockchain evolves into another data management system, the natural question is how it compares against distributed database systems. Existing works on this comparison focus on high-level properties, such as security and throughput. They stop short of showing how the underlying design choices contribute to the overall differences. Our work fills this important gap. We perform a twin study of blockchains and distributed database systems as two types of transactional systems. We propose a taxonomy that illustrates the dichotomy across four dimensions, namely replication, concurrency, storage, and sharding. Within each dimension, we discuss how the design choices are driven by two goals: security for blockchains, and performance for distributed databases. We conduct an extensive and in-depth performance analysis of two blockchains, namely Quorum and Hyperledger Fabric, and three distributed databases, namely CockroachDB, TiDB, and etcd. Our analysis exposes the impact of different design choices on the overall performance. Concisely, our work provides a principled framework for analyzing the emerging trend of blockchain-database fusion.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452789",
        "category": "Databases"
    },
    {
        "title": "Data Modeling and NoSQL Databases - A Systematic Mapping Review",
        "authors": "['Harley Vera-Olivera', 'Ruizhe Guo', 'Ruben Cruz Huacarpuma', 'Ana Paula Bernardi Da Silva', 'Ari Melo Mariano', 'Maristela Holanda']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "Modeling is one of the most important steps in developing a database. In traditional databases, the Entity Relationship (ER) and Unified Modeling Language (UML) models are widely used. But how are NoSQL databases being modeled? We performed a systematic mapping review to answer three research questions to identify and analyze the levels of representation, models used, and contexts where the modeling process occurred in the main categories of NoSQL databases. We found 54 primary studies where we identified that conceptual and logical levels received more attention than the physical level of representation. The UML, ER, and new notation based on ER and UML were adapted to model NoSQL databases, in the same way, formats such as JSON, XML, and XMI were used to generate schemas through the three levels of representation. New contexts such as benchmark, evaluations, migration, and schema generation were identified, as well as new features to be considered for modeling NoSQL databases, such as the number of records by entities, CRUD operations, and system requirements (availability, consistency, or scalability). Additionally, a coupling and co-citation analysis was carried out to identify relevant works and researchers.",
        "link": "https://dl.acm.org/doi/10.1145/3457608",
        "category": "Databases"
    },
    {
        "title": "NALMO: A Natural Language Interface for Moving Objects Databases",
        "authors": "['Xieyang Wang', 'Jianqiu Xu', 'Hua Lu']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "Moving objects databases (MODs) have been extensively studied due to their wide variety of applications including traffic management, tourist service and mobile commerce. However, queries in natural languages are still not supported in MODs. Since most users are not familiar with structured query languages, it is essentially important to bridge the gap between natural languages and the underlying MODs system commands. Motivated by this, we design a natural language interface for moving objects, named NALMO. In general, we use semantic parsing in combination with a location knowledge base and domain-specific rules to interpret natural language queries. We design a corpus of moving objects queries for model training, which is later used to determine the query type. Extracted entities from parsing are mapped through deterministic rules to perform query composition. NALMO is able to well translate moving objects queries into structured (executable) languages. We support four kinds of queries including time interval queries, range queries, nearest neighbor queries and trajectory similarity queries. We develop the system in a prototype system SECONDO and evaluate our approach using 240 natural language queries extracted from popular conference and journal papers in the domain of moving objects. Experimental results show that (i) NALMO achieves accuracy and precision 98.1 and 88.1, respectively, and (ii) the average time cost of translating a query is 1.47s.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470894",
        "category": "Databases"
    },
    {
        "title": "Rethink the Scan in MVCC Databases",
        "authors": "['Jongbin Kim', 'Kihwang Kim', 'Hyunsoo Cho', 'Jaeseon Yu', 'Sooyong Kang', 'Hyungsoo Jung']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "A scan is one of the fundamental operations in databases for retrieving tuples from tables, and research on access methods has been of importance to query optimization. However, our community is aware of the inconvenient truth that its performance may plummet amid steep increases in search costs when acting on MVCC databases since multi-versioning may forfeit all the benefits of using database indexes. An execution plan for a query on multi-versioned data often comprises a series of point lookup operations, of which each internally executes a linear traversal of record versions. Therefore, the generated plan is surprisingly worse than a full table (or version store) scan, mainly due to redundant access to database pages. To address such an all-or-nothing approach, we propose version weaver (vWeaver), a light-weight access method for record versions, that expedites a scan on record versions with each being augmented by just a few pointer fields. vWeaver incrementally constructs a version search structure over even an append-only version store (e.g., undo space) and allows a scan to traverse new version search structures for fast lookup. We applied vWeaver to in-memory and disk-based MVCC databases and demonstrated that the systems with vWeaver generally improved scan performance under various workloads with negligible space overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452783",
        "category": "Databases"
    },
    {
        "title": "Sound of databases: sonification of a semantic web database engine",
        "authors": "['Sven Groppe', 'Rico Klinckenberg', 'Benjamin Warnke']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Sonifications map data to auditory dimensions and offer a new audible experience to their listeners. We propose a sonification of query processing paired with a corresponding visualization both integrated in a web application. In this demonstration we show that the sonification of different types of relational operators generates different sound patterns, which can be recognized and identified by listeners increasing their understanding of the operators' functionality and supports easy remembering of requirements like merge joins work on sorted input. Furthermore, new ways of analyzing query processing are possible with the sonification approach.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476322",
        "category": "Databases"
    },
    {
        "title": "Machine learning for databases",
        "authors": "['Guoliang Li', 'Xuanhe Zhou', 'Lei Cao']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Machine learning techniques have been proposed to optimize the databases. For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, machine learning based techniques can alleviate this problem by judiciously selecting optimization strategy. In this tutorial, we categorize database tasks into three typical problems that can be optimized by different machine learning models, including NP-hard problems (e.g., knob space exploration, index/view selection, partition-key recommendation for offline optimization; query rewrite, join order selection for online optimization), regression problems (e.g., cost/cardinality estimation, index/view benefit estimation, query latency prediction), and prediction problems (e.g., query workload prediction). We review existing machine learning based techniques to address these problems and provide research challenges.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476405",
        "category": "Databases"
    },
    {
        "title": "Version Reconciliation for Collaborative Databases",
        "authors": "['Nalin Ranjan', 'Zechao Shang', 'Sanjay Krishnan', 'Aaron J. Elmore']",
        "date": "November 2021",
        "source": "SoCC '21: Proceedings of the ACM Symposium on Cloud Computing",
        "abstract": "We propose MindPalace, a prototype of a versioned database for efficient collaborative data management. MindPalace supports offline collaboration, where users work independently without real-time correspondence. The core of MindPalace is a critical step of offline collaboration: reconciling divergent branches made by simultaneous data manipulation. We formalize the concept of auto-mergeability, a condition under which branches may be reconciled without human intervention, and propose an efficient framework for determining whether two branches are auto-mergeable and identifying particular records for manual reconciliation.",
        "link": "https://dl.acm.org/doi/10.1145/3472883.3486980",
        "category": "Databases"
    },
    {
        "title": "Enabling personal consent in databases",
        "authors": "['George Konstantinidis', 'Jet Holt', 'Adriane Chapman']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Users have the right to consent to the use of their data, but current methods are limited to very coarse-grained expressions of consent, as \"opt-in/opt-out\" choices for certain uses. In this paper we identify the need for fine-grained consent management and formalize how to express and manage user consent and personal contracts of data usage in relational databases. Unlike privacy approaches, our focus is not on preserving confidentiality against an adversary, but rather cooperate with a trusted service provider to abide by user preferences in an algorithmic way. Our approach enables data owners to express the intended data usage in formal specifications, that we call consent constraints, and enables a service provider that wants to honor these constraints, to automatically do so by filtering query results that violate consent; rather than both sides relying on \"terms of use\" agreements written in natural language. We provide formal foundations (based on provenance), algorithms (based on unification and query rewriting), connections to data privacy, and complexity results for supporting consent in databases. We implement our framework in an open source RDBMS, and provide an evaluation against the most relevant privacy approach using the TPC-H benchmark, and on a real dataset of ICU data.",
        "link": "https://dl.acm.org/doi/10.14778/3489496.3489516",
        "category": "Databases"
    },
    {
        "title": "Query Games in Databases",
        "authors": "['Ester Livshits', 'Leopoldo Bertossi', 'Benny Kimelfeld', 'Moshe Sebag']",
        "date": "March 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Database tuples can be seen as players in the game of jointly realizing the answer to a query. Some tuples may contribute more than others to the outcome, which can be a binary value in the case of a Boolean query, a number for a numerical aggregate query, and so on. To quantify the contributions of tuples, we use the Shapley value that was introduced in cooperative game theory and has found applications in a plethora of domains. Specifically, the Shapley value of an individual tuple quantifies its contribution to the query. We investigate the applicability of the Shapley value in this setting, as well as the computational aspects of its calculation in terms of complexity, algorithms, and approximation.",
        "link": "https://dl.acm.org/doi/10.1145/3471485.3471504",
        "category": "Databases"
    },
    {
        "title": "Retrofitting GDPR compliance onto legacy databases",
        "authors": "['Archita Agarwal', 'Marilyn George', 'Aaron Jeyaraj', 'Malte Schwarzkopf']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "New privacy laws like the European Union's General Data Protection Regulation (GDPR) require database administrators (DBAs) to identify all information related to an individual on request, e.g., to return or delete it. This requires time-consuming manual labor today, particularly for legacy schemas and applications.In this paper, we investigate what it takes to provide mostly-automated tools that assist DBAs in GDPR-compliant data extraction for legacy databases. We find that a combination of techniques is needed to realize a tool that works for the databases of real-world applications, such as web applications, which may violate strict normal forms or encode data relationships in bespoke ways. Our tool, GDPRizer, relies on foreign keys, query logs that identify implied relationships, data-driven methods, and coarse-grained annotations provided by the DBA to extract an individual's data.In a case study with three popular web applications, GDPRizer achieves 100% precision and 96--100% recall. GDPRizer saves work compared to hand-written queries, and while manual verification of its outputs is required, GDPRizer simplifies privacy compliance.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503603",
        "category": "Databases"
    },
    {
        "title": "DP-Sync: Hiding Update Patterns in Secure Outsourced Databases with Differential Privacy",
        "authors": "['Chenghong Wang', 'Johes Bater', 'Kartik Nayak', 'Ashwin Machanavajjhala']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In this paper, we consider privacy-preserving update strategies for secure outsourced growing databases. Such databases allow appendonly data updates on the outsourced data structure while analysis is ongoing. Despite a plethora of solutions to securely outsource database computation, existing techniques do not consider the information that can be leaked via update patterns. To address this problem, we design a novel secure outsourced database framework for growing data, DP-Sync, which interoperate with a large class of existing encrypted databases and supports efficient updates while providing differentially-private guarantees for any single update. We demonstrate DP-Sync's practical feasibility in terms of performance and accuracy with extensive empirical evaluations on real world datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457306",
        "category": "Databases"
    },
    {
        "title": "Efficient Discovery of Functional Dependencies from Incremental Databases",
        "authors": "['Loredana Caruccio', 'Stefano Cirillo', 'Vincenzo Deufemia', 'Giuseppe Polese']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "With the advent of Big Data there is an increasing necessity to incrementally mine information from data originating from sensors and other dynamic sources. Thus, it is necessary to devise algorithms capable of mining useful information upon possible evolutions of databases. Among these, there are certainly data profiling info, such as functional dependencies (fd for short), which are particularly useful for data integration and for assessing the quality of data. The incremental scenario requires the definition of search strategies and validation methods able to analyze only the portion of the dataset affected by the last changes. In this paper, we propose a new validation method, which exploits regular expressions and compressed data structures to efficiently verify whether a candidate fd holds on an updated version of the dataset. Experimental results demonstrate the effectiveness of the proposed method on real-world datasets adapted for incremental scenarios, also compared with a baseline incremental fd discovery algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487719",
        "category": "Databases"
    },
    {
        "title": "COMPASS: Online Sketch-based Query Optimization for In-Memory Databases",
        "authors": "['Yesdaulet Izenov', 'Asoke Datta', 'Florin Rusu', 'Jun Hyung Shin']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Cost-based query optimization remains a critical task in relational databases even after decades of research and industrial development. Query optimizers rely on a large range of statistical synopses for accurate cardinality estimation. As the complexity of selections and the number of join predicates increase, two problems arise. First, statistics cannot be incrementally composed to effectively estimate the cost of the sub-plans generated in plan enumeration. Second, small errors are propagated exponentially through joins, which can lead to severely sub-optimal plans. In this paper, we introduce COMPASS, a novel query optimization paradigm for in-memory databases based on a single type of statistics---Fast-AGMS sketches. In COMPASS, query optimization and execution are intertwined. Selection predicates and sketch updates are pushed-down and evaluated online during query optimization. This allows Fast-AGMS sketches to be computed only over the relevant tuples---which enhances cardinality estimation accuracy. Plan enumeration is performed over the query join graph by incrementally composing attribute-level sketches---not by building a separate sketch for every sub-plan. We prototype COMPASS in MapD -- an open-source parallel database -- and perform extensive experiments over the complete JOB benchmark. The results prove that COMPASS generates better execution plans -- both in terms of cardinality and runtime -- compared to four other database systems. Overall, COMPASS achieves a speedup ranging from 1.35X to 11.28X in cumulative query execution time over the considered competitors.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452840",
        "category": "Databases"
    },
    {
        "title": "εpsolute: Efficiently Querying Databases While Providing Differential Privacy",
        "authors": "['Dmytro Bogatov', 'Georgios Kellaris', 'George Kollios', 'Kobbi Nissim', \"Adam O'Neill\"]",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "As organizations struggle with processing vast amounts of information, outsourcing sensitive data to third parties becomes a necessity. To protect the data, various cryptographic techniques are used in outsourced database systems to ensure data privacy, while allowing efficient querying. A rich collection of attacks on such systems has emerged. Even with strong cryptography, just communication volume or access pattern is enough for an adversary to succeed. In this work we present a model for differentially private outsourced database system and a concrete construction, εpsolute, that provably conceals the aforementioned leakages, while remaining efficient and scalable. In our solution, differential privacy is preserved at the record level even against an untrusted server that controls data and queries. εpsolute combines Oblivious RAM and differentially private sanitizers to create a generic and efficient construction. We go further and present a set of improvements to bring the solution to efficiency and practicality necessary for real-world adoption. We describe the way to parallelize the operations, minimize the amount of noise, and reduce the number of network requests, while preserving the privacy guarantees. We have run an extensive set of experiments, dozens of servers processing up to 10 million records, and compiled a detailed result analysis proving the efficiency and scalability of our solution. While providing strong security and privacy guarantees we are less than an order of magnitude slower than range query execution of a non-secure plain-text optimized RDBMS like MySQL and PostgreSQL.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484786",
        "category": "Databases"
    },
    {
        "title": "PRISM: Private Verifiable Set Computation over Multi-Owner Outsourced Databases",
        "authors": "['Yin Li', 'Dhrubajyoti Ghosh', 'Peeyush Gupta', 'Sharad Mehrotra', 'Nisha Panwar', 'Shantanu Sharma']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "This paper proposes Prism, a secret sharing based approach to compute private set operations (i.e., intersection and union), as well as aggregates over outsourced databases belonging to multiple owners. Prism enables data owners to pre-load the data onto non-colluding servers and exploits the additive and multiplicative properties of secret-shares to compute the above-listed operations in (at most) two rounds of communication between the servers (storing the secret-shares) and the querier, resulting in a very efficient implementation. Also, Prism does not require communication among the servers and supports result verification techniques for each operation to detect malicious adversaries. Experimental results show that Prism scales both in terms of the number of data owners and database sizes, to which prior approaches do not scale.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452839",
        "category": "Databases"
    },
    {
        "title": "Exploring Ratings in Subjective Databases",
        "authors": "['Sihem Amer-Yahia', 'Tova Milo', 'Brit Youngmann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Subjective data links people to content items and reflects who likes or dislikes what. The valuable information this data contains is virtually infinite and satisfies various information needs. Yet, as of today, dedicated tools to explore this data are lacking. In this paper, we develop a framework for Subjective Data Exploration (SDE). Our solution enables the joint exploration of items, people, and people's opinions on items, in a guided multi-step process where each step aggregates the most useful and diverse trends in the form of rating maps. Because of the large search space of possible rating maps, we leverage pruning strategies based on confidence intervals and multi-armed bandits. Our large-scale experiments with human subjects and real datasets, demonstrate the need for dedicated SDE frameworks and the effectiveness and efficiency of our approach.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457259",
        "category": "Databases"
    },
    {
        "title": "ResTune: Resource Oriented Tuning Boosted by Meta-Learning for Cloud Databases",
        "authors": "['Xinyi Zhang', 'Hong Wu', 'Zhuo Chang', 'Shuowei Jin', 'Jian Tan', 'Feifei Li', 'Tieying Zhang', 'Bin Cui']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Modern database management systems (DBMS) contain tens to hundreds of critical performance tuning knobs that determine the system runtime behaviors. To reduce the total cost of ownership, cloud database providers put in drastic effort to automatically optimize the resource utilization by tuning these knobs. There are two challenges. First, the tuning system should always abide by the service level agreement (SLA) while optimizing the resource utilization, which imposes strict constrains on the tuning process. Second, the tuning time should be reasonably acceptable since time-consuming tuning is not practical for production and online troubleshooting. In this paper, we design ResTune to automatically optimize the resource utilization without violating SLA constraints on the throughput and latency requirements. ResTune leverages the tuning experience from the history tasks and transfers the accumulated knowledge to accelerate the tuning process of the new tasks. The prior knowledge is represented from historical tuning tasks through an ensemble model. The model learns the similarity between the historical workloads and the target, which significantly reduces the tuning time by a meta-learning based approach. ResTune can efficiently handle different workloads and various hardware environments. We perform evaluations using benchmarks and real world workloads on different types of resources. The results show that, compared with the manually tuned configurations, ResTune reduces 65%, 87%, 39% of CPU utilization, I/O and memory on average, respectively. Compared with the state-of-the-art methods, ResTune finds better configurations with up to ~18x speedups.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457291",
        "category": "Databases"
    },
    {
        "title": "Similarity Association Pattern Mining in Transaction Databases",
        "authors": "['Phridviraj M.S.B', 'Guru Rao C.V', 'Radhakrishna Vangipuram', 'Aravind Cheruvu']",
        "date": "April 2021",
        "source": "DATA'21: International Conference on Data Science, E-learning and Information Systems 2021",
        "abstract": "Association pattern mining is a method of finding interesting relationships or patterns between item sets present in each of the transactions of the transactional databases. Current researchers in this area are focusing on the data mining task of finding frequent patterns among the item sets based on the interestingness measures like the support and confidence which is called as Frequent pattern mining. Till date, in existing frequent pattern mining algorithms, an itemset is said to be frequent if the support of the itemset satisfies the minimum support input. In this paper, the objective of our algorithm is to find interesting patterns among the item sets based on a Gaussian similarity for an input reference threshold which is first of its kind in the research literature. This study is limited to outlining naïve approach of mining frequent itemsets which requires validating every itemset to verify if the itemset is frequent or not.",
        "link": "https://dl.acm.org/doi/10.1145/3460620.3460752",
        "category": "Databases"
    },
    {
        "title": "Identification of Corresponding Numerical Attributes in Heterogeneous Databases Based on Instances",
        "authors": "['Kenji Nozaki', 'Teruhisa Hochin', 'Hiroki Nomiya']",
        "date": "June 2021",
        "source": "ACIT '21: Proceedings of the the 8th International Virtual Conference on Applied Computing &amp; Information Technology",
        "abstract": "Identification of attributes in heterogeneous databases is widely known as the schema matching problem, and many studies have been published. Although existing studies using schema information have been proposed, schema information cannot always be used. In such a case, a schema matching method using instances is used as an alternative method, which is called instance-based schema matching. In this paper, we propose an instance-based schema matching method for the attributes storing numerical data. It uses data distributions and correlations between two attributes. The proposed method is experimentally evaluated by using several databases. As a result, the proposed method could find appropriate attribute correspondences.",
        "link": "https://dl.acm.org/doi/10.1145/3468081.3471083",
        "category": "Databases"
    },
    {
        "title": "ReStore - Neural Data Completion for Relational Databases",
        "authors": "['Benjamin Hilprecht', 'Carsten Binnig']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Classical approaches for OLAP assume that the data of all tables is complete. However, in case of incomplete tables with missing tuples, classical approaches fail since the result of a SQL aggregate query might significantly differ from the results computed on the full dataset. Today, the only way to deal with missing data is to manually complete the dataset which causes not only high efforts but also requires good statistical skills to determine when a dataset is actually complete. In this paper, we propose an automated approach for relational data completion called ReStore using a new class of (neural) schema-structured completion models that are able to synthesize data which resembles the missing tuples. As we show in our evaluation, this efficiently helps to reduce the relative error of aggregate queries by up to 390% on real-world data compared to using the incomplete data directly for query answering.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457264",
        "category": "Databases"
    },
    {
        "title": "IrisIndexNet: Indexing on Iris Databases for Faster Identification",
        "authors": "['Geetika Arora', 'Shantanu Vichare', 'Kamlesh Tiwari']",
        "date": "January 2022",
        "source": "CODS-COMAD 2022: 5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)",
        "abstract": "Iris is one of the most accurate biometric traits for human authentication due to high reliability, uniqueness, and stability. Identification of a query iris sample requires its comparison with all the templates stored in the database. The process is computationally expensive and becomes inefficient as the number of templates increase in the database. The process can be fastened by reducing the search space through indexing the database. This paper proposes IrisIndexNet, a novel indexing technique that effectively reduces identification search space for the iris database. It design a specialized convolutional neural network architecture and train as a Siamese network to construct a compact feature vectors having low inter-class and high intra-class similarity in the latent space representation for the iris images. Features are subsequently clustered using k-means and agglomerative clustering to generate an index table. During retrieval, the feature of the query image is extracted using the trained network, and it is matched with all the indices of the index table. Feature vectors in neighborhood of most similar index is the candidate set for matching. Obtained candidate set is of fixed size and small as compared to the original database thereby making the identification a constant time operation. Experiments are conducted on widely used publically used iris databases viz. CASIA Interval and CASIA Lamp. The proposed technique achieved a hit rate of 99% at a penetration rate of 2.254% and 0.008% on the respective databases. A speedup of 4 and 27 times is achieved when the CASIA Interval and CASIA Lamp are indexed using the proposed technique as compared to the naive approach for identification.",
        "link": "https://dl.acm.org/doi/10.1145/3493700.3493715",
        "category": "Databases"
    },
    {
        "title": "ATJ-Net: Auto-Table-Join Network for Automatic Learning on Relational Databases",
        "authors": "['Jinze Bai', 'Jialin Wang', 'Zhao Li', 'Donghui Ding', 'Ji Zhang', 'Jun Gao']",
        "date": "April 2021",
        "source": "WWW '21: Proceedings of the Web Conference 2021",
        "abstract": "A relational database, consisting of multiple tables, provides heterogeneous information across various entities, widely used in real-world services. This paper studies the supervised learning task on multiple tables, aiming to predict one label column with the help of multiple-tabular data. However, classical ML techniques mainly focus on single-tabular data. Multiple-tabular data refers to many-to-many mapping among joinable attributes and n-ary relations, which cannot be utilized directly by classical ML techniques. Besides, current graph techniques, like heterogeneous information network (HIN) and graph neural networks (GNN), are infeasible to be deployed directly and automatically in a multi-table environment, which limits the learning on databases.  For automatic learning on relational databases, we propose an auto-table-join network (ATJ-Net). Multiple tables with relationships are considered as a hypergraph, where vertices are joinable attributes and hyperedges are tuples of tables. Then, ATJ-Net builds a graph neural network on the heterogeneous hypergraph, which samples and aggregates the vertices and hyperedges on n-hop sub-graphs as the receptive field. In order to enable ATJ-Net to be automatically deployed to different datasets and avoid the ”no free lunch” dilemma, we use random architecture search to select optimal aggregators and prune redundant paths in the network. For verifying the effectiveness of our methods across various tasks and schema, we conduct extensive experiments on 4 tasks, 8 various schemas, and 19 sub-datasets w.r.t. citing prediction, review classification, recommendation, and task-blind challenge. ATJ-Net achieves the best performance over state-of-the-art approaches on three tasks and is competitive with KddCup Winner solution on task-blind challenge.",
        "link": "https://dl.acm.org/doi/10.1145/3442381.3449980",
        "category": "Databases"
    },
    {
        "title": "Qualinet databases: central resource for QoE research - history, current status, and plans",
        "authors": "['Karel Fliegel', 'Lukáš Krasula', 'Werner Robitza']",
        "date": "September 2019",
        "source": "ACM SIGMultimedia Records",
        "abstract": "No abstract available.",
        "link": "https://dl.acm.org/doi/10.1145/3524460.3524465",
        "category": "Databases"
    },
    {
        "title": "Retrieving Black-box Optimal Images from External Databases",
        "authors": "['Ryoma Sato']",
        "date": "February 2022",
        "source": "WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
        "abstract": "Suppose we have a black-box function (e.g., deep neural network) that takes an image as input and outputs a value that indicates preference. How can we retrieve optimal images with respect to this function from an external database on the Internet? Standard retrieval problems in the literature (e.g., item recommendations) assume that an algorithm has full access to the set of items. In other words, such algorithms are designed for service providers. In this paper, we consider the retrieval problem under different assumptions. Specifically, we consider how users with limited access to an image database can retrieve images using their own black-box functions. This formulation enables a flexible and finer-grained image search defined by each user. We assume the user can access the database through a search query with tight API limits. Therefore, a user needs to efficiently retrieve optimal images in terms of the number of queries. We propose an efficient retrieval algorithm Tiara for this problem. In the experiments, we confirm that our proposed method performs better than several baselines under various settings.",
        "link": "https://dl.acm.org/doi/10.1145/3488560.3498462",
        "category": "Databases"
    },
    {
        "title": "An Approach to an efficient Evaluation of SPARQL Queries in Databases",
        "authors": "['Christina Ehrlinger', 'Burkhard Freitag']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "Addressing the issue of efficient SPARQL 1.1 query evaluation, this paper proposes a reordering approach that can handle any SPARQL query, regardless of the version. The presented method uses the average incoming and outgoing degree of property edges in the underlying RDF graph. Elements next to basic graph patterns available in SPARQL 1.1 were analyzed regarding their impact on the performance to formulate rules for rearranging them. We achieve promising results: Using the widely known Apache Jena and Eclipse RDF4J as triple store, we reached up to 50’000 times faster execution times than the best performing optimization approach included in Apache Jena using queries from three different well-known SPARQL benchmarks: LDBC SNB, BSBM, and SP2Bench.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487765",
        "category": "Databases"
    },
    {
        "title": "OVANA: An Approach to Analyze and Improve the Information Quality of Vulnerability Databases",
        "authors": "['Philipp Kuehn', 'Markus Bayer', 'Marc Wendelborn', 'Christian Reuter']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "Vulnerability databases are one of the main information sources for IT security experts. Hence, the quality of their information is of utmost importance for anyone working in this area. Previous work has shown that machine readable information is either missing, incorrect, or inconsistent with other data sources. In this paper, we introduce a system called Overt Vulnerability source ANAlysis (OVANA), which analyzes the information quality of vulnerability databases utilizing state-of-the-art machine learning (ML) and natural language processing (NLP) techniques, searches the free-form description for relevant information missing from structured fields, and updates it accordingly. Our paper exemplifies that on the National Vulnerability Database, showing that OVANA is able to improve the information quality by 51.23% based on the indicators of accuracy, completeness, and uniqueness. Moreover, we present information which should be incorporated into the structured fields to increase the uniqueness of vulnerability entries and improve the discriminability of different vulnerability entries. The identified information from OVANA enables a more targeted vulnerability search and provides guidance for IT security experts in finding relevant information in vulnerability descriptions for severity assessment.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3465744",
        "category": "Databases"
    },
    {
        "title": "Twitter Association Rule Mining using Clustering and Graph Databases",
        "authors": "['Alessandro Campi', 'Corrado Palese']",
        "date": "May 2021",
        "source": "ICISDM '21: Proceedings of the 2021 5th International Conference on Information System and Data Mining",
        "abstract": "In this scenario, the need to efficiently analyze this kind of data is increasing because of characteristics of such big data, especially their huge and sometimes unpredictable variety. Twitter alone, with 320 M active users every month and more than 500 M tweets per day, could represent an important source of information. For this research, we are focusing solely on social networks. The reason for this choice is that they are increasingly becoming a platform where people will comfortably update their status and share or retrieve information about the world in real time. Often news is spreading through them faster than in traditional channels because user capillarity worldwide makes it possible. In particular, we will focus on Twitter, because its micro-blogging nature makes it suitable for this kind of purpose. It questions the concept of a small private community of friends in favor of less private, less personal broadcast communications of common interest. Another reason why we chose Twitter is because semantic value of hashtags, their power in summarizing tweet content and the spreading model through the social network that allows us to highlight clusters of topics by focusing on these tags.One of the objectives of this thesis is to show how data mining can provide useful techniques to deal with these huge datasets for retrieving information to detect and analyze trending topics and the corresponding user's interactions with them. We identified in Association Rules identification and evolution in time, a systematic approach to conduct the analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3471287.3471309",
        "category": "Databases"
    },
    {
        "title": "Chinese Herbal Recognition Databases Using Human-In-The-Loop Feedback",
        "authors": "['Nan Wu', 'Yujun Zhou', 'Hao Xu', 'Xinjiao Wu']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "Traditional Chinese medicine identification plays an important role in the development of traditional Chinese medicine. Traditional Chinese medicine identification mostly relies on researchers' experience, so traditional Chinese medicine identification is still challenging. Using the computer identification of traditional Chinese medicine seems an effective method, but no dataset can train models. The lack of a dataset is the challenge of traditional Chinese medicine identification by use computers. This paper proposes a method for constructing a Chinese medicine dataset based on human-in-the-loop. This method uses a manual intervention labeling method to realize a labeling mode that saves labour resources. First, we use a web crawler to collect data from the Internet, then use a pre-model to remove some irrelevant data, next, we iterative data annotation based on the classification confidence, finally, we will obtain a dataset named CH42 that annotation by human-computer collaboration. Besides, we designed a backbone network for explicitly modeling interdependencies between channels. The CH42 contains 42 types of Chinese medicine data, a total of 6,112 pictures, the model automatically labeled about 64% of the data. We sampled 6 sets of data and found 6 mislabeled data from 1458 pictures. The model labeling accuracy rate is about 98.6%.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487114",
        "category": "Databases"
    },
    {
        "title": "Static Analysis for the No Termination Problem in Active Databases by Using Petri Nets Modelling",
        "authors": "['Joselito Medina-Marin', 'Maria Guadalupe Serna-Diaz', 'Juan C. Seck-Tuoh-Mora', 'Norberto Hernandez-Romero', 'Irving Barragan-Vite', 'Cinthia Montano-Lara']",
        "date": "November 2021",
        "source": "AISS '21: Proceedings of the 3rd International Conference on Advanced Information Science and System",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3503047.3503152",
        "category": "Databases"
    },
    {
        "title": "New Efficient Algorithm for Distributed Database Classification",
        "authors": "['Ahmed M. Khedr', 'Ibrahim Attiya']",
        "date": "December 2021",
        "source": "ICFNDS 2021: The 5th International Conference on Future Networks &amp; Distributed Systems",
        "abstract": "The problem of distributing database fragments across multiple sites in the network is challenging due to computing and communication costs in the network, storage size, delays caused by data requests queueing and costs for maintaining consistency among the fragments. In addition, the huge amount of the data makes it inefficient to join them and create an explicit database for performing global computations. In this paper, we propose a new k-nearest neighbor’s classifier for situational input stored in geographically distributed databases. The proposed algorithm is suitable and effective for distributed databases that conduct global computations across geographically distributed databases by sharing only summaries and prevent any data tuples from being transferred across the network. It performs global computations without creating an explicit database and reduces communication costs as much as possible. Reliable results are obtained through minimum information disclosure and protects the privacy of data at the individual sites in the network.",
        "link": "https://dl.acm.org/doi/10.1145/3508072.3508118",
        "category": "Databases"
    },
    {
        "title": "A variational database management system",
        "authors": "['Parisa Ataei', 'Fariba Khan', 'Eric Walkingshaw']",
        "date": "October 2021",
        "source": "GPCE 2021: Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences",
        "abstract": "Many problems require working with data that varies in its structure and content. Current approaches, such as schema evolution or data integration tools, are highly tailored to specific kinds of variation in databases. While these approaches work well in their roles, they do not address all kinds of variation and do address the interaction of different kinds of variation in databases. In this paper, we define a framework for capturing variation as a generic and orthogonal con- cern in relational databases. We define variational schemas, variational databases, and variational queries for capturing variation in the structure, content, and information needs of relational databases, respectively. We define a type system that ensures variational queries are consistent with respect to a variational schema. Finally, we design and implement a variational database management system as an abstraction layer over a traditional relational database management system. Using previously developed use cases, we show the feasibility of our framework and demonstrate the performance of different approaches used in our system",
        "link": "https://dl.acm.org/doi/10.1145/3486609.3487197",
        "category": "Databases"
    },
    {
        "title": "Multiple-source Data Collection and Processing into a Graph Database Supporting Cultural Heritage Applications",
        "authors": "['Antonio Origlia', 'Silvia Rossi', 'Sergio Di Martino', 'Francesco Cutugno', 'Maria Laura Chiacchio']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "The continuous growth of available resources on the web, both in the form of Linked Open Data and on Social Networks, provides an important opportunity to gather information concerning specific kinds of touristic activities like, for example, cultural tourism, eco-tourism, bike-tourism, and so on. Both decision makers and tourists can take advantage from these data, as demonstrated by previous works, with institutional actors foreseeing an increase in the use of this data to substitute other time-consuming and expensive approaches. However, managing multiple sources built with different goals and structures is not straightforward, so specific design choices must be made when assembling this kind of information. Graph databases represent an ideal way to combine multiple-source data but, to be successful, strategies accounting for inconsistencies and format differences have to be defined to support coherent analysis. Also, the continuously changing nature of crowd-sourced data makes it difficult, for the research community, to compare technological approaches to the different tasks that are linked to cultural heritage, from recommendation to management. To support the research effort in this direction, we describe the data ingestion and enrichment procedure we followed to organise knowledge coming from three different sources, namely Wikidata, Wikipedia, and Flickr, into a single, application-oriented, resource organised as a graph database. We present the potential use of this resource to perform multiple source analyses targeting the specific case of cultural tourism on a nationwide scale, and we propose its use as a shared benchmark for technological applications designed to support optimal management of cultural resources.",
        "link": "https://dl.acm.org/doi/10.1145/3465741",
        "category": "Databases"
    },
    {
        "title": "COMPARE: accelerating groupwise comparison in relational databases for data analytics",
        "authors": "['Tarique Siddiqui', 'Surajit Chaudhuri', 'Vivek Narasayya']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Data analysis often involves comparing subsets of data across many dimensions for finding unusual trends and patterns. While the comparison between subsets of data can be expressed using SQL, they tend to be complex to write, and suffer from poor performance over large and high-dimensional datasets. In this paper, we propose a new logical operator COMPARE for relational databases that concisely captures the enumeration and comparison between subsets of data and greatly simplifies the expressing of a large class of comparative queries. We extend the database engine with optimization techniques that exploit the semantics of COMPARE to significantly improve the performance of such queries. We have implemented these extensions inside Microsoft SQL Server, a commercial DBMS engine. Our extensive evaluation on synthetic and real-world datasets shows that COMPARE results in a significant speedup over existing approaches, including physical plans generated by today's database systems, user-defined functions (UDFs), as well as middleware solutions that compare subsets outside the databases.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476291",
        "category": "Databases"
    },
    {
        "title": "Towards Enhancing Database Education: Natural Language Generation Meets Query Execution Plans",
        "authors": "['Weiguo Wang', 'Sourav S. Bhowmick', 'Hui Li', 'Shafiq Joty', 'Siyuan Liu', 'Peng Chen']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The database systems course is offered as part of an undergraduate computer science degree program in many major universities. A key learning goal of learners taking such a course is to understand how sql queries are processed in a rdbms in practice. Since aquery execution plan (qep ) describes the execution steps of a query, learners can acquire the understanding by perusing the qep s generated by a rdbms. Unfortunately, in practice, it is often daunting for a learner to comprehend these qep s containing vendor-specific implementation details, hindering her learning process. In this paper, we present a novel, end-to-end,generic system called lantern that generates a natural language description of a qep to facilitate understanding of the query execution steps. It takes as input an sql query and its qep, and generates a natural language description of the execution strategy deployed by the underlying rdbms. Specifically, it deploys adeclarative framework called pool that enablessubject matter experts to efficiently create and maintain natural language descriptions of physical operators used in qep s. Arule-based framework called rule-lantern is proposed that exploits pool to generate natural language descriptions of qep s. Despite the high accuracy of rule-lantern, our engagement with learners reveal that, consistent with existing psychology theories, perusing such rule-based descriptions lead toboredom due to repetitive statements across different qep s. To address this issue, we present a noveldeep learning-based language generation framework called neural -lantern that infuses language variability in the generated description by exploiting a set ofparaphrasing tools andword embedding. Our experimental study with real learners shows the effectiveness of lantern in facilitating comprehension of qep s.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452822",
        "category": "Databases"
    },
    {
        "title": "VeriDB: An SGX-based Verifiable Database",
        "authors": "['Wenchao Zhou', 'Yifan Cai', 'Yanqing Peng', 'Sheng Wang', 'Ke Ma', 'Feifei Li']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The emergence of trusted hardwares (such as Intel SGX) provides a new avenue towards verifiable database. Such trust hardwares act as an additional trust anchor, allowing great simplification and, in turn, performance improvement in the design of verifiable databases. In this paper, we introduce the design and implementation of VeriDB, an SGX-based verifiable database that supports relational tables, multiple access methods and general SQL queries. Built on top of write-read consistent memory, VeriDB provides verifiable page-structured storage, where results of storage operations can be efficiently verified with low, constant overhead. VeriDB further provides verifiable query execution that supports general SQL queries. Through a series of evaluation using practical workload, we demonstrate that VeriDB incurs low overhead for achieving verifiability: an overhead of 1-2 microseconds for read/write operations, and a 9% - 39% overhead for representative analytical workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457308",
        "category": "Databases"
    },
    {
        "title": "Crystal: a unified cache storage system for analytical databases",
        "authors": "['Dominik Durner', 'Badrish Chandramouli', 'Yinan Li']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Cloud analytical databases employ a disaggregated storage model, where the elastic compute layer accesses data persisted on remote cloud storage in block-oriented columnar formats. Given the high latency and low bandwidth to remote storage and the limited size of fast local storage, caching data at the compute node is important and has resulted in a renewed interest in caching for analytics. Today, each DBMS builds its own caching solution, usually based on file-or block-level LRU. In this paper, we advocate a new architecture of a smart cache storage system called Crystal, that is co-located with compute. Crystal's clients are DBMS-specific \"data sources\" with push-down predicates. Similar in spirit to a DBMS, Crystal incorporates query processing and optimization components focusing on efficient caching and serving of single-table hyper-rectangles called regions. Results show that Crystal, with a small DBMS-specific data source connector, can significantly improve query latencies on unmodified Spark and Greenplum while also saving on bandwidth from remote storage.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476292",
        "category": "Databases"
    },
    {
        "title": "Lossless Database Watermarking Based on Order-preserving Encryption",
        "authors": "['Song Yan', 'Shuli Zheng', 'Baohong Ling', 'Donghui Hu']",
        "date": "July 2021",
        "source": "ACM TURC '21: Proceedings of the ACM Turing Award Celebration Conference - China",
        "abstract": "With the development of information technology and the popularity of Internet applications, the database outsourcing service is gaining popularity. However, since the database services provided by third parties are not completely trusted, the outsourced database model has to face a series of security issues such as privacy leakage, data theft, malicious tampering, and so forth. To both provide privacy protection and copyright tracking, this paper proposes a lossless database watermarking method based on order-preserving encryption. In the proposed method, the data owner encrypts the original data using order-preserving encryption algorithms. The outsourced database administrator then embeds the watermark bits by modifying the parities of the cover encrypted attribute values. At the database client, the database user extracts the embedded watermark bits based on the majority voting mechanism. The original database is recovered by directly decrypting the marked encrypted database. The experimental results and analysis show that the proposed method has the advantages of lossless, strong robustness, high embedding rate, no auxiliary message and low computational complexity.",
        "link": "https://dl.acm.org/doi/10.1145/3472634.3474075",
        "category": "Databases"
    },
    {
        "title": "User-defined operators: efficiently integrating custom algorithms into modern databases",
        "authors": "['Moritz Sichert', 'Thomas Neumann']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In recent years, complex data mining and machine learning algorithms have become more common in data analytics. Several specialized systems exist to evaluate these algorithms on ever-growing data sets, which are built to efficiently execute different types of complex analytics queries.However, using these various systems comes at a price. Moving data out of traditional database systems is often slow as it requires exporting and importing data, which is typically performed using the relatively inefficient CSV format. Additionally, database systems usually offer strong ACID guarantees, which are lost when adding new, external systems. This disadvantage can be detrimental to the consistency of the results.Most data scientists still prefer not to use classical database systems for data analytics. The main reason why RDBMS are not used is that SQL is difficult to work with due to its declarative and set-oriented nature, and is not easily extensible.We present User-Defined Operators (UDOs) as a concept to include custom algorithms into modern query engines. Users can write idiomatic code in the programming language of their choice, which is then directly integrated into existing database systems. We show that our implementation can compete with specialized tools and existing query engines while retaining all beneficial properties of the database system.",
        "link": "https://dl.acm.org/doi/10.14778/3510397.3510408",
        "category": "Databases"
    },
    {
        "title": "Database-adaptive Re-ranking for Enhancing Cross-modal Image Retrieval",
        "authors": "['Rintaro Yanagi', 'Ren Togo', 'Takahiro Ogawa', 'Miki Haseyama']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "We propose an approach that enhances arbitrary existing cross-modal image retrieval performance. Most of the cross-modal image retrieval methods mainly focus on direct computation of similarities between a text query and candidate images in an accurate way. However, their retrieval performance is affected by the ambiguity of text queries and the bias of target databases (DBs). Dealing with ambiguous text queries and DBs with bias will lead to accurate cross-modal image retrieval in real-world applications. A DB-adaptive re-ranking method using modality-driven spaces, which can extend arbitrary cross-modal image retrieval methods for enhancing their performance, is proposed in this paper. The proposed method includes two approaches: \"DB-adaptive re-ranking'' and \"modality-driven clue information extraction''. Our method estimates clue information that can effectively clarify the desired image from the whole set of a target DB and then receives user's feedback for the estimated information. Furthermore, our method extracts more detailed information of a query text and a target DB by focusing on modality-driven spaces, and it enables more accurate re-ranking. Our method allows users to reach their desired single image by just answering questions. Experimental results using MSCOCO, Visual Genome and newly introduced datasets including images with a particular object show that the proposed method can enhance the performance of state-of-the-art cross-modal image retrieval methods.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475681",
        "category": "Databases"
    },
    {
        "title": "Database technology for the masses: sub-operators as first-class entities",
        "authors": "['Maximilian Bandle', 'Jana Giceva']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "A wealth of technology has evolved around relational databases over decades that has been successfully tried and tested in many settings and use cases. Yet, the majority of it remains overlooked in the pursuit of performance (e.g., NoSQL) or new functionality (e.g., graph data or machine learning). In this paper, we argue that a wide range of techniques readily available in databases are crucial to tackling the challenges the IT industry faces in terms of hardware trends management, growing workloads, and the overall complexity of a rapidly changing application and platform landscape.However, to be truly useful, these techniques must be freed from the legacy component of database engines: relational operators. Therefore, we argue that to make databases more flexible as platforms and to extend their functionality to new data types and operations requires exposing a lower level of abstraction: instead of working with SQL it would be desirable for database engines to compile, optimize, and run a collection of sub-operators for manipulating and managing data, offering them as an external interface. In this paper, we discuss the advantages of this, provide an initial list of such sub-operators, and show how they can be used in practice.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476296",
        "category": "Databases"
    },
    {
        "title": "MB2: Decomposed Behavior Modeling for Self-Driving Database Management Systems",
        "authors": "['Lin Ma', 'William Zhang', 'Jie Jiao', 'Wuwen Wang', 'Matthew Butrovich', 'Wan Shen Lim', 'Prashanth Menon', 'Andrew Pavlo']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Database management systems (DBMSs) are notoriously difficult to deploy and administer. The goal of a self-driving DBMS is to remove these impediments by managing itself automatically. However, a critical problem in achieving full autonomy is how to predict the DBMS's runtime behavior and resource consumption. These predictions guide a self-driving DBMS's decision-making components to tune and optimize all aspects of the system. We present the ModelBot2 end-to-end framework for constructing and maintaining prediction models using machine learning (ML) in self-driving DBMSs. Our approach decomposes a DBMS's architecture into fine-grained operating units that make it easier to estimate the system's behavior for configurations that it has never seen before. ModelBot2 then provides an offline execution environment to exercise the system to produce the training data used to train its models. We integrated ModelBot2 in an in-memory DBMS and measured its ability to predict its performance for OLTP and OLAP workloads running in dynamic environments. We also compare ModelBot2 against state-of-the-art ML models and show that our models are up to 25x more accurate in multiple scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457276",
        "category": "Databases"
    },
    {
        "title": "In-Database Machine Learning with SQL on GPUs",
        "authors": "['Maximilian Schule', 'Harald Lang', 'Maximilian Springer', 'Alfons Kemper', 'Thomas Neumann', 'Stephan Gunnemann']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "In machine learning, continuously retraining a model guarantees accurate predictions based on the latest data as training input. But to retrieve the latest data from a database, time-consuming extraction is necessary as database systems have rarely been used for operations such as matrix algebra and gradient descent.  In this work, we demonstrate that SQL with recursive tables makes it possible to express a complete machine learning pipeline out of data preprocessing, model training and its validation. To facilitate the specification of loss functions, we extend the code-generating database system Umbra by an operator for automatic differentiation for use within recursive tables: With the loss function expressed in SQL as a lambda function, Umbra generates machine code for each partial derivative. We further use automatic differentiation for a dedicated gradient descent operator, which generates LLVM code to train a user-specified model on GPUs. We fine-tune GPU kernels at hardware level to allow a higher throughput and propose non-blocking synchronisation of multiple units.  In our evaluation, automatic differentiation accelerated the runtime by the number of cached subexpressions compared to compiling each derivative separately. Our GPU kernels with independent models allowed maximal throughput even for small batch sizes, making machine learning pipelines within SQL more competitive.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468840",
        "category": "Databases"
    },
    {
        "title": "A distributed database system for event-based microservices",
        "authors": "['Rodrigo Laigner', 'Yongluan Zhou', 'Marcos Antonio Vaz Salles']",
        "date": "June 2021",
        "source": "DEBS '21: Proceedings of the 15th ACM International Conference on Distributed and Event-based Systems",
        "abstract": "Microservice architectures are an emerging industrial approach to build large scale and event-based systems. In this architectural style, an application is functionally partitioned into several small and autonomous building blocks, so-called microservices, communicating and exchanging data with each other via events. By pursuing a model where fault isolation is enforced at microservice level, each microservice manages their own database, thus database systems are not shared across microservices. Developers end up encoding substantial data management logic in the application-tier and encountering a series of challenges on enforcing data integrity and maintaining data consistency across microservices. In this vision paper, we argue that there is a need to rethink how database systems can better support microservices and relieve the burden of handling complex data management tasks faced by programmers. We envision the design and research opportunities for a novel distributed database management system targeted at event-driven microservices.",
        "link": "https://dl.acm.org/doi/10.1145/3465480.3466919",
        "category": "Databases"
    },
    {
        "title": "SQL Ledger: Cryptographically Verifiable Data in Azure SQL Database",
        "authors": "['Panagiotis Antonopoulos', 'Raghav Kaushik', 'Hanuma Kodavalla', 'Sergio Rosales Aceves', 'Reilly Wong', 'Jason Anderson', 'Jakub Szymaszek']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "SQL Ledger is a new technology that allows cryptographically verifying the integrity of relational data stored in Azure SQL Database and SQL Server. This is achieved by maintaining all historical data in the database and persisting its cryptographic (SHA-256) digests in an immutable, tamper-evident ledger. Digests representing the overall state of the ledger can then be extracted and stored outside of the RDBMS to protect the data from any attacker or high privileged user, including DBAs, system and cloud administrators. The ledger and the historical data are managed transparently, offering protection without any application changes. Historical data is maintained in a relational form to support SQL queries for auditing, forensics and other purposes. SQL Ledger provides cryptographic data integrity guarantees while maintaining the power, flexibility and performance of a commercial RDBMS. In contrast to Blockchain solutions that aim for full integrity, SQL Ledger offers a form of integrity protection known as Forward Integrity. The proposed technology is significantly cheaper and more secure than traditional solutions that establish trust based on audits or mediators, but also has substantial advantages over Blockchain solutions that are complex to deploy, lack data management capabilities and suffer in terms of performance due to their decentralized nature.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457558",
        "category": "Databases"
    },
    {
        "title": "A Quantitative Analysis of Student Solutions to Graph Database Problems",
        "authors": "['Mei Chen', 'Seth Poulsen', 'Ridha Alkhabaz', 'Abdussalam Alawini']",
        "date": "June 2021",
        "source": "ITiCSE '21: Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1",
        "abstract": "As data grow both in size and in connectivity, the interest to use graph databases in the industry has been proliferating. However, there has been little research on graph database education. In response to the need to introduce college students to graph databases, this paper is the first to analyze students' errors in homework submissions of queries written in Cypher, the query language for Neo4j---the most prominent graph database. Based on 40,093 student submissions from homework assignments in an upper-level computer science database course at one university, this paper provides a quantitative analysis of students' learning when solving graph database problems. The data shows that students struggle the most to correctly use Cypher's WITH clause to define variable names before referencing in the WHERE clause and these errors persist over multiple homework problems requiring the same techniques, and we suggest a further improvement on the classification of syntactic errors.",
        "link": "https://dl.acm.org/doi/10.1145/3430665.3456314",
        "category": "Databases"
    },
    {
        "title": "Application of Database Read and Write Separation and Table Partition in Liquefied Gas Information Management System",
        "authors": "['Xiong Wei Hu', 'Yi Du']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "With the expansion of LPG information management system business and the increase in orders year by year, In addition to satisfying the normal functional requirements of users, the LPG information management system also needs to consider the high concurrent requests of the database and the response performance of accessing the order form during peak periods. This paper proposes a design scheme based on database read-write separation and table partitioning in order to solve the response delay of high concurrency and query large data volume order tables and relieve the load pressure of the database server. Experimental results show that read-write separation can greatly improve the responsiveness of the system; table partitioning can greatly shorten the query time for data. And this solution can effectively achieve high availability.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501600",
        "category": "Databases"
    },
    {
        "title": "Towards a Flexible Relational Database Query System",
        "authors": "['Rachid Mama', 'Mustapha Machkour', 'Karam Ahkouk', 'Khadija Majhadi']",
        "date": "April 2021",
        "source": "NISS '21: Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security",
        "abstract": "In many cases information is found to be naturally fuzzy or imprecise, that's why flexible query systems have become indispensable to represent and manage this information and especially facilitate interrogation to a non-expert user, the problem is that Boolean queries do not allow the user to use vague and imprecise language terms in the qualification criteria of the searched data or to express preferences between these criteria. Nowadays, there are many proposals that allow users to make flexible queries on relational databases. In this paper, we will briefly review the main attempts to find a perfect solution to this problem, highlighting their advantages, disadvantages and difficulties encountered. Finally, as a result of this comparative study, we present an intended model of the flexible relational database query system.",
        "link": "https://dl.acm.org/doi/10.1145/3454127.3456579",
        "category": "Databases"
    },
    {
        "title": "Riso-Tree: An Efficient and Scalable Index for Spatial Entities in Graph Database Management Systems",
        "authors": "['Yuhan Sun', 'Mohamed Sarwat']",
        "date": "None",
        "source": "ACM Transactions on Spatial Algorithms and Systems",
        "abstract": "With the ubiquity of spatial data, vertexes or edges in graphs can possess spatial location attributes side by side with other non-spatial attributes. For instance, as of June 2018, the Wikidata knowledge graph contains 48,547,142 data items (i.e., vertexes) and \\(\\)13% of them have spatial location attributes. The article proposes Riso-Tree, a generic efficient and scalable indexing framework for spatial entities in graph database management systems. Riso-Tree enables the fast execution of graph queries that involve different types of spatial predicates (GraSp queries). The proposed framework augments the classic R-Tree structure with pre-materialized sub-graph entries. The pruning power of R-Tree is enhanced with the sub-graph information. Riso-Tree partitions the graph into sub-graphs based on their connectivity to the spatial sub-regions. The proposed index allows for the fast execution of GraSp queries by efficiently pruning the traversed vertexes/edges based upon the materialized sub-graph information. The experiments show that the proposed Riso-Tree achieves up to two orders of magnitude faster execution time than its counterparts when executing GraSp queries on real graphs (e.g., Wikidata). The strategy of limiting the size of each sub-graph entry (PNmax) is proposed to reduce the storage overhead of Riso-Tree. The strategy can save up to around 70% storage without harming the query performance according to the experiments. Another strategy is proposed to ensure the performance of the index maintenance (Irrelevant Vertexes Skipping). The experiments show that the strategy can improve performance, especially for slow updates. It proves that Riso-Tree is useful for applications that need to support frequent updates.",
        "link": "https://dl.acm.org/doi/10.1145/3450945",
        "category": "Databases"
    },
    {
        "title": "Scooter & Sidecar: a domain-specific approach to writing secure database migrations",
        "authors": "['John Renner', 'Alex Sanchez-Stern', 'Fraser Brown', 'Sorin Lerner', 'Deian Stefan']",
        "date": "June 2021",
        "source": "PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
        "abstract": "Web applications often handle large amounts of sensitive user data. Modern secure web frameworks protect this data by (1) using declarative languages to specify security policies alongside database schemas and (2) automatically enforcing these policies at runtime. Unfortunately, these frameworks do not handle the very common situation in which the schemas or the policies need to evolve over time---and updates to schemas and policies need to be performed in a carefully coordinated way. Mistakes during schema or policy migrations can unintentionally leak sensitive data or introduce privilege escalation bugs. In this work, we present a domain-specific language (Scooter) for expressing schema and policy migrations, and an associated SMT-based verifier (Sidecar) which ensures that migrations are secure as the application evolves. We describe the design of Scooter and Sidecar and show that our framework can be used to express realistic schemas, policies, and migrations, without giving up on runtime or verification performance.",
        "link": "https://dl.acm.org/doi/10.1145/3453483.3454072",
        "category": "Databases"
    },
    {
        "title": "GAWD: graph anomaly detection in weighted directed graph databases",
        "authors": "['Meng-Chieh Lee', 'Hung T. Nguyen', 'Dimitris Berberidis', 'Vincent S. Tseng', 'Leman Akoglu']",
        "date": "November 2021",
        "source": "ASONAM '21: Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
        "abstract": "Given a set of node-labeled directed weighted graphs, how to find the most anomalous ones? How can we summarize the normal behavior in the database without losing information? We propose GAWD, for detecting anomalous graphs in directed weighted graph databases. The idea is to (1) iteratively identify the \"best\" substructure (i.e., subgraph or motif) that yields the largest compression when each of its occurrences is replaced by a super-node, and (2) score each graph by how much it compresses over iterations --- the more the compression, the lower the anomaly score. Different from existing work [1] on which we build, GAWD exhibits (i) a lossless graph encoding scheme, (ii) ability to handle numeric edge weights, (iii) interpretability by common patterns, and (iv) scalability with running time linear in input size. Experiments on four datasets injected with anomalies show that GAWD achieves significantly better results than state-of-the-art baselines.",
        "link": "https://dl.acm.org/doi/10.1145/3487351.3488325",
        "category": "Databases"
    },
    {
        "title": "Technical Perspective for: Query Games in Databases",
        "authors": "['Dan Suciu']",
        "date": "March 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "When a data analyst runs some query to analyze her data, she often wants to ask some follow-up questions, about the result of the query. Why-questions take many shapes, and occur in many scenarios. Why is a particular tuple in the answer? Why is it not in the answer? Why is this graph decreasing? Why did we observe a sudden burst of error messages in online monitoring? Database researchers have noted the need for why-questions, and the literature contains several approaches, mostly tailored to specific applications. Despite the interest and the work in this area, there is currently no consensus of what an explanation to a query answer should be, and how one should compute it.",
        "link": "https://dl.acm.org/doi/10.1145/3471485.3471503",
        "category": "Databases"
    },
    {
        "title": "Web application database protection from SQLIA using permutation encoding",
        "authors": "['Mohammed Abdulridha Hussain', 'Salah H. Abbdal Refish', 'Mustafa S. Khalefa', 'Sarah Abdulridha Hussain', 'Zaid Alaa Hussien', 'Zaid Ameen Abduljabbar', 'Mustafa A. Al Sibahee']",
        "date": "March 2021",
        "source": "ICISS '21: Proceedings of the 4th International Conference on Information Science and Systems",
        "abstract": "Web application is the base of online businesses through the Internet. The emergence of COVID 19 forced almost every job to operate online so as to bridge the distance amongst individuals. The rapid increment in the needs of web application increases security threats on information and data. According to the Open Web Application Security Project, Structured Query Language Injection Attack (SQLIA) is a top security threat for web application. SQLIA inserts malicious code to gain access or to manipulate database information by cheating the server to bypass the code to the database, thereby causing a severe impact on web application. In this paper, permutation encoding method has been proposed to prevent SQLIA, which is based on encoding all database information using the proposed method. Initially, a special character is inserted to restrict the method from reversing. Subsequently, permutation encoding method is applied. Permutation refers to the method wherein the bit location is changed within three characters and then radix encoding is applied. Permutation is based on the primitive root value. Encoding has been used to hide permutation. The proposed method is implemented and tested using PHP and MySQL databases, where the proposal result has been compared with those of other proposal methods. The results with security analysis prove that the proposal method prevents SQLIA and protects database information.",
        "link": "https://dl.acm.org/doi/10.1145/3459955.3460594",
        "category": "Databases"
    },
    {
        "title": "A Privacy Preserving Two-Ticket Management System Based on Negative Database for Smart Grid",
        "authors": "['Xinxiu Xiao', 'Jinglong Xie', 'Guibin Xu', 'Jun Yan']",
        "date": "December 2021",
        "source": "ACAI '21: Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence",
        "abstract": "In China, the electric power industry has used the two-ticket system to ensure safety of power production and employees. Currently, the two-ticket system are normally paper based and manually operated, suffering problems such as invalid ticket filling, no ticket operation, share tickets with others, etc. As the development of information technologies, digitization of the two-ticket system can improve both efficiency and safety of power production. However, there are also privacy concerns with user’s password in the digital two-ticket system. In order to address the above requirements, this paper introduces a privacy preserving two-ticket management system. The system stores user’s password in the smart card using negative database, and the system can be accessed by the NFC technology without leaking user’s passwords. Security analysis demonstrates that the proposed authentication method based on negative databases can effectively resist identity stealing attack, password guessing attack, replay attack, man-in-the-middle attack, and smart card loss attack. This method has potential applications in the digital two-ticket system in smart grid.",
        "link": "https://dl.acm.org/doi/10.1145/3508546.3508595",
        "category": "Databases"
    },
    {
        "title": "An intermediate representation for hybrid database and machine learning workloads",
        "authors": "['Amir Shaikhha', 'Maximilian Schleich', 'Dan Olteanu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "IFAQ is an intermediate representation and compilation framework for hybrid database and machine learning workloads expressible using iterative programs with functional aggregate queries. We demonstrate IFAQ for several OLAP queries, linear algebra expressions, and learning factorization machines over training datasets defined by feature extraction queries over relational databases.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476356",
        "category": "Databases"
    },
    {
        "title": "A study of work distribution and contention in database primitives on heterogeneous CPU/GPU architectures",
        "authors": "['Michael Gowanlock', 'Zane Fink', 'Ben Karsin', 'Jordan Wright']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Graphics Processing Units (GPUs) provide very high on-card memory bandwidth which can be exploited to address data-intensive workloads. To maximize algorithm throughput, it is important to concurrently utilize both the CPU and GPU to carry out database queries. We select data-intensive algorithms that are common in databases and data analytic applications including: (i) scan; (ii) batched predecessor searches; (iii) multiway merging; and, (iv) partitioning. For each algorithm, we examine the performance of parallel CPU/GPU-only, and hybrid CPU/GPU approaches. There are several challenges to combining the CPU and GPU for query processing, including distributing work between architectures. We demonstrate that despite being able to accurately split the work between the CPU and GPU, contention for memory bandwidth is a major limiting factor for hybrid CPU/GPU data-intensive algorithms. We employ performance models that allow us to explore several research questions. We find that while hybrid data-intensive algorithms may be limited by contention, these algorithms are more robust to workload characteristics; therefore, they are preferable to CPU/GPU-only approaches. We also find that hybrid algorithms achieve good performance when there is low memory contention between the CPU and GPU, such that the GPU can perform its operations without significantly reducing CPU throughput.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441913",
        "category": "Databases"
    },
    {
        "title": "Cryptanalysis of an encrypted database in SIGMOD '14",
        "authors": "['Xinle Cao', 'Jian Liu', 'Hao Lu', 'Kui Ren']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Encrypted database is an innovative technology proposed to solve the data confidentiality issue in cloud-based DB systems. It allows a data owner to encrypt its database before uploading it to the service provider; and it allows the service provider to execute SQL queries over the encrypted data. Most of existing encrypted databases (e.g., CryptDB in SOSP '11) do not support data interoperability: unable to process complex queries that require piping the output of one operation to another.To the best of our knowledge, SDB (SIGMOD '14) is the only encrypted database that achieves data interoperability. Unfortunately, we found SDB is not secure! In this paper, we revisit the security of SDB and propose a ciphertext-only attack named co-prime attack. It successfully attacks the common operations supported by SDB, including addition, comparison, sum, equi-join and group-by. We evaluate our attack in three real-world benchmarks. For columns that support addition and comparison, we recover 84.9% -- 99.9% plaintexts. For columns that support sum, equi-join and group-by, we recover 100% plaintexts.Besides, we provide potential countermeasures that can prevent the attacks against sum, equi-join, group-by and addition. It is still an open problem to prevent the attack against comparison.",
        "link": "https://dl.acm.org/doi/10.14778/3467861.3467865",
        "category": "Databases"
    },
    {
        "title": "Embedded Functional Dependencies and Data-completeness Tailored Database Design",
        "authors": "['Ziheng Wei', 'Sebastian Link']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.",
        "link": "https://dl.acm.org/doi/10.1145/3450518",
        "category": "Databases"
    },
    {
        "title": "A Multi-modal Males Face Database and Application in Research",
        "authors": "['Guzalnur Amrurla', 'Nurbiya Yadikar', 'Mamatjan Tursun', 'Kurban Ubul']",
        "date": "October 2021",
        "source": "ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition",
        "abstract": "This paper presents a face database composed mainly from male faces and testing of some image processing algorithms on the database. The face database takes into account the variation in illumination conditions and beard style which include mustache, beard and face without beard. The database could provide with training and test set to researches related to face recognition and occlusion detection or removing algorithms. Using the database as test set and training set, feature points detection algorithm based on traditional AAM model was tested and similarity of bearded face and normal face images were checked using Perceptual Hashing algorithm to evaluate effectiveness of those algorithms with our database.",
        "link": "https://dl.acm.org/doi/10.1145/3497623.3497654",
        "category": "Databases"
    },
    {
        "title": "Fuzzy Database and Interface to Analyze Management System Operations",
        "authors": "['Yasunori Shiono', 'Takaaki Goto', 'Toshihiro Yoshizumi', 'Kensei Tsuchida']",
        "date": "June 2021",
        "source": "ACIT '21: Proceedings of the the 8th International Virtual Conference on Applied Computing &amp; Information Technology",
        "abstract": "Management systems, such as Information Security Management Systems (ISMS) and IT Service Management Systems (ITSMS), are typically applied to the entire organization, and such systems attempt to provide continual improvement. Efficient engagement with business operations is an issue; consequently, understanding and analyzing actual situations is required. We have been researching analysis methods using fuzzy theory, which enables imprecise expressions. By building a useful fuzzy database and realizing an analysis method for management system operations, our goal is to develop a useful tool for organizational management. In this paper, we introduce the concept of fuzzy database construction, an analysis method, and a case study related to management system operations.",
        "link": "https://dl.acm.org/doi/10.1145/3468081.3471063",
        "category": "Databases"
    },
    {
        "title": "Development of a Thermographic Database with Images of the Vulva",
        "authors": "['Aida I.G Raposo', 'Lius C.C Pires', 'Jose A.F. Moutinho', 'Pedro D. Silva', 'Ana R. Gouveia']",
        "date": "May 2021",
        "source": "ICBEA '21: 4th International Conference on Biometric Engineering and Applications",
        "abstract": "This work aims to present a thermographic database of healthy vulva images and to explain the entire process of acquisition, processing, and analysis of the thermograms and other non-image information. We performed thermographic acquisitions of the vulva of 86 asymptomatic women and, after pre-processing and analyzing the thermograms, we obtained a total of 59 women with viable thermograms. To ensure we obtained at least one viable thermogram per women and to avoid loss of information due to human error, multiple acquisitions were made for each woman resulting in a total of 113 viable thermograms. All 113 thermograms were analyzed to guarantee that all the information obtained was included in the database. In this way, each thermogram was sectioned in 6 ROIs and the descriptive statistics were computed for each one. We identify the major difficulties of this process and present solutions for them.",
        "link": "https://dl.acm.org/doi/10.1145/3476779.3476785",
        "category": "Databases"
    },
    {
        "title": "Computing Component Alignment Normalization Degree in Digital Ink Chinese Character from International Students Based on Information Database",
        "authors": "['Xijin Sun', 'Xiwen Zhang']",
        "date": "August 2021",
        "source": "ICICM '21: Proceedings of the 11th International Conference on Information Communication and Management",
        "abstract": "There are many irregularities of components in the handwritten Chinese character by junior international students. Among them, the problem of component alignment is more prominent. For digital ink Chinese characters (DICCs), this paper proposes a difference computing approach based on the component alignment information database of reference Chinese characters (RCCs). The component alignment database stores information related to various types of component alignments, and facilitates automatically extracting handwritten Chinese character components by reading the structural information in the database. Component alignment computing is based on the barycenter approach, which takes full account of the structure type and component layout rule of Chinese characters, supplemented by component boundary alignment computing, and the average value of each alignment data is normalized. Using the component alignment data of the RCC as a standard, the difference between the component alignment of the DICC and the RCC is computed to obtain the alignment difference. The experimental data includes three kinds of Chinese character structures, which are suitable for computing ten kinds of structures of Chinese characters. Experimental results show that the computed component alignment difference is consistent with human subjective visual perception. The quantitative value of the alignment difference can provide correction data, which is convenient for the visual correction of subsequent work and the teaching of component writing. It can also perform statistical analysis and rule discovery, and provide data support for the study of Chinese character acquisition.",
        "link": "https://dl.acm.org/doi/10.1145/3484399.3484408",
        "category": "Databases"
    },
    {
        "title": "Using the UCSC genome browser in a database course",
        "authors": "['Barbara M. Anthony']",
        "date": "None",
        "source": "Journal of Computing Sciences in Colleges",
        "abstract": "Students can benefit greatly from working with real databases in their first Database course. A database for a university is a common textbook example, in part due to its familiarity, but privacy and other considerations typically preclude course access to this and many other large, meaningful databases. This paper reports on two semesters' experience using the University of California Santa Cruz Genome Browser [6] in a Database course, allowing mid-level computer science undergraduates to gain hands-on experience with a large real-world database. Anonymous survey feedback from students in both semesters was positive for both engagement and increased knowledge. The activity described within can easily be adopted by others, requires no software installation, and can be adapted to the desired length and difficulty level.",
        "link": "https://dl.acm.org/doi/10.5555/3503984.3503986",
        "category": "Databases"
    },
    {
        "title": "Replicated layout for in-memory database systems",
        "authors": "['Sivaprasad Sudhir', 'Michael Cafarella', 'Samuel Madden']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Scanning and filtering are the foundations of analytical database systems. Modern DBMSs employ a variety of techniques to partition and layout data to improve the performance of these operations. To accelerate query performance, systems tune data layout to reduce the cost of accessing and processing data. However, these layouts optimize for the average query, and with heterogeneous data access patterns in parts of the data, their performance degrades. To mitigate this, we present CopyRight, a layout-aware partial replication engine that replicates parts of the data differently and lays out each replica differently to maximize the overall query performance. Across a range of real-world query workloads, CopyRight is able to achieve 1.1X to 7.9X faster performance than the best non-replicated layout with 0.25X space overhead. When compared to full table replication with 100% overhead, CopyRight attains the same or up to 5.2X speedup with 25% space overhead.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503606",
        "category": "Databases"
    },
    {
        "title": "Database isolation by scheduling",
        "authors": "['Kevin P. Gaffney', 'Robert Claus', 'Jignesh M. Patel']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Transaction isolation is conventionally achieved by restricting access to the physical items in a database. To maximize performance, isolation functionality is often packaged with recovery, I/O, and data access methods in a monolithic transactional storage manager. While this design has historically afforded high performance in online transaction processing systems, industry trends indicate a growing need for a new approach in which intertwined components of the transactional storage manager are disaggregated into modular services. This paper presents a new method to modularize the isolation component. Our work builds on predicate locking, an isolation mechanism that enables this modularization by locking logical rather than physical items in a database. Predicate locking is rarely used as the core isolation mechanism because of its high theoretical complexity and perceived overhead. However, we show that this overhead can be substantially reduced in practice by optimizing for common predicate structures.We present DIBS, a transaction scheduler that employs our predicate locking optimizations to guarantee isolation as a modular service. We evaluate the performance of DIBS as the sole isolation mechanism in a data processing system. In this setting, DIBS scales up to 10.5 million transactions per second on a TATP workload. We also explore how DIBS can be applied to existing database systems to increase transaction throughput. DIBS reduces per-transaction file system writes by 90% on TATP in SQLite, resulting in a 3X improvement in throughput. Finally, DIBS reduces row contention on YCSB in MySQL, providing serializable isolation with a 1.4X improvement in throughput.",
        "link": "https://dl.acm.org/doi/10.14778/3461535.3461537",
        "category": "Databases"
    },
    {
        "title": "Study on establishment of bloodletting vein information database in Mongolian medicine under the background of big data",
        "authors": "['Chunrong Wang', 'Yongqing Zhao', 'Huori Cha']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "In this paper, the bloodletting vein information database of Mongolian medicine was established by using literature materials. On the basis of it, a simple Mongolian medicine bloodletting vein information search system was realized. This study will promote the digitization of Mongolian medicine theory, and It will play a significant role in the field of Mongolian medicine education and teaching, scientific research and clinical diagnosis and treatment.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470227",
        "category": "Databases"
    },
    {
        "title": "Managing Query Processing on Encrypted Database on the Cloud Efficiently",
        "authors": "['Osama M. Ben Omran']",
        "date": "October 2021",
        "source": "ICEMIS'21: The 7th International Conference on Engineering &amp; MIS 2021",
        "abstract": "Abstract- Many advantages have been provided by Cloud computing to organizations and computer users. It allows many services and applications to distribute as services in an economical way to organizations and users, Therefore, these organizations and users have begun using Cloud computing. On the other hand, they are concerned about their information when they store it on the Cloud Computing. In this paper, a technique has been proposed to encrypt the data on the client site and send it to the Cloud, and the SQL queries execute and run on the Cloud over encrypted data. The technique begins by studying Where statements that are used with the SELECT statement and define all the attributes that are used with the Where statement. In addition, Encoding codes have been generated to use with SQL statements on the Cloud instead of using conditions with main attributes. Moreover, to achieve efficiency, exact records or data that are requested from the Cloud are returned to the client. The client site has to decrypt the data to prevent any leakage at the Cloud or during transmission of the data. This method improves query processing performance while protecting database tables on the Cloud and helps in reducing the entire processing time.",
        "link": "https://dl.acm.org/doi/10.1145/3492547.3492640",
        "category": "Databases"
    },
    {
        "title": "Projection-compliant database generation",
        "authors": "['Anupam Sanghi', 'Shadab Ahmed', 'Jayant R. Haritsa']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Synthesizing data using declarative formalisms has been persuasively advocated in contemporary data generation frameworks. In particular, they specify operator output volumes through row-cardinality constraints. However, thus far, adherence to these volumetric constraints has been limited to the Filter and Join operators. A critical deficiency is the lack of support for the Projection operator, which is at the core of basic SQL constructs such as Distinct, Union and Group By. The technical challenge here is that cardinality unions in multi-dimensional space, and not mere summations, need to be captured in the generation process. Further, dependencies across different data subspaces need to be taken into account.We address the above lacuna by presenting PiGen, a dynamic data generator that incorporates Projection cardinality constraints in its ambit. The design is based on a projection subspace division strategy that supports the expression of constraints using optimized linear programming formulations. Further, techniques of symmetric refinement and workload decomposition are introduced to handle constraints across different projection subspaces. Finally, PiGen supports dynamic generation, where data is generated on-demand during query processing, making it amenable to Big Data environments. A detailed evaluation on workloads derived from real-world and synthetic benchmarks demonstrates that PiGen can accurately and efficiently model Projection outcomes, representing an essential step forward in customized database generation.",
        "link": "https://dl.acm.org/doi/10.14778/3510397.3510398",
        "category": "Databases"
    },
    {
        "title": "RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications",
        "authors": "['Siying Dong', 'Andrew Kryczka', 'Yanqin Jin', 'Michael Stumm']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "This article is an eight-year retrospective on development priorities for RocksDB, a key-value store developed at Facebook that targets large-scale distributed systems and that is optimized for Solid State Drives (SSDs). We describe how the priorities evolved over time as a result of hardware trends and extensive experiences running RocksDB at scale in production at a number of organizations: from optimizing write amplification, to space amplification, to CPU utilization. We describe lessons from running large-scale applications, including that resource allocation needs to be managed across different RocksDB instances, that data formats need to remain backward- and forward-compatible to allow incremental software rollouts, and that appropriate support for database replication and backups are needed. Lessons from failure handling taught us that data corruption errors needed to be detected earlier and that data integrity protection mechanisms are needed at every layer of the system. We describe improvements to the key-value interface. We describe a number of efforts that in retrospect proved to be misguided. Finally, we describe a number of open problems that could benefit from future research.",
        "link": "https://dl.acm.org/doi/10.1145/3483840",
        "category": "Databases"
    },
    {
        "title": "Enhancing the learning of database access programming using continuous integration and aspect oriented programming",
        "authors": "['Beatriz Pérez']",
        "date": "May 2021",
        "source": "ICSE-JSEET '21: Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training",
        "abstract": "Database access programming is a noteworthy component of Software Engineering (SE) education on databases that students are expected to acquire during training for their careers. In our university, we cover such an education in a course that emphasizes the use of the JDBC API to access databases. This paper presents our experiences in developing and running a framework to enhance the learning experience of database access programming, which is motivated by several factors. First, our students face great demands on acquiring JDBC acknowledge, and providing them with constructive feedback serves a critical role. Second, the increasing number of students leads to high efforts in managing and grading their assignments. Finally, we consider of strategic importance to bring modern industrial SE techniques into the classroom, so that students obtain a better experience with industry practices. Our framework draws upon constructive alignment and automated formative assessment, combining Continuous Integration (CI) and Aspect Oriented Programming (AOP). We include an innovative application of AOP, a programming technique that aims to modularize inherently scattered functionality into single functional units, to help students adopt well-established JDBC best practices. We also use well-known industrial software tools (Travis CI and GitHub) to manage and grade students' assignments and support automated integration testing with databases. The findings of this study, applied to a class of 53 students, suggest positive effects, such as motivate students to implement JDBC best practices, streamline the management and grading of their assignments, help them get familiar with industrial tools, or improve their grades.",
        "link": "https://dl.acm.org/doi/10.1109/ICSE-SEET52601.2021.00032",
        "category": "Databases"
    },
    {
        "title": "Database workload characterization with query plan encoders",
        "authors": "['Debjyoti Paul', 'Jie Cao', 'Feifei Li', 'Vivek Srikumar']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Smart databases are adopting artificial intelligence (AI) technologies to achieve instance optimality, and in the future, databases will come with prepackaged AI models within their core components. The reason is that every database runs on different workloads, demands specific resources, and settings to achieve optimal performance. It prompts the necessity to understand workloads running in the system along with their features comprehensively, which we dub as workload characterization.To address this workload characterization problem, we propose our query plan encoders that learn essential features and their correlations from query plans. Our pretrained encoders captures the structural and the computational performance of queries independently. We show that our pretrained encoders are adaptable to workloads that expedites the transfer learning process. We performed independent assessments of structural encoder and performance encoders with multiple downstream tasks. For the overall evaluation of our query plan encoders, we architect two downstream tasks (i) query latency prediction and (ii) query classification. These tasks show the importance of feature-based workload characterization. We also performed extensive experiments on individual encoders to verify the effectiveness of representation learning, and domain adaptability.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503600",
        "category": "Databases"
    },
    {
        "title": "Automatic Generation and Marking of UML Database Design Diagrams",
        "authors": "['Sarah Foss', 'Tatiana Urazova', 'Ramon Lawrence']",
        "date": "February 2022",
        "source": "SIGCSE 2022: Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1",
        "abstract": "Interactive question systems improve student engagement and provide opportunities for increased practice and skill mastery. Developing database design diagrams is a key skill for database courses, but providing evaluation feedback is time-consuming for instructors and accurate auto-grading is challenging due to the variability of student answers especially when labeling diagram components. This work presents a system for the automatic creation and real-time evaluation of database design questions using UML diagrams. Students directly interact with the question text, and the system continuously generates a visual representation of their answer as well as provides immediate feedback at any time. By utilizing a web-based, customizable user interface, the system supports precise marking and the ability to practice variants of design questions to mastery. A classroom evaluation demonstrates high student satisfaction compared to traditional UML design questions and preference for using the software to improve their learning outcomes.",
        "link": "https://dl.acm.org/doi/10.1145/3478431.3499376",
        "category": "Databases"
    },
    {
        "title": "Real-Time Entity Resolution by Forest-Based Indexing in Database Systems with Vertical Fragmentations",
        "authors": "['Liang Zhu', 'Jiapeng Yang', 'Xin Song', 'Yu Wang', 'Yonggang Wei']",
        "date": "October 2021",
        "source": "CSAE '21: Proceedings of the 5th International Conference on Computer Science and Application Engineering",
        "abstract": "Entity resolution (ER) is the process of identifying and matching which tuples/records in a dataset/relation refer to the same real-world entity. Real-time ER is a challenge for large datasets. Schema decomposition is of importance in (distributed) database systems, which partitions a relation/table into a set of vertical fragmentations. For this scenario, we study real-time ER in this paper. By creating forest-based indexing and defining ranking functions and corresponding algorithms, we propose an approach to resolve query tuples over dirty relations of a set of vertical fragmentations with duplicates, misspellings, or NULL values of text attributes. Extensive experiments are conducted to demonstrate the performances of our proposed approach.",
        "link": "https://dl.acm.org/doi/10.1145/3487075.3487142",
        "category": "Databases"
    },
    {
        "title": "Columnar storage and list-based processing for graph database management systems",
        "authors": "['Pranjal Gupta', 'Amine Mhedhbi', 'Semih Salihoglu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We revisit column-oriented storage and query processing techniques in the context of contemporary graph database management systems (GDBMSs). Similar to column-oriented RDBMSs, GDBMSs support read-heavy analytical workloads that however have fundamentally different data access patterns than traditional analytical workloads. We first derive a set of desiderata for optimizing storage and query processors of GDBMS based on their access patterns. We then present the design of columnar storage, compression, and query processing techniques based on these desiderata. In addition to showing direct integration of existing techniques from columnar RDBMSs, we also propose novel ones that are optimized for GDBMSs. These include a novel list-based query processor, which avoids expensive data copies of traditional block-based processors under many-to-many joins, a new data structure we call single-indexed edge property pages and an accompanying edge ID scheme, and a new application of Jacobson's bit vector index for compressing NULL values and empty lists. We integrated our techniques into the GraphflowDB in-memory GDBMS. Through extensive experiments, we demonstrate the scalability and query performance benefits of our techniques.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476297",
        "category": "Databases"
    },
    {
        "title": "Towards cost-effective and elastic cloud database deployment via memory disaggregation",
        "authors": "['Yingqiang Zhang', 'Chaoyi Ruan', 'Cheng Li', 'Xinjun Yang', 'Wei Cao', 'Feifei Li', 'Bo Wang', 'Jing Fang', 'Yuhui Wang', 'Jingze Huo', 'Chao Bi']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "It is challenging for cloud-native relational databases to meet the ever-increasing needs of scaling compute and memory resources independently and elastically. The recent emergence of memory disaggregation architecture, relying on high-speed RDMA network, offers opportunities to build cost-effective and elastic cloud-native databases. There exist proposals to let unmodified applications run transparently on disaggregated systems. However, running relational database kernel atop such proposals experiences notable performance degradation and time-consuming failure recovery, offsetting the benefits of disaggregation.To address these challenges, in this paper, we propose a novel database architecture called LegoBase, which explores the co-design of database kernel and memory disaggregation. It pushes the memory management back to the database layer for bypassing the Linux I/O stack and re-using or designing (remote) memory access optimizations with an understanding of data access patterns. LegoBase further splits the conventional ARIES fault tolerance protocol to independently handle the local and remote memory failures for fast recovery of compute instances. We implemented LegoBase atop MySQL. We compare LegoBase against MySQL running on a standalone machine and the state-of-the-art disaggregation proposal Infiniswap. Our evaluation shows that even with a large fraction of data placed on the remote memory, LegoBase's system performance in terms of throughput (up to 9.41% drop) and P99 latency (up to 11.58% increase) is comparable to the monolithic MySQL setup, and significantly outperforms (1.99x-2.33x, respectively) the deployment of MySQL over Infiniswap. Meanwhile, LegoBase introduces an up to 3.87x and 5.48x speedup of the recovery and warm-up time, respectively, over the monolithic MySQL and MySQL over Infiniswap, when handling failures or planned re-configurations.",
        "link": "https://dl.acm.org/doi/10.14778/3467861.3467877",
        "category": "Databases"
    },
    {
        "title": "Construction of Food Safety Inspection Subject Database Based on Improved Association Rule Mining Algorithm",
        "authors": "['Chengmei Zhang', 'Miao Hao', 'Bing Yang', 'Bin Su']",
        "date": "February 2022",
        "source": "DSDE '22: 2022 the 5th International Conference on Data Storage and Data Engineering",
        "abstract": "The frequent occurrence of food safety problems has seriously endangered people's health. Food safety has gradually become a major problem in China, and it has become a very urgent task to dig out the potential information of food safety and conduct corresponding analysis and judgment based on the mined information, so as to provide the basis for food safety testing by the regulatory authorities, prevent major food safety problems and strengthen food safety supervision. In this paper, we analyze the characteristics of current food safety data, improve the Apriori association rule algorithm by using Hash technology, and then realize the effective mining of food safety information data, and on this basis, we develop a food safety inspection subject database for enterprises and regulatory authorities. The experimental results show that the improved association rule algorithm-based food safety detection subject database can effectively identify key factors affecting food safety, and the computing efficiency is faster, thus providing an important reference for regulatory authorities to complete food safety testing, which is important for food safety supervision.",
        "link": "https://dl.acm.org/doi/10.1145/3528114.3528121",
        "category": "Databases"
    },
    {
        "title": "Secure Cloud Database based on Verification Procedure",
        "authors": "['Momoko Shiraishi']",
        "date": "November 2021",
        "source": "ICCBD 2021: 2021 4th International Conference on Computing and Big Data",
        "abstract": "More and more sensitive data represented by financial data connect with cyberspace. Cloud data management contributes to control the data efficiently. This paper shows a cloud database scheme that holds two verification mechanisms. One is verification whether the retrieved data corresponds to the intended data. Second is the retrieved is surely belongs to the data owner by confirming multi-signature on the data. The cloud does not guarantee the correctness of the returned data; therefore, the former feature is necessary. Besides, since multiple entities are related to data creation nowadays, the data user concern about the authenticity of the multi-data owners. Multi-signature on the cloud database solves this issue. We provide performance analysis and it is shown that the proposed scheme assures the verification, being realized with practical computation time.",
        "link": "https://dl.acm.org/doi/10.1145/3507524.3507538",
        "category": "Databases"
    },
    {
        "title": "Construction of Uyghur Scene Text Image Database",
        "authors": "['Yilihamu Aili', 'Yiwen Wang', 'Pei Liu', 'Alimujiang Abudiriyimu', 'Kurban Ubul']",
        "date": "October 2021",
        "source": "ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition",
        "abstract": "In recent years, the International Conference on Document Analysis and Recognition (ICDAR) has released a data set of multilingual text in natural scene images. The data set contains natural scene images in nine different languages, including Chinese, Arabic, Korean, etc. At present, it is rare to see a comprehensive data set of Uyghur scene text images in natural scene images. Therefore, the main purpose of this article is to create a new Uyghur text data set in natural scene images and provide it to the research community to develop and evaluate the latest Uyghur text recognition and detection algorithms for text retrieval and recognition. Images of natural scenes, segmented characters and composite images of scenes, and Uyghur words in the images are manually marked at the word level from the captured images. The data set includes 2381 Uyghur advertisement pictures, 17,638 video interceptions, 394 complex natural scenes that can be used for Uyghur text detection in complex scenes, and 200067 Uyghur text pictures that can be used for Uyghur text recognition. These pictures come from web crawlers, Uighur animated films and natural street scenes in Xinjiang, China. They include flat text, multi-angle text, raised 3D text, artistic text, distant text, low-light text, partially occluded text, etc. Due to the diversity of the collection environment and the complexity of the image background, the database can be used as a benchmark for Uyghur text detection and recognition in natural scene images, which poses great challenges.",
        "link": "https://dl.acm.org/doi/10.1145/3497623.3497660",
        "category": "Databases"
    },
    {
        "title": "SSstory: 3D data storytelling based on SuperSQL and Unity",
        "authors": "['Jingrui Li', 'Kento Goto', 'Motomichi Toyama']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "SuperSQL is an extended SQL language, which brings out a rich layout presentation of a relational database with a particular query. This paper proposes SSstory, a storytelling system in a 3D data space created by a relational database. SSstory uses SuperSQL and Unity to generate a data video and add cinematic directions to the data video. Without learning special authoring tooling, users can easily create data videos with a small quantity of code.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472277",
        "category": "Databases"
    },
    {
        "title": "Database Principles and Challenges in Text Analysis",
        "authors": "['Johannes Doleschal', 'Benny Kimelfeld', 'Wim Martens']",
        "date": "June 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "A common conceptual view of text analysis is that of a two-step process, where we first extract relations from text documents and then apply a relational query over the result. Hence, text analysis shares technical challenges with, and can draw ideas from, relational databases. A framework that formally instantiates this connection is that of the document spanners. In this article, we review recent advances in various research efforts that adapt fundamental database concepts to text analysis through the lens of document spanners. Among others, we discuss aspects of query evaluation, aggregate queries, provenance, and distributed query planning.",
        "link": "https://dl.acm.org/doi/10.1145/3484622.3484624",
        "category": "Databases"
    },
    {
        "title": "Hybrid blockchain database systems: design and performance",
        "authors": "['Zerui Ge', 'Dumitrel Loghin', 'Beng Chin Ooi', 'Pingcheng Ruan', 'Tianwen Wang']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "With the emergence of hybrid blockchain database systems, we aim to provide an in-depth analysis of the performance and trade-offs among a few representative systems. To achieve this goal, we implement Veritas and BlockchainDB from scratch. For Veritas, we provide two flavors to target the crash fault-tolerant (CFT) and Byzantine fault-tolerant (BFT) application scenarios. Specifically, we implement Veritas with Apache Kafka to target CFT application scenarios, and Veritas with Tendermint to target BFT application scenarios. We compare these three systems with the existing open-source implementation of BigchainDB. BigchainDB uses Tender-mint for consensus and provides two flavors: a default implementation with blockchain pipelining and an optimized version that includes blockchain pipelining and parallel transaction validation. Our experimental analysis confirms that CFT designs, which are typically used by distributed databases, exhibit much higher performance than BFT designs, which are specific to blockchains. On the other hand, our extensive analysis highlights the variety of design choices faced by the developers and sheds some light on the trade-offs that need to be done when designing a hybrid blockchain database system.",
        "link": "https://dl.acm.org/doi/10.14778/3510397.3510406",
        "category": "Databases"
    },
    {
        "title": "Side-Channel Attacks on Query-Based Data Anonymization",
        "authors": "['Franziska Boenisch', 'Reinhard Munz', 'Marcel Tiepelt', 'Simon Hanisch', 'Christiane Kuhn', 'Paul Francis']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "A longstanding problem in computer privacy is that of data anonymization. One common approach is to present a query interface to analysts, and anonymize on a query-by-query basis. In practice, this approach often uses a standard database back end, and presents the query semantics of the database to the analyst. This paper presents a class of novel side-channel attacks that work against any query-based anonymization system that uses a standard database back end. The attacks exploit the implicit conditional logic of database runtime optimizations. They manipulate this logic to trigger timing and exception-throwing side-channels based on the contents of the data. We demonstrate the attacks on the implementation of the CHORUS Differential Privacy system released by Uber as an open source project. We obtain perfect reconstruction of millions of data values even with a Differential Privacy budget smaller than epsilon = 1.0 and no prior knowledge. The paper also presents the design of a general defense to the runtime-optimization attacks, and a concrete implementation of the defense in the latest version of Diffix. The defense works without modifications to the back end database, and operates by modifying SQL to eliminate the runtime optimization or disable the side-channels. In addition, two other attacks that exploit specific flaws in Diffix and CHORUS are reported. These have been fixed in the respective implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484751",
        "category": "Databases"
    },
    {
        "title": "Repairing serializability bugs in distributed database programs via automated schema refactoring",
        "authors": "['Kia Rahmani', 'Kartik Nagar', 'Benjamin Delaware', 'Suresh Jagannathan']",
        "date": "June 2021",
        "source": "PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
        "abstract": "Serializability is a well-understood concurrency control mechanism that eases reasoning about highly-concurrent database programs. Unfortunately, enforcing serializability has a high performance cost, especially on geographically distributed database clusters. Consequently, many databases allow programmers to choose when a transaction must be executed under serializability, with the expectation that transactions would only be so marked when necessary to avoid serious concurrency bugs. However, this is a significant burden to impose on developers, requiring them to (a) reason about subtle concurrent interactions among potentially interfering transactions, (b) determine when such interactions would violate desired invariants, and (c) then identify the minimum number of transactions whose executions should be serialized to prevent these violations. To mitigate this burden, this paper presents a sound fully-automated schema refactoring procedure that refactors a program’s data layout – rather than its concurrency control logic – to eliminate statically identified concurrency bugs, allowing more transactions to be safely executed under weaker and more performant database guarantees. Experimental results over a range of realistic database benchmarks indicate that our approach is highly effective in eliminating concurrency bugs, with safe refactored programs showing an average of 120% higher throughput and 45% lower latency compared to a serialized baseline.",
        "link": "https://dl.acm.org/doi/10.1145/3453483.3454028",
        "category": "Databases"
    },
    {
        "title": "Database Tuning using Natural Language Processing",
        "authors": "['Immanuel Trummer']",
        "date": "September 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Introduction. We have seen significant advances in the state of the art in natural language processing (NLP) over the past few years [20]. These advances have been driven by new neural network architectures, in particular the Transformer model [19], as well as the successful application of transfer learning approaches to NLP [13]. Typically, training for specific NLP tasks starts from large language models that have been pre-trained on generic tasks (e.g., predicting obfuscated words in text [5]) for which large amounts of training data are available. Using such models as a starting point reduces task-specific training cost as well as the number of required training samples by orders of magnitude [7]. These advances motivate new use cases for NLP methods in the context of databases.",
        "link": "https://dl.acm.org/doi/10.1145/3503780.3503788",
        "category": "Databases"
    },
    {
        "title": "Upgrading of academic database: acceptance or resistance",
        "authors": "['Hanghua Lian', 'Zhaorong Huang', 'Jieying Zhou', 'Quande Qin']",
        "date": "September 2021",
        "source": "WSSE '21: Proceedings of the 3rd World Symposium on Software Engineering",
        "abstract": "This study examines the factors that influence people use intention of the upgraded Academic literature databases (ALD) based on the dual-factor concepts of “enablers” and “inhibitors”. This paper proposed a new model that considers the continuance intention from two pathways of acceptance and resistance and further integrates inertia and experience of new version. The results demonstrated the impact that inertia has on perceived loss of control and resistance intention as well as the impact that experience of new version and perceived usefulness on acceptance intention. It was also indicated that observability, compatibility and testability are the main factors of perceived usefulness, which in turn lead to subsequent consumer acceptance intention. Such consideration will help decrease the mismatch between ALD design and users’ needs, and further facilitate the adoption of the newer ALD. Furthermore, this study provides a universal database conceptual upgrade model in the field of information system upgrading and enriching knowledge management system research.",
        "link": "https://dl.acm.org/doi/10.1145/3488838.3488853",
        "category": "Databases"
    },
    {
        "title": "Always-on time-series database: keeping up where there's no way to catch up",
        "authors": "['Theo Schlossnagle', 'Justin Sheehy', 'Chris McCubbin']",
        "date": "July 2021",
        "source": "Communications of the ACM",
        "abstract": "A discussion with Theo Schlossnagle, Justin Sheehy, and Chris McCubbin.",
        "link": "https://dl.acm.org/doi/10.1145/3442518",
        "category": "Databases"
    },
    {
        "title": "Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions",
        "authors": "['Madiha Tahir', 'Zahid Halim', 'Atta Ur Rahman', 'Muhammad Waqas', 'Shanshan Tu', 'Sheng Chen', 'Zhu Han']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human–computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3480968",
        "category": "Databases"
    },
    {
        "title": "Research on the Construction Method of University Graduates Employment Quality Analysis Database",
        "authors": "['Chunlei Zhang', 'Guoxin Han', 'Shuang Wang']",
        "date": "February 2022",
        "source": "DSDE '22: 2022 the 5th International Conference on Data Storage and Data Engineering",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3528114.3528120",
        "category": "Databases"
    },
    {
        "title": "The Evolution of Hot Topics in the Field of Opportunism in Marketing Channels Based on the Visualization Analysis of WOS Core Database (1990-2019)",
        "authors": "['Hongqi Chen', 'Guoshun Liu', 'Hua Zhang']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "Opportunism has been a hot topic in marketing channel for long time. It is not conducive to channel harmony and performance. The development of 5G technology provides a realistic basis for big data analysis. In the data age, scholars have to face another challenge that how to reveal the research status of opportunism from comprehensive analysis of objective data, efficiently identify potential hot topics in the field from massive papers, track its evolution path and provide visual analysis. Using 365 papers indexed by WOS core databases with the theme of “opportunism” and “opportunistic behavior” published from 1990 to 2019 as the research object, and uses visualization software such as Citespace to draw authors, institution cooperation maps, keyword co-occurrence maps, etc. The study analyzes the hot topics and development trends in the field of marketing channel opportunism, and also point out the future development direction for marketing channel opportunism research.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470292",
        "category": "Databases"
    },
    {
        "title": "Establishment and Design of English Translation Memory and Terminology Database Based on Artificial Intelligence",
        "authors": "['Chen Chen']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "In the application field of professional translation and localization market, since the quality of computer translation basically meets actual needs, most of the translation products that can be sold in the market use computer-assisted translation technology. However, due to the complexity and variability of linguistic knowledge, some people have proposed translation memory, which makes full use of the powerful functions of files and databases to improve the reliability and reuse of existing translation materials. Therefore, in some professional translation technology applications with high data and repetitiveness, the use of this type of technology can effectively eliminate the double operation of translators, avoid multiple translation results of the same word or two words, and significantly improve its work efficiency. This article is based on artificial intelligence-based English translation memory and terminology establishment and design. First of all, it uses literature research to explain the functions of English translation memory technology and English translation screening methods. After analyzing the memory and terminology, according to analyzing the results of the design of the memory bank and the design library, this article locates the granularity of the memory bank at the simple sentence level, and establishes a correspondence between the various components of a sentence based on the establishment of a simple word-sentence alignment. The English sentence similarity algorithm based on syntax and semantics can well measure the structural and semantic similarity between two English sentences, and can correctly return the most similar example sentences in a memory bank. After we obtained the most similar example sentences, we translated them according to the corresponding relationship between the sentence to be translated and the various components of the most similar example sentences.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3483146",
        "category": "Databases"
    },
    {
        "title": "Analysis and Introduction of Graph Database",
        "authors": "['Xianyu Meng', 'Xiaoyan Cai', 'Yanping Cui']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495086",
        "category": "Databases"
    },
    {
        "title": "A study of database performance sensitivity to experiment settings",
        "authors": "['Yang Wang', 'Miao Yu', 'Yujie Hui', 'Fang Zhou', 'Yuyang Huang', 'Rui Zhu', 'Xueyuan Ren', 'Tianxi Li', 'Xiaoyi Lu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "To allow performance comparison across different systems, our community has developed multiple benchmarks, such as TPC-C and YCSB, which are widely used. However, despite such effort, interpreting and comparing performance numbers is still a challenging task, because one can tune benchmark parameters, system features, and hardware settings, which can lead to very different system behaviors. Such tuning creates a long-standing question of whether the conclusion of a work can hold under different settings.This work tries to shed light on this question by reproducing 11 works evaluated under TPC-C and YCSB, measuring their performance under a wider range of settings, and investigating the reasons for the change of performance numbers. By doing so, this paper tries to motivate the discussion about whether and how we should address this problem. While this paper does not give a complete solution---this is beyond the scope of a single paper, it proposes concrete suggestions we can take to improve the state of the art.",
        "link": "https://dl.acm.org/doi/10.14778/3523210.3523221",
        "category": "Databases"
    },
    {
        "title": "Research on Verifiable Access Technology of Mixed Database in Distributed System under Big Data",
        "authors": "['Kangle He']",
        "date": "December 2021",
        "source": "ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology",
        "abstract": "With the rapid development of Internet applications and the explosive growth of information data, the mixed database of distributed storage management has high requirements for verifiable access technology. The problem of poor application security efficiency in traditional technology is adopted, and the research on verifiable access technology of distributed system mixed database is proposed. According to the principle of verifiable access technology, the mixed database access model is analyzed. Through mixed database access model, a set of access allocation and use strategy can be provided for the distributed system. Configure the data source, design the structure of the access model service module, and complete the research on the application of the verifiable access technology of the mixed database in the distributed system according to the implementation process. By comparing the results of the experiment, we can see that the security efficiency of the verifiable access technology of the hybrid database is 90%, and the efficient utilization of the mixed database in the distributed system is realized.",
        "link": "https://dl.acm.org/doi/10.1145/3510858.3511346",
        "category": "Databases"
    },
    {
        "title": "Research on the design of large data storage structure of database based on Data Mining",
        "authors": "['Lihua Wang']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "With the continuous development of China's social economy, the informationization function of power grid companies is becoming increasingly powerful, and its effectiveness is very remarkable. However, in the related business of power system, the performance of database cannot be guaranteed. In the process of power marketing, a large amount of data will be generated. In the process of processing a large amount of data, it is necessary to constantly optimize the database to improve the structure of the database. Data mining is a promising subject frontier of database system and new database application. In this paper, a large data storage structure of database based on data mining is designed. The ETL tool in signaling data warehouse is used to realize the integrated design of signaling data storage and application.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3501223",
        "category": "Databases"
    },
    {
        "title": "On the Construction Path of Digital Teaching Resource Database under the Background of “Classroom Revolution”",
        "authors": "['Huannian Meng', 'Hua Fan', 'Cuiying Lin']",
        "date": "October 2021",
        "source": "ICETC '21: Proceedings of the 13th International Conference on Education Technology and Computers",
        "abstract": "Aiming at the contemporary problems in the construction and application of digital teaching resources in colleges and universities, this paper deeply analyzes the main reasons. Focusing on aims to improve the classroom teaching effect, this paper forms the framework of the “classroom revolution”, which puts forward the practical designs of promoting the construction of resource database with performance evaluation. The solution presents the specific path of the construction of digital teaching resource database, urges students to “return to the acquisition of basic knowledge”, teachers “return to their duties” and improve the quality of talent cultivation.",
        "link": "https://dl.acm.org/doi/10.1145/3498765.3498806",
        "category": "Databases"
    },
    {
        "title": "Robust and budget-constrained encoding configurations for in-memory database systems",
        "authors": "['Martin Boissier']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Data encoding has been applied to database systems for decades as it mitigates bandwidth bottlenecks and reduces storage requirements. But even in the presence of these advantages, most in-memory database systems use data encoding only conservatively as the negative impact on runtime performance can be severe. Real-world systems with large parts being infrequently accessed and cost efficiency constraints in cloud environments require solutions that automatically and efficiently select encoding techniques, including heavy-weight compression. In this paper, we introduce workload-driven approaches to automaticaly determine memory budget-constrained encoding configurations using greedy heuristics and linear programming. We show for TPC-H, TPC-DS, and the Join Order Benchmark that optimized encoding configurations can reduce the main memory footprint significantly without a loss in runtime performance over state-of-the-art dictionary encoding. To yield robust selections, we extend the linear programming-based approach to incorporate query runtime constraints and mitigate unexpected performance regressions.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503588",
        "category": "Databases"
    },
    {
        "title": "Learned Buffer Replacement for Database Systems",
        "authors": "['Yigui Yuan', 'Peiquan Jin']",
        "date": "February 2022",
        "source": "DSDE '22: 2022 the 5th International Conference on Data Storage and Data Engineering",
        "abstract": "Most current database buffering schemes adopt an empirical design, which cannot adapt to the change of workloads. In this paper, we show how we can use machine learning to help design a new buffer replacement policy for database systems. We name the new policy LBR (Learned Buffer Replacement). The key idea of LBR is to use machine learning models to periodically learn the access pattern from historical requests to make the buffer replacement adaptive to the workload change. Particularly, we present two ways to learn the access pattern. One is a classifier named LBR-c, which can distinguish hot pages from cold ones based on the training on historical requests; the other is a regressor called LBR-r, which can predict the future replacement behavior according to historical accesses. We implement the proposed LBR-c and LBR-r and compare them to a number of existing schemes, including the theoretically optimal Belady's algorithm, three traditional algorithms (LRU, 2Q, and ARC), and LeCaR, which is a recently-proposed adaptive buffer scheme. The results show that our algorithms achieve a higher hit ratio than LRU, ARC, 2Q, and LeCaR. In addition, both LBR-c and LBR-r can adapt to workload changes, which is better than LRU, 2Q, ARC, and LeCaR. Overall, our proposal achieves comparable performance with the optimal buffer replacement algorithm, advancing the state-of-the-art in the well-studied area of buffer management in DBMSs.",
        "link": "https://dl.acm.org/doi/10.1145/3528114.3528118",
        "category": "Databases"
    },
    {
        "title": "Encoding, Analysing and Modeling I-Folk: A New Database of Iberian Folk Music",
        "authors": "['Nádia Carvalho', 'Sara Gonzalez-Gutierrez', 'Javier Merchan Sanchez-Jara', 'Gilberto Bernardes', 'Maria Navarro-Cáceres']",
        "date": "July 2021",
        "source": "DLfM '21: Proceedings of the 8th International Conference on Digital Libraries for Musicology",
        "abstract": "Folk music is a fundamental immaterial heritage that promotes cultural identity. However, it lacks a substantial body of open access materials, and its promotion has been disconnected from the education curriculum. In this context, facilitated access to annotated high-quality folk music content can promote better educational tools and enhance cultural heritage literacy. Based on this, we advance and detail three main contributions: 1) a standardized model to musically annotate Iberian folk music; 2) a new database, named I-Folk, with annotated files following the proposed model; and 3) tools for navigating and retrieving folk music contents from the database. A particular emphasis is given to the educational application of the proposed model, contents, and tools in education. Ultimately, we strive for the promotion of Iberian folk music to the educators’ community.",
        "link": "https://dl.acm.org/doi/10.1145/3469013.3469023",
        "category": "Databases"
    },
    {
        "title": "Regular Sequential Serializability and Regular Sequential Consistency",
        "authors": "['Jeffrey Helt', 'Matthew Burke', 'Amit Levy', 'Wyatt Lloyd']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Strictly serializable (linearizable) services appear to execute transactions (operations) sequentially, in an order consistent with real time. This restricts a transaction's (operation's) possible return values and in turn, simplifies application programming. In exchange, strictly serializable (linearizable) services perform worse than those with weaker consistency. But switching to such services can break applications. This work introduces two new consistency models to ease this trade-off: regular sequential serializability (RSS) and regular sequential consistency (RSC). They are just as strong for applications: we prove any application invariant that holds when using a strictly serializable (linearizable) service also holds when using an RSS (RSC) service. Yet they relax the constraints on services---they allow new, better-performing designs. To demonstrate this, we design, implement, and evaluate variants of two systems, Spanner and Gryff, relaxing their consistency to RSS and RSC, respectively. The new variants achieve better read-only transaction and read tail latency than their counterparts.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483566",
        "category": "Databases"
    },
    {
        "title": "Design of Smart Washing Platform through Mysql Database and Intelligent Computing",
        "authors": "['Yu Zhang']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "This paper proposes the design of an online intelligent washing platform based on the background of the Internet era, the habit of online consumption and the gradual development of the washing industry, which relies on Mysql database to save and query the information of users and orders, and uses B/S architecture to make the platform have good cross-platform characteristics. The two subsystems, user subsystem and management subsystem, are designed in detail. Finally, a summary and outlook on the smart washing industry are given.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3501081",
        "category": "Databases"
    },
    {
        "title": "DORMAN: Database of Reconstructed MetAbolic Networks",
        "authors": "['Furkan Ozden', 'Metin Can Siper', 'Necmi Acarsoy', 'Tugrulcan Elmas', 'Bryan Marty', 'Xinjian Qi', 'A. Ercument Cicek']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Genome-scale reconstructed metabolic networks have provided an organism specific understanding of cellular processes and their relations to phenotype. As they are deemed essential to study metabolism, the number of organisms with reconstructed metabolic networks continues to increase. This everlasting research interest lead to the development of online systems/repositories that store existing reconstructions and enable new model generation, integration, and constraint-based analyses. While features that support model reconstruction are widely available, current systems lack the means to help users who are interested in analyzing the topology of the reconstructed networks. Here, we present the Database of Reconstructed Metabolic Networks - DORMAN. DORMAN is a centralized online database that stores SBML-based reconstructed metabolic networks published in the literature, and provides web-based computational tools for visualizing and analyzing the model topology. Novel features of DORMAN are (i) interactive visualization interface that allows rendering of the complete network as well as editing and exporting the model, (ii) hierarchical navigation that provides efficient access to connected entities in the model, (iii) built-in query interface that allow posing topological queries, and finally, and (iv) model comparison tool that enables comparing models with different nomenclatures, using approximate string matching. DORMAN is online and freely accessible at <uri>http://ciceklab.cs.bilkent.edu.tr/dorman</uri>.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2019.2944905",
        "category": "Databases"
    },
    {
        "title": "Research on Key Technologies of 3D Model Database Based on Network Platform",
        "authors": "['Ye Tao', 'Miaorui Song']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "Due to the low efficiency of traditional manufacturing industry design, it cannot achieve reuse and resource sharing, it is necessary to design 3D model database based on network platform. ISO 13584 (parts library, PLIB) standard can transfer the data of supplier library. According to plib-42 standard, the part library is constructed, and the step method is used to share and exchange the part library information, through the network interface and Internet, we can build a web-based shared resource library. According to the geometric view expression technology, the interface of Pro / E, CATIA and inventor is developed. This paper starts with the technical research and engineering application, the construction of part library and the development of geometric programming interface are introduced in detail, the format and development method of geometry program file are proposed, Completed the standard parts model library system. Provide design resources for modular design system to realize the information integration and network collaborative design of part resources, It is of great significance to improve the speed of mechanical product design and development.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495497",
        "category": "Databases"
    },
    {
        "title": "Research on Application of Computer Technology in Lacquerware Knowledge Resource Database",
        "authors": "['Xinhui Lu', 'Xueying Huang']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "In the context of digitalization, the lacquerware of Yi nationality with local characteristics needs to use computer-related technology to burst out new cores.Aiming at the problem that the resource database based on humanities such as history and archaeology cannot meet the demand for lacquerware resources and knowledge in the field of cultural creative design. This research starts from the design and introduces the concept of ontology through the general creative design process. After deconstruction and reconstruction, the knowledge of lacquerware objects will be processed, stored and applied through computers, so as to construct a multi-level information resource framework for modern creation and design, and build a dialogue and cooperation platform for multi-party professionals.The designer uses this resource frame directly through the computer to obtain information resources of Yi nationality lacquerware that meets the design needs. Finally, it is verified by application that the resource framework can assist in the creation of contemporary design language and the cultural creation and design of Yi nationality lacquerware.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3501178",
        "category": "Databases"
    },
    {
        "title": "Make your database system dream of electric sheep: towards self-driving operation",
        "authors": "['Andrew Pavlo', 'Matthew Butrovich', 'Lin Ma', 'Prashanth Menon', 'Wan Shen Lim', 'Dana Van Aken', 'William Zhang']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Database management systems (DBMSs) are notoriously difficult to deploy and administer. Self-driving DBMSs seek to remove these impediments by managing themselves automatically. Despite decades of DBMS auto-tuning research, a truly autonomous, self-driving DBMS is yet to come. But recent advancements in artificial intelligence and machine learning (ML) have moved this goal closer.Given this, we present a system implementation treatise towards achieving a self-driving DBMS. We first provide an overview of the NoisePage self-driving DBMS that uses ML to predict the DBMS's behavior and optimize itself without human support or guidance. The system's architecture has three main ML-based components: (1) workload forecasting, (2) behavior modeling, and (3) action planning. We then describe the system design principles to facilitate holistic autonomous operations. Such prescripts reduce the complexity of the problem, thereby enabling a DBMS to converge to a better and more stable configuration more quickly.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476411",
        "category": "Databases"
    },
    {
        "title": "The next 50 years in database indexing or: the case for automatically generated index structures",
        "authors": "['Jens Dittrich', 'Joris Nix', 'Christian Schön']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Index structures are a building block of query processing and computer science in general. Since the dawn of computer technology there have been index structures. And since then, a myriad of index structures are being invented and published each and every year.In this paper we argue that the very idea of \"inventing an index\" is a misleading concept in the first place. It is the analogue of \"inventing a physical query plan\". This paper is a paradigm shift in which we propose to drop the idea to handcraft index structures (as done for binary search trees over B-trees to any form of learned index) altogether. We present a new automatic index breeding framework coined Genetic Generic Generation of Index Structures (GENE). It is based on the observation that almost all index structures are assembled along three principal dimensions: (1) structural building blocks, e.g., a B-tree is assembled from two different structural node types (inner and leaf nodes), (2) a couple of invariants, e.g., for a B-tree all paths have the same length, and (3) decisions on the internal layout of nodes (row or column layout, etc.). We propose a generic indexing framework that can mimic many existing index structures along those dimensions. Based on that framework we propose a generic genetic index generation algorithm that, given a workload and an optimization goal, can automatically assemble and mutate, in other words 'breed' new index structure 'species'. In our experiments we follow multiple goals. We reexamine some good old wisdom from database technology. Given a specific workload, will GENE even breed an index that is equivalent to what our textbooks and papers currently recommend for such a workload? Or can we do even more? Our initial results strongly indicate that generated indexes are the next step in designing index structures.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494136",
        "category": "Databases"
    },
    {
        "title": "An Investigational Approach for Vowels of the Salar Language Based on a Database of Speech Acoustic Parameters",
        "authors": "['Jun Ma', 'Hongzhi Yu', 'Yan Xu', 'Kaiying Deng']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "According to relevant specifications, this article divides, marks, and extracts the acquired speech signals of the Salar language, and establishes the speech acoustic parameter database of the Salar language. Then, the vowels of the Salar language are analyzed and studied by using the parameter database. The vowel bitmap (average value at the beginning of words), the vowel bitmap (average value at the abdomen of words), the vowel bitmap (average value at the ending of words), and the vowel bitmap (average value) are obtained. Through the vowel bitmaps, we can observe the vowel in different positions of the word, the overall appearance of an obtuse triangle. The high vowel [i], [o], and low vowel [a] occupy three vertices, respectively. Among the three lines, [i] to [o] are the longest, [i] to [a] are the second longest, and [a] to [o] are the shortest. The lines between [a] to [o] and [a] and [i] are asymmetric. Combining with the vowel bitmap, the vowels were discretized, and the second formant (F2) frequency parameter was used as the coordinate of the X axis, and the first formant (F1) frequency was used as the coordinate of the Y axis to draw the region where the vowel was located, and then the vowel pattern was formed. These studies provide basic data and parameters for the future development of modern phonetics such as the database of Sarah language speech, speech recognition, and speech synthesis. It also provides the basic parameters of speech acoustics for the rare minority acoustic research work of the national language project.",
        "link": "https://dl.acm.org/doi/10.1145/3459927",
        "category": "Databases"
    },
    {
        "title": "A Shared E-Learning Resources Database Using Big Data and Cloud Environment",
        "authors": "['Shudong Jiang', 'Jiagui Xie']",
        "date": "July 2021",
        "source": "ICEME '21: Proceedings of the 2021 12th International Conference on E-business, Management and Economics",
        "abstract": "The world has entered into the era of Big Data unwittingly. Although no universally accepted definition can be given as of now, the term “Big Data” is commonly referred to as extensive datasets, primarily in the characteristics of 5 Vs: value, variability, variety, velocity, and volume, which require scalable architectures for efficient storage, manipulation, and analysis [1]. Higher education institutions are encouraged to construct a shared E-learning resources database based on Big Data and Cloud because of its convenience and cost-savings to E-learning teachers in terms of data crawling, storage, analysis, as well as data processing, optimization and sharing [2]. This article introduces the conceptual underlying frameworks for developing a shared E-learning resources database based on Big Data and Cloud as well as its prospective benefits to teachers, students, and university-corporation connections. Its prospective application holds a vision to assist associated teachers for improved efficiency and effectiveness of data acquisition, teaching material preparation and processing, user (students) satisfaction and prominent university-corporation connections.",
        "link": "https://dl.acm.org/doi/10.1145/3481127.3481215",
        "category": "Databases"
    },
    {
        "title": "A Sequence-to-Sequence Model for Large-scale Chinese Abbreviation Database Construction",
        "authors": "['Chao Wang', 'Jingping Liu', 'Tianyi Zhuang', 'Jiahang Li', 'Juntao Liu', 'Yanghua Xiao', 'Wei Wang', 'Rui Xie']",
        "date": "February 2022",
        "source": "WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
        "abstract": "Abbreviations often used in our daily communication play an important role in natural language processing. Most of the existing studies regard the Chinese abbreviation prediction as a sequence labeling problem. However, sequence labeling models usually ignore label dependencies in the process of abbreviation prediction, and the label prediction of each character should be conditioned on its previous labels. In this paper, we propose to formalize the Chinese abbreviation prediction task as a sequence generation problem, and a novel sequence-to-sequence model is designed. To boost the performance of our deep model, we further propose a multi-level pre-trained model that incorporates character, word, and concept-level embeddings. To evaluate our methods, a new dataset for Chinese abbreviation prediction is automatically built, which contains 81,351 pairs of full forms and abbreviations. Finally, we conduct extensive experiments on a public dataset and the built dataset, and the experimental results on both datasets show that our model outperforms the state-of-the-art methods. More importantly, we build a large-scale database for a specific domain, i.e., life services in Meituan Inc., with high accuracy of about 82.7%, which contains 4,134,142 pairs of full forms and abbreviations. The online A/B testing on Meituan APP and Dianping APP suggests that Click-Through Rate increases by 0.59% and 0.86% respectively when the built database is used in the searching system. We have released our API on http://kw.fudan.edu.cn/ddemos/abbr/ with over 87k API calls in 9 months.",
        "link": "https://dl.acm.org/doi/10.1145/3488560.3498430",
        "category": "Databases"
    },
    {
        "title": "Resource-Efficient Database Query Processing on FPGAs",
        "authors": "['Mehdi Moghaddamfar', 'Christian Färber', 'Wolfgang Lehner', 'Norman May', 'Akash Kumar']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "FPGA technology has introduced new ways to accelerate database query processing, that often result in higher performance and energy efficiency. This is thanks to the unique architecture of FPGAs using reconfigurable resources to behave like an application-specific integrated circuit upon programming. The limited amount of these resources restricts the number and type of modules that an FPGA can simultaneously support. In this paper, we propose \"morphing sort-merge\": a set of run-time configurable FPGA modules that achieves resource efficiency by reusing the FPGA's resources to support different pipeline-breaking database operators, namely sort, aggregation, and equi-join. The proposed modules use dynamic optimization mechanisms that adapt the implementation to the distribution of data at run-time, thus resulting in higher performance. Our benchmarks show that morphing sort-merge reaches an average speedup of 5x compared to MonetDB.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466006",
        "category": "Databases"
    },
    {
        "title": "Thromboembolism Adverse Events Caused by Immune Checkpoint Inhibitor Therapy: A Disproportionality Study from 2010 to 2020 based on FAERS Database",
        "authors": "['Xin-Yu Liu']",
        "date": "January 2022",
        "source": "ICBBB '22: Proceedings of the 2022 12th International Conference on Bioscience, Biochemistry and Bioinformatics",
        "abstract": "Purpose: We aimed to assess the association between immune checkpoint inhibitors (ICIs) and thromboembolism adverse events. Methods: We extracted data from the US FDA Adverse Event Reporting System Database and conducted disproportionality analysis. Proportional reporting ratio (PRR) and information component (IC) were used to detect potential signals. Results: In the FAERS database, we identified 2675 ICI-mediated thromboembolism (TE) cases from 2010Q1 to 2020Q3. Significant signals were detected in venous thromboembolism (VTE) (PRR 1.493 [95% CI (Confidence Interval) 1.407-1.584]; IC025 0.490) and pulmonary embolism (PRR 1.743 [95% CI 1.630-1.865]; IC025 0.701). Conclusion: ICIs were found significant associations with overall VTE, especially pulmonary embolism, but they had no connection with arterial thromboembolism (ATE).",
        "link": "https://dl.acm.org/doi/10.1145/3510427.3510434",
        "category": "Databases"
    },
    {
        "title": "Joy Arulraj Speaks Out on Non- Volatile Memory Database Systems",
        "authors": "['Marianne Winslett', 'Vanessa Braganholo']",
        "date": "September 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Welcome to this installment of the ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today I have here with me Joy Arulraj, who won the 2019 ACM SIGMOD Jim Gray Dissertation Award for his thesis entitled The Design and Implementation of Non-volatile Memory Database Management Systems. Joy is now an Assistant Professor at Georgia Tech, and his PhD is from the Carnegie Mellon University, where he worked with Andy Pavlo, who won this same award in his time. So, Joy, welcome!",
        "link": "https://dl.acm.org/doi/10.1145/3503780.3503790",
        "category": "Databases"
    },
    {
        "title": "Research on Programming Technology of Computer Software Engineering Database Based on Multi-platform",
        "authors": "['Jiaxiang Zheng']",
        "date": "December 2021",
        "source": "ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology",
        "abstract": "For computer software engineering, the important technical means of database programming should run through the whole process of safe operation of software engineering, aiming at using professional programming technology principles to edit database running programs, and then achieving the goal of ensuring the integrity and safety of database storage information. With the development of mobile computing technology, it appears or shows wide applications in many fields. Such as public information publishing, location-related query, mobile commerce, etc., all depend on the support of mobile database system. In order to improve the efficiency of computer software development and reduce the probability of software vulnerabilities and running errors, technicians are required to take necessary test methods in the process of development and design to realize the summary analysis of computer software information data. This paper expounds the existence value of computer database, analyzes the process of establishing computer database, and studies the database programming technology of computer software engineering based on multi-platform testing.",
        "link": "https://dl.acm.org/doi/10.1145/3510858.3511339",
        "category": "Databases"
    },
    {
        "title": "Database Theory Column Report on PODS 2021",
        "authors": "['Reinhard Pichler']",
        "date": "September 2021",
        "source": "ACM SIGACT News",
        "abstract": "The 40th edition of the ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS) was held from June 20 to June 25, 2021, in Xi'an, Shaanxi, China. Due to the COVID-19 pandemic, it was organized in hybrid mode, with a local event primarily targeting the Chinese data management community and as a virtual (on- line) conference for the international community. As in previous years, the symposium was held jointly with the ACM International Conference on Management of Data (SIGMOD). PODS focuses on theoretical aspects of data management systems, and the co-location with SIGMOD stimulates interaction between theory-oriented and system-oriented research.",
        "link": "https://dl.acm.org/doi/10.1145/3494656.3494669",
        "category": "Databases"
    },
    {
        "title": "Real-Time Spatial-Temporal Database for Geographic Polygon Data Using HD-Map",
        "authors": "['Atsushi Isomura', 'Kazuhiro Miyahara', 'Ichibe Naito', 'Masayuki Hanadate']",
        "date": "April 2021",
        "source": "ICGDA '21: Proceedings of the 2021 4th International Conference on Geoinformatics and Data Analysis",
        "abstract": "Recently, real-time geometric data retrieval is the key technology for real-time mobility services to extract spatial-temporal data of moving objects (e.g. self-driving car, connected car and people using augmented reality) inside a static area (e.g. roads, residential area). Also, the high-definition map provides the high precise geographic polygon data on the static area. In general, the system of the real-time geometric data retrieval consists in relational database managing geographic polygon data, key-value store managing spatial-temporal data of the moving objects, and points-in-polygon algorithm for the extraction described in above. However, the system to apply high-definition map for geographic polygon data has two problems; First, too much vertices of high-definition map deteriorates the performance of the points-in-polygon algorithm. Secondly, a user cannot give an appropriate search query since the user does not know the exact shape of geographic polygon data stored in SDB beforehand, only knows the approximate location of it. To solve these two problems, this paper proposes two technologies; (1) reduction of vertices of high-definition map using Douglas-Peucker algorithm, and (2) PG-code, geo code for indexing of geographic polygon data. As the result, evaluation of the system implementing these proposals confirmed that our system can satisfy the requirements of real-time mobility services.",
        "link": "https://dl.acm.org/doi/10.1145/3465222.3465230",
        "category": "Databases"
    },
    {
        "title": "A neural database for differentially private spatial range queries",
        "authors": "['Sepanta Zeighami', 'Ritesh Ahuja', 'Gabriel Ghinita', 'Cyrus Shahabi']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Mobile apps and location-based services generate large amounts of location data. Location density information from such datasets benefits research on traffic optimization, context-aware notifications and public health (e.g., disease spread). To preserve individual privacy, one must sanitize location data, which is commonly done using differential privacy (DP). Existing methods partition the data domain into bins, add noise to each bin and publish a noisy histogram of the data. However, such simplistic modelling choices fall short of accurately capturing the useful density information in spatial datasets and yield poor accuracy. We propose a machine-learning based approach for answering range count queries on location data with DP guarantees. We focus on countering the sources of error that plague existing approaches (i.e., noise and uniformity error) through learning, and we design a neural database system that models spatial data such that density features are preserved, even when DP-compliant noise is added. We also devise a framework for effective system parameter tuning on top of public data, which helps set important system parameters without expending scarce privacy budget. Extensive experimental results on real datasets with heterogeneous characteristics show that our proposed approach significantly outperforms the state of the art.",
        "link": "https://dl.acm.org/doi/10.14778/3510397.3510404",
        "category": "Databases"
    },
    {
        "title": "Practice of Flipped Classroom Teaching Mode of Computer Course – Taking Access Database as an Example",
        "authors": "['Bin Meng']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "The establishment of applied talents cultivation mode has become the focus of the development of private colleges and universities, and the reform of teaching mode has also become an important aspect of the transformation of applied talents cultivation. In view of the disadvantages of the traditional classroom teaching mode, the reform is inevitable. The flipped classroom teaching mode provides new ideas and methods for the cultivation of Applied Talents in Colleges and universities. This paper takes Access Database as the research object to study the teaching mode of flipped classroom, and through teaching practice, compares the teaching effect of traditional teaching methods and flipped classroom. Practice has proved that the flipped classroom teaching mode can stimulate students' interest in learning, improve their autonomous learning ability, and improve their understanding and mastery of the course content.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3483139",
        "category": "Databases"
    },
    {
        "title": "The BIR database – Identifying typographic emphasis in list-like historical documents",
        "authors": "['Anna Scius-Bertrand', 'Simon Gabay', 'Juliette Janes', 'Ljudmila Petkovic', 'Caroline Corbieres', 'Thibault Clerice']",
        "date": "September 2021",
        "source": "HIP '21: Proceedings of the 6th International Workshop on Historical Document Imaging and Processing",
        "abstract": "Layout analysis and optical character recognition have become traditional tasks for processing historical prints, but are now insufficient. Additional information is found in typographic emphasis, such as bold and italic letters. They carry semantic meaning (titles, emphasis...) and also outline the structure of the page (entries, sub-parts...). Retrieving such data is therefore crucial for information extraction and automatic document structuring. In this paper, we introduce the Bold-Italic-Regular (BIR) database, which contains 285 pages of scanned, list-like historical prints that have been annotated at word level with bold and italic emphasis. Baseline results are provided for word detection and style classification using state-of-the-art deep neural network models, highlighting promising possibilities, such as near-human performance for isolated word classification, but also demonstrating limitations for the task at hand.",
        "link": "https://dl.acm.org/doi/10.1145/3476887.3476913",
        "category": "Databases"
    },
    {
        "title": "Research and Design of Piano Automatic Accompaniment System Based on Sound Database",
        "authors": "['Cuiying Xia']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "The purpose of this paper is to design an automatic piano accompaniment system. It takes a series of samples with piano accompaniment PuLi for input, through the training phase, the system collects original piano accompaniment figurations and converts to figure yuan and store it in the database structure. While adjusting a certain parameter, it forms a certain accompaniment style of the input sample.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3484123",
        "category": "Databases"
    },
    {
        "title": "Research on Master-Slave Database Management and Scheduling Algorithm of Distance Education Based on Streaming Media Technology",
        "authors": "['Qian Wang', 'Yanxia Wang']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "At present, the application of streaming media (SM) technology and network technology in distance education is a hot issue. Because distance education, especially distance multimedia teaching, involves the transmission of audio and video on the network, and the current network situation is not ideal for the transmission of audio and video streams, therefore, solving the problem of master-slave database management and scheduling algorithm in distance education is the key research content of this paper. Multi-level feedback queues are used to store different types of data, and the idea of dynamic buffer partition is proposed. In different playing stages, the node buffer is divided into different parts. In the initial playing stage of the node, the data to be played is cached in the emergency area to reduce the buffering time. The data scheduling path loss model controlled by SM network communication cost matrix is constructed, and the algorithm is improved. Experimental results show that the model can effectively reduce the scheduling path loss of SM data, optimize SM data transmission, and finally achieve the minimum transmission delay and avoid scheduling delay and distortion.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3487535",
        "category": "Databases"
    },
    {
        "title": "Research on the Effectiveness of Piano Hierarchical Teaching for Preschool Education Majors Based on Musical Database",
        "authors": "['Yan Zhao', 'Qing Zheng']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "Piano course is a compulsory art course for preschool teachers in China, and the teaching effect of this course determines teachers' professional ability. However, there are still many problems in teaching system and teaching methods in preschool teachers' teaching in China, which need to be further improved. Because of the poor music foundation and foundation of vocational college students, the collective teaching in a big pot often ignores the individual differences of students, which is not conducive to teachers teaching students in accordance with their aptitude. In order to cultivate students' piano playing ability in an all-round way, the teaching method of piano layered teaching came into being. Based on the musical database, this paper discusses the scientific countermeasures of actively developing hierarchical teaching in this process. It is of great practical significance to comprehensively improve the effectiveness of collective curriculum teaching and improve the teaching quality and level.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3482706",
        "category": "Databases"
    },
    {
        "title": "Data Mining in Education: Discussing Knowledge Discovery in Database (KDD) with Cluster Associative Study",
        "authors": "['Abdulkadir Abdulahi Hasan', 'Huan Fang']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "Data Mining, being an emerging field has ties with ever-increasing knowledge base, contributing to historical data trends and patterns as well as providing evidence to predict future behaviors. With the adoption of data mining techniques and their thorough implications, the knowledge base could be enhanced and a futuristic strategic management tool could be formulated in the form of modern data mining techniques. The technique and terminology used in the paper involves extracting knowledge from large datasets. In this paper, cluster sampling associated with data mining is used to dig out a beneficial way of tackling data mining steps. With these combined data mining efforts, the aim of highlighting the significance of data mining could be amalgamated with data warehouses and the steps that need to involve selecting, transforming, mining, and output evaluation to get beneficial or desired results. Furthermore, a mild emphasize is put on the cluster sampling data mining technique, steps, and its implication to enhance knowledge, explore new terms, and broaden our output scenarios so that there is a better understanding of how to move along the data mining channel. The objective of this study is to explain the basics of data mining and provide a way to further this research over data mining with cluster sampling and Knowledge discovery is data bases (KDD). For the said purpose, two clusters of the population of China are used in this study. One of which includes the percentage of students enrolled in the primary schools; and the other includes percentage of students enrolled in secondary schools in China. The steps of cluster sampling technique are followed to give a better understanding of the data mining approach.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3471319",
        "category": "Databases"
    },
    {
        "title": "Database systems research in the Arab world: a tradition that spans decades",
        "authors": "['Ashraf Aboulnaga', 'Azza Abouzied', 'Karima Echihabi', 'Mourad Ouzzani']",
        "date": "April 2021",
        "source": "Communications of the ACM",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3447750",
        "category": "Databases"
    },
    {
        "title": "Design and Implementation of Digital Database of Government Finance System Based on Net Framework",
        "authors": "['Zhenxun Tian']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3484088",
        "category": "Databases"
    },
    {
        "title": "MiniDB: A Teaching Oriented Lightweight Database",
        "authors": "['Hongwei Zhou', 'Keda Kang', 'Zhang Yuchen', 'Yuan Jinhui']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470334",
        "category": "Databases"
    },
    {
        "title": "Research on mechanical optimization design software based on database and configuration software",
        "authors": "[\"Li'e Yang\"]",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3482759",
        "category": "Databases"
    },
    {
        "title": "CuART - a CUDA-based, scalable Radix-Tree lookup and update engine",
        "authors": "['Martin Koppehel', 'Tobias Groth', 'Sven Groppe', 'Thilo Pionteck']",
        "date": "August 2021",
        "source": "ICPP '21: Proceedings of the 50th International Conference on Parallel Processing",
        "abstract": "In this work we present an optimized version of the Adaptive Radix Tree (ART) index structure for GPUs. We analyze an existing GPU implementation of ART (GRT), identify bottlenecks and present an optimized data structure and layout to improve the lookup and update performance. We show that our implementation outperforms the existing approach by a factor up to 2 times for lookups and up to 10 times for updates using the same GPU. We also show that the sequential memory layout presented here is beneficial for lookup-intensive workloads on the CPU, outperforming the ART by up to 10 times. We analyze the impact of the memory architecture of the GPU, where it becomes visible that traditional GDDR6(X) is beneficial for the index lookups due to the faster clock rates compared to High Bandwidth Memory (HBM).",
        "link": "https://dl.acm.org/doi/10.1145/3472456.3472511",
        "category": "Databases"
    },
    {
        "title": "Automated code refactoring upon database-schema changes in web applications",
        "authors": "['Sophie Xie', 'Junwen Yang', 'Shan Lu']",
        "date": "November 2021",
        "source": "ASE '21: Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering",
        "abstract": "Modern web applications manipulate a large amount of user data and undergo frequent data-schema changes. These changes bring up a unique refactoring task: updating application code to be consistent with data schema. Previous study and our own investigation show that this type of refactoring is error-prone and time-consuming for developers. This paper presents EvolutionSaver, a static code analysis and transformation tool that automates schema-related code refactoring and consistency checking. EvolutionSaver is implemented as an IDE plugin that works for both Rails and Django applications. The source code of EvolutionSaver is available on Github [1] and the plugin can be downloaded from Visual Studio Marketplace [2], with its tutorial available at https://www.youtube.com/watch?v=qBiMkLFIjbE and DOI 10.5281/zenodo.5276127.",
        "link": "https://dl.acm.org/doi/10.1109/ASE51524.2021.9678934",
        "category": "Databases"
    },
    {
        "title": "A Review of Rural Basic Education Research in the CSSCI Database: Visualization Analysis of Knowledge Graph",
        "authors": "['Xiaoxue Song', 'Hong Li', 'Minna Liu']",
        "date": "May 2021",
        "source": "ICDEL '21: Proceedings of the 2021 6th International Conference on Distance Education and Learning",
        "abstract": "Abstract: Rural basic education plays an important role in China's modernization construction. This paper examined the publications related to rural basic education in academic journals from CSSCI between 1998 and 2018. Using the bibliometrics methods, this paper analyzed the annual distribution, journals, authors, institutions, as well as high-frequency keywords and emergent keywords through CiteSpace and Excel. Results showed that the field of rural basic education continues to be a research hotspot for scholars. In spite of a large number of publications, most of the authors were affiliated with colleges and universities, and few researchers were directly involved in educational administration and instruction in primary education. The collaborations across institutions were few and far between. The research hotspots primarily focused on school layout, educational equity, educational informatization, curriculum reform and educational finance. The research frontiers mainly concentrated on the informatization of rural basic education, local normal universities, urbanization, and educational poverty. With the rural revitalization strategy putting forward at the 19th national congress of the CPC, rural basic education research will attract more attention. The results provide implications in diversifying the directions of rural basic education, teacher training which focuses on local educational features and needs, developing responsibilities and accountability of rural teachers, and broadening the rural educational environment.",
        "link": "https://dl.acm.org/doi/10.1145/3474995.3475015",
        "category": "Databases"
    },
    {
        "title": "”Why are they all obsessed with Gender?” — (Non)binary Navigations through Technological Infrastructures",
        "authors": "['Katta Spiel']",
        "date": "June 2021",
        "source": "DIS '21: Proceedings of the 2021 ACM Designing Interactive Systems Conference",
        "abstract": "Gender is encoded in multiple technological infrastructures, most prominently in digital forms across educational, commercial, medical and governmental contexts. To illustrate the pervasiveness of (binary) gender ideologies and the impact this can have on non-binary individuals – like me – encountering them, I conducted an autoethnography. For more than a year, starting with me receiving a legal non-binary status, I documented systems that did not allow me to register my gender correctly. The findings indicate how technological infrastructures predominantly encode gender as a fixed, immutable and static binary variable with limited options for non-binary people to adequately register self-determined choices for gender and/or (gendered) titles. I further analyse the range of reactions that I received when pro-actively asking for workarounds, fixes and updates, indicating how pointing towards those issues can trouble the status quo, identities and power hierarchies in unintended ways. I close on suggestions for the refactoring of existing and design of new technological infrastructures around gender and reflect on the value of lived experience in knowledge production – as well as the cost it comes with for those doing this research.",
        "link": "https://dl.acm.org/doi/10.1145/3461778.3462033",
        "category": "Databases"
    },
    {
        "title": "Don't Look Back, Look into the Future: Prescient Data Partitioning and Migration for Deterministic Database Systems",
        "authors": "['Yu-Shan Lin', 'Ching Tsai', 'Tz-Yu Lin', 'Yun-Sheng Chang', 'Shan-Hung Wu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Deterministic database systems have been shown to significantly improve the availability and scalability of a distributed database system deployed on a shared-nothing architecture across WAN while ensuring strong consistency. However, their scalability and performance advantages highly depend on the quality of data partitioning due to the reduced flexibility in transaction processing. Although a deterministic database system can employ workload driven data (re-)partitioning and live data migration algorithms to partition data, we found that the effectiveness of these algorithms is limited in complex real-world environments due to the unpredictability of machine workloads. In this paper, we present Hermes, a deterministic database system prototype that, for the first time, does not rely on sophisticated data partitioning to achieve high scalability and performance. Hermes employs a novel transaction routing mechanism that jointly optimizes the balance of machine workloads, data (re-)partitioning, and live data migration by looking into the queued transactions to be executed in the near future. We conducted extensive experiments which show that Hermes is able to yield 29% to 137% increase in transaction throughput as compared to the state-of-the-art systems under complex real-world workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452827",
        "category": "Databases"
    },
    {
        "title": "A Programming Language for Data Privacy with Accuracy Estimations",
        "authors": "['Elisabet Lobo-Vesga', 'Alejandro Russo', 'Marco Gaboardi']",
        "date": "None",
        "source": "ACM Transactions on Programming Languages and Systems",
        "abstract": "Differential privacy offers a formal framework for reasoning about the privacy and accuracy of computations on private data. It also offers a rich set of building blocks for constructing private data analyses. When carefully calibrated, these analyses simultaneously guarantee the privacy of the individuals contributing their data, and the accuracy of the data analysis results, inferring useful properties about the population. The compositional nature of differential privacy has motivated the design and implementation of several programming languages to ease the implementation of differentially private analyses. Even though these programming languages provide support for reasoning about privacy, most of them disregard reasoning about the accuracy of data analyses. To overcome this limitation, we present DPella, a programming framework providing data analysts with support for reasoning about privacy, accuracy, and their trade-offs. The distinguishing feature of DPella is a novel component that statically tracks the accuracy of different data analyses. To provide tight accuracy estimations, this component leverages taint analysis for automatically inferring statistical independence of the different noise quantities added for guaranteeing privacy. We evaluate our approach by implementing several classical queries from the literature and showing how data analysts can calibrate the privacy parameters to meet the accuracy requirements, and vice versa.",
        "link": "https://dl.acm.org/doi/10.1145/3452096",
        "category": "Databases"
    },
    {
        "title": "Greenplum: A Hybrid Database for Transactional and Analytical Workloads",
        "authors": "['Zhenghua Lyu', 'Huan Hubert Zhang', 'Gang Xiong', 'Gang Guo', 'Haozhou Wang', 'Jinbao Chen', 'Asim Praveen', 'Yu Yang', 'Xiaoming Gao', 'Alexandra Wang', 'Wen Lin', 'Ashwin Agrawal', 'Junfeng Yang', 'Hao Wu', 'Xiaoliang Li', 'Feng Guo', 'Jiang Wu', 'Jesse Zhang', 'Venkatesh Raghavan']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Demand for enterprise data warehouse solutions to support real-time Online Transaction Processing (OLTP) queries as well as long-running Online Analytical Processing (OLAP) workloads is growing. Greenplum database is traditionally known as an OLAP data warehouse system with limited ability to process OLTP workloads. In this paper, we augment Greenplum into a hybrid system to serve both OLTP and OLAP workloads. The challenge we address here is to achieve this goal while maintaining the ACID properties with minimal performance overhead. In this effort, we identify the engineering and performance bottlenecks such as the under-performing restrictive locking and the two-phase commit protocol. Next we solve the resource contention issues between transactional and analytical queries. We propose a global deadlock detector to increase the concurrency of query processing. When transactions that update data are guaranteed to reside on exactly one segment we introduce one-phase commit to speed up query processing. Our resource group model introduces the capability to separate OLAP and OLTP workloads into more suitable query processing mode. Our experimental evaluation on the TPC-B and CH-benCHmark benchmarks demonstrates the effectiveness of our approach in boosting the OLTP performance without sacrificing the OLAP performance.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457562",
        "category": "Databases"
    },
    {
        "title": "Demonstration of apperception: a database management system for geospatial video data",
        "authors": "['Yongming Ge', 'Vanessa Lin', 'Maureen Daum', 'Brandon Haynes', 'Alvin Cheung', 'Magdalena Balazinska']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Many recent video applications---including traffic monitoring, drone analytics, autonomous driving, and virtual reality---require piecing together, combining, and operating over many related video streams. Despite the massive data volumes involved and the need to jointly reason (both spatially and temporally) about these videos, current techniques to store and manipulate such data are often limited to file systems and simple video processing frameworks that reason about a single video in isolation.We present Apperception, a new type of database management system optimized for geospatial video applications. Apperception comes with an easy to use data model to reason about multiple geospatial video data streams, and a programming interface for developers to collectively reason about the entities observed in those videos. Our demo will let users write queries over video using Apperception and retrieve (in real-time) both metadata and rendered video data. Users can also compare results and observe speedups achieved by using Apperception.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476340",
        "category": "Databases"
    },
    {
        "title": "Scalar DL: scalable and practical byzantine fault detection for transactional database systems",
        "authors": "['Hiroyuki Yamada', 'Jun Nemoto']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "This paper presents Scalar DL, a Byzantine fault detection (BFD) middleware for transactional database systems. Scalar DL manages two separately administered database replicas in a database system and can detect Byzantine faults in the database system as long as either replica is honest (not faulty). Unlike previous BFD works, Scalar DL executes non-conflicting transactions in parallel while preserving a correctness guarantee. Moreover, Scalar DL is database-agnostic middleware so that it achieves the detection capability in a database system without either modifying the databases or using database-specific mechanisms. Experimental results with YCSB and TPC-C show that Scalar DL outperforms a state-of-the-art BFD system by 3.5 to 10.6 times in throughput and works effectively on multiple database implementations. We also show that Scalar DL achieves near-linear (91%) scalability when the number of nodes composing each replica increases.",
        "link": "https://dl.acm.org/doi/10.14778/3523210.3523212",
        "category": "Databases"
    },
    {
        "title": "ATLANTIC: making database differentially private and faster with accuracy guarantee",
        "authors": "['Lei Cao', 'Dongqing Xiao', 'Yizhou Yan', 'Samuel Madden', 'Guoliang Li']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Differential privacy promises to enable data sharing and general data analytics while protecting individual privacy. Because the private data is often stored in the form of relational database that supports SQL queries, making SQL-based analytics differentially private is thus critical. However, the existing SQL-based differentially private systems either only focus on specific type of SQL queries such as COUNT or substantially modify the database engine, thus obstructing adoption in practice. Worse yet, these systems often do not guarantee the desired accuracy by the applications. In this demonstration, using the driving trace workload from Cambridge Mobile Telematics (CMT), we show that our ATLANTIC system, as a database middleware, enforces differential privacy for real-world SQL queries with provable accuracy guarantees and is compatible with existing databases. Moreover, using a sampling-based technique, ATLANTIC significantly speeds up the query execution, yet effectively amplifying the privacy guarantee.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476337",
        "category": "Databases"
    },
    {
        "title": "PolarDB Serverless: A Cloud Native Database for Disaggregated Data Centers",
        "authors": "['Wei Cao', 'Yingqiang Zhang', 'Xinjun Yang', 'Feifei Li', 'Sheng Wang', 'Qingda Hu', 'Xuntao Cheng', 'Zongzhi Chen', 'Zhenjun Liu', 'Jing Fang', 'Bo Wang', 'Yuhui Wang', 'Haiqing Sun', 'Ze Yang', 'Zhushi Cheng', 'Sen Chen', 'Jian Wu', 'Wei Hu', 'Jianwei Zhao', 'Yusong Gao', 'Songlu Cai', 'Yunyang Zhang', 'Jiawang Tong']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "\\beginabstract The trend in the DBMS market is to migrate to the cloud for elasticity, high availability, and lower costs. The traditional, monolithic database architecture is difficult to meet these requirements. With the development of high-speed network and new memory technologies, disaggregated data center has become a reality: it decouples various components from monolithic servers into separated resource pools (e.g., compute, memory, and storage) and connects them through a high-speed network. The next generation cloud native databases should be designed for disaggregated data centers. In this paper, we describe the novel architecture of \\name, which follows thedisaggregation design paradigm: the CPU resource on compute nodes is decoupled from remote memory pool and storage pool. Each resource pool grows or shrinks independently, providing \\revon-demand provisoning at multiple dimensions while improving reliability. We also design our system to mitigate the inherent penalty brought by resource disaggregation, and introduce optimizations such as optimistic locking and index awared prefetching. Compared to the architecture that uses local resources, \\name achieves better dynamic resource provisioning capabilities and 5.3 times faster failure recovery speed, while achieving comparable performance. \\endabstract",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457560",
        "category": "Databases"
    },
    {
        "title": "openGauss: an autonomous database system",
        "authors": "['Guoliang Li', 'Xuanhe Zhou', 'Ji Sun', 'Xiang Yu', 'Yue Han', 'Lianyuan Jin', 'Wenbo Li', 'Tianqing Wang', 'Shifu Li']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Although learning-based database optimization techniques have been studied from academia in recent years, they have not been widely deployed in commercial database systems. In this work, we build an autonomous database framework and integrate our proposed learning-based database techniques into an open-source database system openGauss. We propose effective learning-based models to build learned optimizers (including learned query rewrite, learned cost/cardinality estimation, learned join order selection and physical operator selection) and learned database advisors (including self-monitoring, self-diagnosis, self-configuration, and self-optimization). We devise an effective validation model to validate the effectiveness of learned models. We build effective training data management and model management platforms to easily deploy learned models. We have evaluated our techniques on real-world datasets and the experimental results validated the effectiveness of our techniques. We also provide our learnings of deploying learning-based techniques.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476380",
        "category": "Databases"
    },
    {
        "title": "HeuristicDB: a hybrid storage database system using a non-volatile memory block device",
        "authors": "['Jinfeng Yang', 'Bingzhe Li', 'David J. Lilja']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "Hybrid storage systems are widely used in big data fields to balance system performance and cost. However, due to a poor understanding of the characteristics of database block requests, past studies in this area cannot fully utilize the performance gain from emerging storage devices. This study presents a hybrid storage database system, called HeuristicDB, which uses an emerging non-volatile memory (NVM) block device as an extension of the database buffer pool. To consider the unique performance behaviors of NVM block devices and the block-level characteristics of database requests, a set of heuristic rules that associate database (block) requests with the appropriate quality of service for the purpose of caching priority are proposed. Using online analytical processing (OLAP) and online transactional processing (OLTP) benchmarks, both trace-based examination and system implementation on MySQL are carried out to evaluate the effectiveness of the proposed design. The experimental results indicate that HeuristicDB provides up to 75% higher performance and migrates 18X fewer data between storage and the NVM block device than existing systems.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463774",
        "category": "Databases"
    },
    {
        "title": "SaS: SSD as SQL database system",
        "authors": "['Jong-Hyeok Park', 'Soyee Choi', 'Gihwan Oh', 'Sang-Won Lee']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Every database engine runs on top of an operating system in the host, strictly separated with the storage. This more-than-half-century-old IHDE (In-Host-Database-Engine) architecture, however, reveals its limitations when run on fast flash memory SSDs. In particular, the IO stacks incur significant run-time overhead and also hinder vertical optimizations between database engines and SSDs. In this paper, we envisage a new database architecture, called SaS (SSD as SQL database engine), where a full-blown SQL database engine runs inside SSD, tightly integrated with SSD architecture without intervening kernel stacks. As IO stacks are removed, SaS is free from their run-time overhead and further can explore numerous vertical optimizations between database engine and SSD. SaS evolves SSD from dummy block device to database server with SQL as its primary interface. The benefit of SaS will be more outstanding in the data centers where the distance between database engine and the storage is ever widening because of virtualization, storage disaggregation, and open software stacks. The advent of computational SSDs with more compute resource will enable SaS to be more viable and attractive database architecture.",
        "link": "https://dl.acm.org/doi/10.14778/3461535.3461538",
        "category": "Databases"
    },
    {
        "title": "DSB: a decision support benchmark for workload-driven and traditional database systems",
        "authors": "['Bailu Ding', 'Surajit Chaudhuri', 'Johannes Gehrke', 'Vivek Narasayya']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We describe a new benchmark, DSB, for evaluating both workload-driven and traditional database systems on modern decision support workloads. DSB is adapted from the widely-used industrial-standard TPC-DS benchmark. It enhances the TPC-DS benchmark with complex data distribution and challenging yet semantically meaningful query templates. DSB also introduces configurable and dynamic workloads to assess the adaptability of database systems. Since workload-driven and traditional database systems have different performance dimensions, including the additional resources required for tuning and maintaining the systems, we provide guidelines on evaluation methodology and metrics to report. We show a case study on how to evaluate both workload-driven and traditional database systems with the DSB benchmark. The code for the DSB benchmark is open sourced and is available at https://aka.ms/dsb.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484234",
        "category": "Databases"
    },
    {
        "title": "The art of balance: a RateupDB™ experience of building a CPU/GPU hybrid database product",
        "authors": "['Rubao Lee', 'Minghong Zhou', 'Chi Li', 'Shenggang Hu', 'Jianping Teng', 'Dongyang Li', 'Xiaodong Zhang']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "GPU-accelerated database systems have been studied for more than 10 years, ranging from prototyping development to industry products serving in multiple domains of data applications. Existing GPU database research solutions are often focused on specific aspects in parallel algorithms and system implementations for specific features, while industry product development generally concentrates on delivering a whole system by considering its holistic performance and cost. Aiming to fill this gap between academic research and industry development, we present a comprehensive industry product study on a complete CPU/GPU HTAP system, called RateupDB. We firmly believe \"the art of balance\" addresses major issues in the development of RateupDB. Specifically, we consider balancing multiple factors in the software development cycle, such as the trade-off between OLAP and OLTP, the trade-off between system performance and development productivity, and balanced choices of algorithms in the product. We also present RateupDB's complete TPC-H test performance to demonstrate its significant advantages over other existing GPU DBMS products.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476378",
        "category": "Databases"
    },
    {
        "title": "Privacy and Security in Mixed Reality Learning Environments by Input and User/Bot Interaction Protection",
        "authors": "['Lan Anh Tran', 'Benedikt Hensen', 'Ralf Klamma', 'Soamsiri Chantaraskul']",
        "date": "January 2022",
        "source": "APIT '22: Proceedings of the 2022 4th Asia Pacific Information Technology Conference",
        "abstract": "Mixed reality is known as an advanced technology that provides a new approach for learning environments. Such environments allow learners to interact with both virtual and real worlds and bringing in potential enhancements to the learning process at the same time. For example, chatbots can facilitate the learning process. However, security and privacy settings for interacting with chatbots in such mixed reality environments are complex. In this paper, we introduce a mixed reality virtual assistant that is integrated into the collaborative environment of our existing application VIAProMa. This embodied chatbot allows lecturers and students to participate in mixed reality and online classrooms in real-time. The participants can interact with each other via VIAProMa’s avatar representations and can communicate with the chatbot that is represented by the mixed reality bot. The bot is realized by connecting a Slack chatbot with the mixed reality learning environment. It is displayed as an intuitive 3D model and is able to communicate with the users in spoken language. In this environment, privacy and security settings are conducted to protect the user input and user interaction with the bot. The evaluation results show that the system works stably with good performance. All the visualizations and features are well designed and were understood by the users. Users preferred the speech interface with the bot over a textual interface. The research has a strong impact on the design of security and privacy features for mixed reality environments in general.",
        "link": "https://dl.acm.org/doi/10.1145/3512353.3512363",
        "category": "Databases"
    },
    {
        "title": "Idol dataset: a database on religious idols and its recognition with deep networks",
        "authors": "['B. Sathya Bama', 'S. Mohamed Mansoor Roomi', 'D. Sabarinathan', 'M. Senthilarasi', 'G. Manimala']",
        "date": "December 2021",
        "source": "ICVGIP '21: Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing",
        "abstract": "Idols are rich descriptors capturing both visual and historical information about temples and therefore enhance the process of documenting and managing cultural heritage of a place. There are very limited annotated databases for artistic cultural heritage images especially for idols. To meet this need, we collected, annotated, and prepared a new database of Hindu Religious Idols. The first version of Idol dataset contains 14,592 images collected from Internet by querying three major search engines using 150 name manifestations related keywords about 31 idol categories. Correctly identifying a particular God/Goddess image in the form of paintings, photographs and sculptures is crucial. In this paper, we investigate the use of deep neural networks to solve the problem of recognizing religious idols. To achieve this result, the network is first pre-trained on 10 ImageNets and selected Densenet121 which outperforms the other networks. A modified Densenet architecture is proposed with a softmax output for idol recognition to achieve 74.9% and 84.28% of Top1 and Top 2 Rank Accuracies respectively on imbalanced learning and 74.93% and 84.02% Top1 and Top2 Rank accuracies respectively for weighted loss learning.",
        "link": "https://dl.acm.org/doi/10.1145/3490035.3490295",
        "category": "Databases"
    },
    {
        "title": "A Dichotomy for the Generalized Model Counting Problem for Unions of Conjunctive Queries",
        "authors": "['Batya Kenig', 'Dan Suciu']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "We study the \\em generalized model counting problem, defined as follows: given a database, and a set of deterministic tuples, count the number of subsets of the database that include all deterministic tuples and satisfy the query. This problem is computationally equivalent to the evaluation of the query over a tuple-independent probabilistic database where all tuples have probabilities in $\\set0,\\frac1 2, 1 $. Previous work has established a dichotomy for Unions of Conjunctive Queries (UCQ) when the probabilities are arbitrary rational numbers, showing that, for each query, its complexity is either in polynomial time or \\#P-hard. The query is called \\em safe in the first case, and \\em unsafe in the second case. Here, we strengthen the hardness proof, by proving that an unsafe UCQ query remains \\#P-hard even if the probabilities are restricted to $\\set0,\\frac1 2, 1 $. This requires a complete redesign of the hardness proof, using new techniques. A related problem is the \\em model counting problem, which asks for the probability of the query when the input probabilities are restricted to $\\set0,\\frac1 2 $. While our result does not extend to model counting for all unsafe UCQs, we prove that model counting is \\#P-hard for a class of unsafe queries called Type-I forbidden queries.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458313",
        "category": "Databases"
    },
    {
        "title": "Privacy: From Database Reconstruction to Legal Theorems",
        "authors": "['Kobbi Nissim']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "There are significant gaps between legal and technical thinking around data privacy. Technical standards are described using mathematical language whereas legal standards are not rigorous from a mathematical point of view and often resort to concepts which they only partially define. As a result, arguments about the adequacy of technical privacy measures for satisfying legal privacy often lack rigor, and their conclusions are uncertain. The uncertainty is exacerbated by a litany of successful privacy attacks on privacy measures thought to meet legal expectations but then shown to fall short of doing so. As computer systems manipulating individual privacy-sensitive data become integrated in almost every aspect of society, and as such systems increasingly make decisions of legal significance, the need to bridge the diverging, and sometimes conflicting legal and technical approaches becomes urgent. We formulate and prove formal claims -- \"legal theorems'' -- addressing legal questions such as whether the use of technological measures satisfies the requirements of a legal privacy standard. In particular, we analyze the notion of singling out from the GDPR and whether technologies such as k-anonymity and differential privacy prevent singling out. Our long-term goal is to develop concepts which are on one hand technical, so they can be integrated in the design of computer systems, and can be used in legal reasoning and for policymaking on the other hand.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458816",
        "category": "Databases"
    },
    {
        "title": "Moneyball: proactive auto-scaling in Microsoft Azure SQL database serverless",
        "authors": "['Olga Poppe', 'Qun Guo', 'Willis Lang', 'Pankaj Arora', 'Morgan Oslake', 'Shize Xu', 'Ajay Kalhan']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Microsoft Azure SQL Database is among the leading relational database service providers in the cloud. Serverless compute automatically scales resources based on workload demand. When a database becomes idle its resources are reclaimed. When activity returns, resources are resumed. Customers pay only for resources they used. However, scaling is currently merely reactive, not proactive, according to customers' workloads. Therefore, resources may not be immediately available when a customer comes back online after a prolonged idle period. In this work, we focus on reducing this delay in resource availability by predicting the pause/resume patterns and proactively resuming resources for each database. Furthermore, we avoid taking away resources for short idle periods to relieve the back-end from ineffective pause/resume workflows. Results of this study are currently being used worldwide to find the middle ground between quality of service and cost of operation.",
        "link": "https://dl.acm.org/doi/10.14778/3514061.3514073",
        "category": "Databases"
    },
    {
        "title": "Consistent Query Answering for Primary Keys on Path Queries",
        "authors": "['Paraschos Koutris', 'Xiating Ouyang', 'Jef Wijsen']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "We study the data complexity of consistent query answering (CQA) on databases that may violate the primary key constraints. A repair is a maximal consistent subset of the database. For a Boolean query q, the problem CERTAINTY(q) takes a database as input, and asks whether or not each repair satisfies the query q. It is known that for any self-join-free Boolean conjunctive query q, CERTAINTY(q) is in FO, L-complete, or coNP-complete. In particular, CERTAINTY(q) is in FO for any self-join-free Boolean path query q. In this paper, we show that if self-joins are allowed, then the complexity of CERTAINTY(q) for Boolean path queries q exhibits a tetrachotomy between FO, NL-complete, PTIME-complete, and coNP-complete. Moreover, it is decidable, in polynomial time in the size of the query q, which of the four cases applies.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458334",
        "category": "Databases"
    },
    {
        "title": "Research on Dimensional Model Design and Application Method in State Grid",
        "authors": "['ZhengDa Cui', 'HaoYuan Sun', 'Gang Li']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "This article aims to quickly, accurately, and flexibly construct a dimensional model of the state grid. An OLAP modeling scheme based on relational database and XML is proposed. The modeling scheme divides the modeling mission into two stages: storage layer modeling and application layer modeling. In the storage layer, a relational database is used to establish a relational data model with multi-dimensional characteristics according to the \"star chart\" method; in the application layer, XML Schema is designed to describe the multi-dimensional data cube and physical mapping. The efficiency and effect of multidimensional data modeling in the state grid can be effectively improved by using this modeling scheme.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501607",
        "category": "Databases"
    },
    {
        "title": "Integrating cloud computing into computer science curriculum: a pilot study with teaching introductory database courses",
        "authors": "['Aijuan Dong']",
        "date": "None",
        "source": "Journal of Computing Sciences in Colleges",
        "abstract": "In this study, we designed, implemented and assessed the integration of cloud computing education and public cloud services into introductory database courses. This is the pilot study for a curricular initiative that integrates cloud computing into core courses across computer science curriculum at a liberal arts college. Direct and indirect evidence show that it is feasible to integrate cloud computing basics into an existing course without impacting students' learning experience and curricular coverage and, given the current billing model, $50 Google cloud credits are enough for four hands-on course activities, but not enough to support students to continue their class projects after class. The paper also discusses cost involved, students' evaluation of Google Cloud Platform (GCP), students' perception of cloud computing, and difficulties encountered and lessons learned in the process.",
        "link": "https://dl.acm.org/doi/10.5555/3512489.3512502",
        "category": "Databases"
    },
    {
        "title": "Report on SEBD 2020: the 28th Italian symposium on advanced database systems",
        "authors": "['Maristella Agosti', 'Maurizio Atzori', 'Paolo Ciaccia', 'Letizia Tanca']",
        "date": "December 2020",
        "source": "ACM SIGIR Forum",
        "abstract": "This paper reports on the 28th Italian Symposium on Advanced Database Systems (SEBD 2020), held online as a virtual conference from the 21st to the 24th of June 2020. The topics that were addressed in this edition of the conference were organized in the sessions: ontologies and data integration, anomaly detection and dependencies, text analysis and search, deep learning, noSQL data, trajectories and diffusion, health and medicine, context and ranking, social and knowledge graphs, multimedia content analysis, security issues, and data mining.",
        "link": "https://dl.acm.org/doi/10.1145/3483382.3483392",
        "category": "Databases"
    },
    {
        "title": "LogStore: A Cloud-Native and Multi-Tenant Log Database",
        "authors": "['Wei Cao', 'Xiaojie Feng', 'Boyuan Liang', 'Tianyu Zhang', 'Yusong Gao', 'Yunyang Zhang', 'Feifei Li']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "With the prevalence of cloud computing, more and more enterprises are migrating applications to cloud infrastructures. Logs are the key to helping customers understand the status of their applications running on the cloud. They are vital for various scenarios, such as service stability assessment, root cause analysis and user activity profiling. Therefore, it is essential to manage the massive amount of logs collected on the cloud and tap their value. Although various log storages have been widely used in the past few decades, it is still a non-trivial problem to design a cost-effective log storage for cloud applications. It faces challenges of heavy write throughput of tens of millions of log records per second, retrieval on PB-level logs and massive hundreds of thousands of tenants. Traditional log processing systems cannot satisfy all these requirements. To address these challenges, we propose the cloud-native log database LogStore. It combines shared-nothing and shared-data architecture, and utilizes highly scalable and low-cost cloud object storage, while overcoming the bandwidth limitations and high latency of using remote storage when writing a large number of logs. We also propose a multi-tenant management method that physically isolates tenant data to ensure compliance and flexible data expiration policies, and uses a novel traffic scheduling algorithm to mitigate the impact of traffic skew and hotspots among tenants. In addition, we design an efficient column index structure LogBlock to support queries with full-text search, and combined several query optimization techniques to reduce query latency on cloud object storage. LogStore has been deployed in Alibaba Cloud on a large scale (more than 500 machines), processing logs of more than 100 GB per second, and has been running stably for more than two years.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457565",
        "category": "Databases"
    },
    {
        "title": "Caracal: Contention Management with Deterministic Concurrency Control",
        "authors": "['Dai Qin', 'Angela Demke Brown', 'Ashvin Goel']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Deterministic databases offer several benefits: they ensure serializable execution while avoiding concurrency-control related aborts, and they scale well in distributed environments. Today, most deterministic database designs use partitioning to scale up and avoid contention. However, partitioning requires significant programmer effort, leads to poor performance under skewed workloads, and incurs unnecessary overheads in certain uncontended workloads. We present the design of Caracal, a novel shared-memory, deterministic database that performs well under both skew and contention. Our deterministic scheme batches transactions in epochs and executes the transactions in an epoch in a predetermined order. Our scheme enables reducing contention by batching concurrency control operations. It also allows analyzing the transactions in the epoch to determine contended keys accurately. Certain transactions can then be split into independent contended and uncontended pieces and run deterministically and in parallel, further reducing contention. Based on these ideas, we present two novel optimizations, batch append and split-on-demand, for managing contention. With these optimizations, Caracal scales well and outperforms existing deterministic schemes in most workloads by 1.9x to 9.7x.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483591",
        "category": "Databases"
    },
    {
        "title": "UDO: universal database optimization using reinforcement learning",
        "authors": "['Junxiong Wang', 'Immanuel Trummer', 'Debabrota Basu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "UDO is a versatile tool for offline tuning of database systems for specific workloads. UDO can consider a variety of tuning choices, reaching from picking transaction code variants over index selections up to database system parameter tuning. UDO uses reinforcement learning to converge to near-optimal configurations, creating and evaluating different configurations via actual query executions (instead of relying on simplifying cost models). To cater to different parameter types, UDO distinguishes heavy parameters (which are expensive to change, e.g. physical design parameters) from light parameters. Specifically for optimizing heavy parameters, UDO uses reinforcement learning algorithms that allow delaying the point at which the reward feedback becomes available. This gives us the freedom to optimize the point in time and the order in which different configurations are created and evaluated (by benchmarking a workload sample). UDO uses a cost-based planner to minimize reconfiguration overheads. For instance, it aims to amortize the creation of expensive data structures by consecutively evaluating configurations using them. We evaluate UDO on Postgres as well as MySQL and on TPC-H as well as TPC-C, optimizing a variety of light and heavy parameters concurrently.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484236",
        "category": "Databases"
    },
    {
        "title": "Efficient Uncertainty Tracking for Complex Queries with Attribute-level Bounds",
        "authors": "['Su Feng', 'Boris Glavic', 'Aaron Huber', 'Oliver A. Kennedy']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Incomplete and probabilistic database techniques are principled methods for coping with uncertainty in data. Unfortunately, the class of queries that can be answered efficiently over such databases is severely limited, even when advanced approximation techniques are employed.We introduce attribute-annotated uncertain databases (AU-DBs), an uncertain data model that annotates tuples and attribute values with bounds to compactly approximate an incomplete database. AU-DBs are closed under relational algebra with aggregation using an efficient evaluation semantics. Using optimizations that trade accuracy for performance, our approach scales to complex queries and large datasets, and produces accurate results.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452791",
        "category": "Databases"
    },
    {
        "title": "Conversational AI and Knowledge Graphs for Social Robot Interaction",
        "authors": "['Graham Wilcock', 'Kristiina Jokinen']",
        "date": "March 2022",
        "source": "HRI '22: Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction",
        "abstract": "The paper describes an approach that combines work from three fields with previously separate research communities: social robotics, conversational AI, and graph databases. The aim is to develop a generic framework in which a variety of social robots can provide high-quality information to users by accessing semantically-rich knowledge graphs about multiple different domains. An example implementation uses a Furhat robot with Rasa open source conversational AI and knowledge graphs in Neo4j graph databases.",
        "link": "https://dl.acm.org/doi/10.5555/3523760.3523941",
        "category": "Databases"
    },
    {
        "title": "Instance-Optimized Data Layouts for Cloud Analytics Workloads",
        "authors": "['Jialin Ding', 'Umar Farooq Minhas', 'Badrish Chandramouli', 'Chi Wang', 'Yinan Li', 'Ying Li', 'Donald Kossmann', 'Johannes Gehrke', 'Tim Kraska']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Today, businesses rely on efficiently running analytics on large amounts of operational and historical data to gain business insights and competitive advantage. Increasingly, such analytics are run using cloud-based data analytics services, such as Google BigQuery, Microsoft Azure Synapse, Amazon Redshift, and Snowflake. These services persist and process data in compressed, columnar formats, stored in large blocks, each of which contains thousands or millions of records. For these services, disk I/O from (remote) cloud storage is often one of the dominant costs for query processing. To reduce the amount of I/O, services often maintain per-block metadata, such as zone maps, which are used to skip blocks that are irrelevant to the query, leading to lower query execution times. However, the effectiveness of block skipping via zone maps is dependent on how the records are assigned to blocks. Recent work on instance-optimized data layouts aims to maximize block skipping by specializing the block assignment strategy to a specific dataset and workload. However, these existing approaches only optimize the layout for a single table. In this paper, we propose MTO, an instance-optimized data layout framework that determines the blocking strategy for all tables in a multi-table database in the presence of joins, such as in a star or snowflake schema common in real-world workloads. MTO takes advantage of sideways information passing through joins to jointly optimize the layout for all tables, which results in better block skipping and hence reduced query execution times. Experiments on a commercial cloud-based analytics service show that MTO achieves up to 93% reduction in blocks accessed and 75% reduction in end-to-end query times compared to alternative blocking strategies.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457270",
        "category": "Databases"
    },
    {
        "title": "Perceptual Quality Assessment of Internet Videos",
        "authors": "['Jiahua Xu', 'Jing Li', 'Xingguang Zhou', 'Wei Zhou', 'Baichao Wang', 'Zhibo Chen']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "With the fast proliferation of online video sites and social media platforms, user, professionally and occupationally generated content (UGC, PGC, OGC) videos are streamed and explosively shared over the Internet. Consequently, it is urgent to monitor the content quality of these Internet videos to guarantee the user experience. However, most existing modern video quality assessment (VQA) databases only include UGC videos and cannot meet the demands for other kinds of Internet videos with real-world distortions. To this end, we collect 1,072 videos from Youku, a leading Chinese video hosting service platform, to establish the Internet video quality assessment database (Youku-V1K). A special sampling method based on several quality indicators is adopted to maximize the content and distortion diversities within a limited database, and a probabilistic graphical model is applied to recover reliable labels from noisy crowdsourcing annotations. Based on the properties of Internet videos originated from Youku, we propose a spatio-temporal distortion-aware model (STDAM). First, the model works blindly which means the pristine video is unnecessary. Second, the model is familiar with diverse contents by pre-training on the large-scale image quality assessment databases. Third, to measure spatial and temporal distortions, we introduce the graph convolution and attention module to extract and enhance the features of the input video. Besides, we leverage the motion information and integrate the frame-level features into video-level features via a bi-directional long short-term memory network. Experimental results on the self-built database and the public VQA databases demonstrate that our model outperforms the state-of-the-art methods and exhibits promising generalization ability.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475486",
        "category": "Databases"
    },
    {
        "title": "Toto – Benchmarking the Efficiency of a Cloud Service",
        "authors": "['Justin Moeller', 'Zi Ye', 'Katherine Lin', 'Willis Lang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Microsoft aims to increase the efficiency of Azure SQL DB by maximizing the number of databases that can be hosted in a cluster. However, resource contention among customers increases when changing the configurations, policies, and features that control database co-location on cluster nodes. Tuning and evaluating the efficiency and customer impact of these variables in a scientific manner in production, with a dynamic system and customer workloads, is difficult or infeasible. Here, we present Toto, a benchmark framework for evaluating the efficiency of any cloud service that leverages orchestrators like Service Fabric or Kubernetes. Toto allows for reliable and repeatable specification of a benchmarking scenario of arbitrary scale, complexity, and time-length. An implementation of Toto is deployed in all SQL DB staging clusters and is used to evaluate system efficiency and behaviors. As an example of Toto's capabilities, we present a study to explore the balance between cluster database density and quality of service.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457555",
        "category": "Databases"
    },
    {
        "title": "The Shape of Our Bias: Perceived Age and Gender in the Humanoid Robots of the ABOT Database",
        "authors": "['Giulia Perugia', 'Stefano Guidi', 'Margherita Bicchi', 'Oronzo Parlangeli']",
        "date": "March 2022",
        "source": "HRI '22: Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction",
        "abstract": "The present study was aimed at determining the age and gender distribution of the humanoid robots in the ABOT dataset, and providing a systematic data-driven formalization of the process of age and gender categorization of humanoid robots. We involved 153 participants in an online study and asked them to rate the humanoid robots in the ABOT dataset in terms of perceived age, femininity, masculinity, and gender neutrality. Our analyses disclosed that most of the robots in the ABOT dataset were perceived as young adults, and the vast majority of them were attributed a neutral or masculine gender. By merging our data with the data in the ABOT dataset, we discovered that humanlikeness is crucial to elicit social categorization. Moreover, we found out that body manipulators (e.g., legs, torso) guide the attribution of masculinity, surface look features (e.g., eyelashes, apparel) the attribution of femininity, and that robots without facial features (e.g., head, eyes) are perceived as older. Finally, yet importantly, we unveiled that men tend to attribute lower age scores and higher femininity ratings to humanoid robots than women. Our work provides evidence of an existing underlying bias in the design of humanoid robots that needs to be addressed: the under-representation of feminine robots and lack of representation of androgynous ones. We make the results of this study publicly available to the HRI community by attaching the dataset we collected to the present paper and creating a dedicated website.",
        "link": "https://dl.acm.org/doi/10.5555/3523760.3523779",
        "category": "Databases"
    },
    {
        "title": "Benchmarking Approximate Consistent Query Answering",
        "authors": "['Marco Calautti', 'Marco Console', 'Andreas Pieris']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Consistent query answering (CQA) aims to deliver meaningful answers when queries are evaluated over inconsistent databases. Such answers must be certainly true in all repairs, which are consistent databases whose difference from the inconsistent one is somehow minimal. Although CQA provides a clean framework for querying inconsistent databases, it is arguably more informative to compute the percentage of repairs in which a candidate answer is true, instead of simply saying that is true in all repairs, or is false in at least one repair. It should not be surprising, though, that computing this percentage is computationally hard. On the other hand, for practically relevant settings such as conjunctive queries and primary keys, there are data-efficient randomized approximation schemes for approximating this percentage. Our goal is to perform a thorough experimental evaluation and comparison of those approximation schemes. Our analysis provides new insights on which technique is indicated depending on key characteristics of the input, and it further provides evidence that making approximate CQA as described above feasible in practice is not an unrealistic goal.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458309",
        "category": "Databases"
    },
    {
        "title": "Geolocating Traffic Signs using Large Imagery Datasets",
        "authors": "['Kasper F. Pedersen', 'Kristian Torp']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "Maintaining a database with the type, location, and direction of traffic signs is a labor-intensive part of asset management for many road authorities. Today there are high-quality cameras in cell-phones that can add location (EXIF) metadata to the images. This makes it efficient and cheap to collect large geo-located imagery datasets. Detecting traffic signs from imagery is also much simpler today due to the availability of several high-quality open-source object-detection solutions. In this paper, we use the detection of traffic signs to find both the location and the direction of physical traffic signs. Five approaches to cluster the detections are presented. An extensive experimental evaluation shows that it is important to consider both the location and the direction. The evaluation is done on a novel dataset with 21,565 images that is available free for download. This includes the ground-truth location of 277 traffic signs and all source code. The conclusion is that traffic signs are detected with an F1 score of 0.8889, a location accuracy of 5.097-meter (MAE), and a direction accuracy of ± 11.375°(MAE). Only data from two trips are needed to get these results.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470900",
        "category": "Databases"
    },
    {
        "title": "To Partition, or Not to Partition, That is the Join Question in a Real System",
        "authors": "['Maximilian Bandle', 'Jana Giceva', 'Thomas Neumann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "An efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance. In this paper, we address these questions, show how to integrate the radix join in Umbra, a code-generating DBMS, and make it competitive for selective queries by introducing a Bloom-filter based semi-join reducer. We have evaluated how well it runs when used in queries from more representative workloads like TPC-H. Surprisingly, the radix join brings a noticeable improvement in only one out of all 59 joins in TPC-H. Thus, with an extensive range of microbenchmarks, we have isolated the effects of the most important workload factors and synthesized the range of values where partitioning the data for the radix join pays off. Our analysis shows that the benefit of data partitioning quickly diminishes as soon as we deviate from the optimal parameters, and even late materialization rarely helps in real workloads. We thus, conclude that integrating the radix join within a code-generating database rarely justifies the increase in code and optimizer complexity and advise against it for processing real-world workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452831",
        "category": "Databases"
    },
    {
        "title": "A Novel Indexing Method for Spatial-Keyword Range Queries",
        "authors": "['Panagiotis Tampakis', 'Dimitris Spyrellis', 'Christos Doulkeridis', 'Nikos Pelekis', 'Christos Kalyvas', 'Akrivi Vlachou']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "Spatial-keyword queries are important for a wide range of applications that retrieve data based on a combination of keyword search and spatial constraints. However, efficient processing of spatial-keyword queries is not a trivial task because the combination of textual and spatial data results in a high-dimensional representation that is challenging to index effectively. To address this problem, in this paper, we propose a novel indexing scheme for efficient support of spatial-keyword range queries. At the heart of our approach lies a carefully-designed mapping of spatio-textual data to a two-dimensional (2D) space that produces compact partitions of spatio-textual data. In turn, the mapped 2D data can be indexed effectively by traditional spatial data structures, such as an R-tree. We propose bounds, theoretically proven for correctness, that lead to the design of a filter-and-refine algorithm that prunes the search space effectively. In this way, our approach for spatial-keyword range queries is readily applicable to any database system that provides spatial support. In our experimental evaluation, we demonstrate how our algorithm can be implemented over PostgreSQL and exploit its underlying spatial index provided by PostGIS, in order to process spatial-keyword range queries efficiently. Moreover, we show that our solution outperforms different competitor approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470897",
        "category": "Databases"
    },
    {
        "title": "Clustering of Adverse Events of Post-Market Approved Drugs",
        "authors": "['Ahmed Askar', 'Andreas Zuefle']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "Adverse side effects of a drug may vary over space and time due to different populations, environments, and drug quality. Discovering all side effects during the development process is impossible. Once a drug is approved, observed adverse effects are reported by doctors and patients and made available in the Adverse Event Reporting System provided by the U.S. Food and Drug Administration . Mining such records of reported adverse effects, this study proposes a spatial clustering approach to identify regions that exhibit similar adverse effects. We apply a topic modeling approach on textual representations of reported adverse effects using Latent Dirichlet Allocation. By describing a spatial region as a mixture of the resulting latent topics, we find clusters of regions that exhibit similar (topics of) adverse events for the same drug using Hierarchical Agglomerative Clustering. We investigate the resulting clusters for spatial autocorrelation to test the hypothesis that certain (topics of) adverse effects may occur only in certain spatial regions using Moran’s I measure of spatial autocorrelation.  Our experimental evaluation exemplary applies our proposed framework to a number of blood-thinning drugs, showing that some drugs exhibit more coherent textual topics among their reported adverse effects than other drugs, but showing no significant spatial autocorrelation of these topics. Our approach can be applied to other drugs or vaccines to study if spatially localized adverse effects may justify further investigation.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470903",
        "category": "Databases"
    },
    {
        "title": "MANTIS: Multiple Type and Attribute Index Selection using Deep Reinforcement Learning",
        "authors": "['Vishal Sharma', 'Curtis Dyreson', 'Nicholas Flann']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "DBMS performance is dependent on many parameters, such as index selection, cache size, physical layout, and data partitioning. Some combinations of these parameters can lead to optimal performance for a given workload but selecting an optimal or near-optimal combination is challenging, especially for large databases with complex workloads. Among the hundreds of parameters, index selection is arguably the most critical parameter for performance. We propose a self-administered framework, called the Multiple Type and Attribute Index Selector (MANTIS), that automatically selects near-optimal indexes. The framework advances the state-of-the-art index selection by considering both multi-attribute and multiple types of indexes within a bounded storage size constraint, a combination not previously addressed. MANTIS combines supervised and reinforcement learning, a Deep Neural Network recommends the type of index for a given workload while a Deep Q-Learning network recommends the multi-attribute aspect. MANTIS is sensitive to storage cost constraints and incorporates noisy rewards in its reward function for better performance. Our experimental evaluation shows that MANTIS outperforms the current state-of-art methods by an average of 9.53% QphH@size.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472176",
        "category": "Databases"
    },
    {
        "title": "Good to the Last Bit: Data-Driven Encoding with CodecDB",
        "authors": "['Hao Jiang', 'Chunwei Liu', 'John Paparrizos', 'Andrew A. Chien', 'Jihong Ma', 'Aaron J. Elmore']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Columnar databases rely on specialized encoding schemes to reduce storage requirements. These encodings also enable efficient in-situ data processing. Nevertheless, many existing columnar databases are encoding-oblivious. When storing the data, these systems rely on a global understanding of the dataset or the data types to derive simple rules for encoding selection. Such rule-based selection leads to unsatisfactory performance. Specifically, when performing queries, the systems always decode data into memory, ignoring the possibility of optimizing access to encoded data. We develop CodecDB, an encoding-aware columnar database, to demonstrate the benefit of tightly-coupling the database design with the data encoding schemes. CodecDB chooses in a principled manner the most efficient encoding for a given data column and relies on encoding-aware query operators to optimize access to encoded data. Storage-wise, CodecDB achieves on average 90% accuracy for selecting the best encoding and improves the compression ratio by up to 40% compared to the state-of-the-art encoding selection solution. Query-wise, CodecDB is on average one order of magnitude faster than the latest open-source and commercial columnar databases on the TPC-H benchmark, and on average 3x faster than a recent research project on the Star-Schema Benchmark (SSB).",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457283",
        "category": "Databases"
    },
    {
        "title": "FoundationDB: A Distributed Unbundled Transactional Key Value Store",
        "authors": "['Jingyu Zhou', 'Meng Xu', 'Alexander Shraer', 'Bala Namasivayam', 'Alex Miller', 'Evan Tschannen', 'Steve Atherton', 'Andrew J. Beamon', 'Rusty Sears', 'John Leach', 'Dave Rosenthal', 'Xin Dong', 'Will Wilson', 'Ben Collins', 'David Scherer', 'Alec Grieser', 'Young Liu', 'Alvin Moore', 'Bhaskar Muppana', 'Xiaoge Su', 'Vishesh Yadav']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "FoundationDB is an open source transactional key value store created more than ten years ago. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions (a.k.a. NewSQL). FoundationDB adopts an unbundled architecture that decouples an in-memory transaction management system, a distributed storage system, and a built-in distributed configuration system. Each sub-system can be independently provisioned and configured to achieve the desired scalability, high-availability and fault tolerance properties. FoundationDB uniquely integrates a deterministic simulation framework, used to test every new feature of the system under a myriad of possible faults. This rigorous testing makes FoundationDB extremely stable and allows developers to introduce and release new features in a rapid cadence. FoundationDB offers a minimal and carefully chosen feature set, which has enabled a range of disparate systems (from semi-relational databases, document and object stores, to graph databases and more) to be built as layers on top. FoundationDB is the underpinning of cloud infrastructure at Apple, Snowflake and other companies, due to its consistency, robustness and availability for storing user data, system metadata and configuration, and other critical information.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457559",
        "category": "Databases"
    },
    {
        "title": "Adaptive flash sorting for memory-constrained embedded devices",
        "authors": "['Ramon Lawrence']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Databases running on embedded devices require efficient sorting algorithms for aggregation, joins, and output ordering. Previous research has produced algorithms optimized for small-memory, flash embedded devices that either use multiple read passes to avoid writes or optimize the external merge sort algorithm. Depending on the data input distribution and memory characteristics, neither approach always outperforms the other. This work produces an adaptive flash sorting algorithm that dynamically determines the best sorting approach at run-time. Experimental results demonstrate that the adaptive sorting algorithm combines the best features of both approaches and allows overall superior performance.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441914",
        "category": "Databases"
    },
    {
        "title": "Datalog Unchained",
        "authors": "['Victor Vianu']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "This is the companion paper of a talk in the Gems of PODS series, that reviews the development, starting at PODS 1988, of a family of Datalog-like languages with procedural, forward chaining semantics, providing an alternative to the classical declarative, model-theoretic semantics. These languages also provide a unified formalism that can express important classes of queries including fixpoint, while, and all computable queries. They can also incorporate in a natural fashion updates and nondeterminism. Datalog variants with forward chaining semantics have been adopted in a variety of settings, including active databases, production systems, distributed data exchange, and data-driven reactive systems.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458815",
        "category": "Databases"
    },
    {
        "title": "Extended XACML Language and Architecture for Access Control in Graph-structured Data",
        "authors": "['Aya Mohamed', 'Dagmar Auer', 'Daniel Hofer', 'Josef Küng']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "The rapidly increasing use of graph databases for a wide variety of applications demands flexible authorization and fine-grained access control at the level of attributes associated with the basic entities (i.e., accessing subject, requested resource, performed action, and environmental conditions) but also the vertices and edges along a particular access path. We present a solution for authorization policy specification and enforcement in a graph database to apply fine-grained path-specific constraints on graph-structured data. Therefore, we extend the well-established declarative policy definition language eXtensible Access Control Markup Language (XACML) and its architecture to describe path patterns and enforce the policies using the standard functional components of XACML. Our approach, XACML for Graph-structured data (XACML4G), defines an extended XACML grammar for the authorization policy and access request. To enforce XACML4G policies, we relied on the extensibility points of the XACML architecture and added proprietary extensions. We show the significance of our approach by means of a demonstration prototype in the university domain. Finally, we provide an initial evaluation of the expressiveness and performance of XACML4G with regard to XACML.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487789",
        "category": "Databases"
    },
    {
        "title": "Effective Traffic Forecasting with Multi-Resolution Learning",
        "authors": "['Abdullah Aldwyish', 'Egemen Tanin', 'Hairuo Xie', 'Shanika Karunasekera', 'Kotagiri Ramamohanarao']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "Traffic forecasting plays a vital role in traffic management systems. Recently, deep learning models have been applied to citywide traffic forecasting. However, the existing work models and predicts traffic at a single (dense) resolution, making it challenging to capture long-range spatial dependencies or high-level traffic dynamics. This shortcoming limits the accuracy of prediction and results in computationally expensive models. We propose a traffic forecasting model based on deep convolutional networks to improve the accuracy of citywide traffic forecasting. Our model uses a hierarchical architecture that captures traffic dynamics at multiple spatial resolutions. Based on this architecture, we apply a multi-task learning scheme, which trains the model to predict traffic at different resolutions. Our model helps provide a coherent understanding of traffic dynamics by capturing spatial dependencies between different regions of a city. Experimental results on multiple real datasets show that our model can achieve competitive results compared to complex state-of-the-art approaches while being more computationally efficient.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470904",
        "category": "Databases"
    },
    {
        "title": "Time-Constrained Indoor Keyword-aware Routing",
        "authors": "['Harry Kai-Ho Chan', 'Tiantian Liu', 'Huan Li', 'Hua Lu']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "With the increasingly available indoor positioning technologies, indoor location-based services (LBS) are becoming popular. Among indoor LBS applications, indoor routing is particularly in demand. In the literature, there are several existing studies on indoor keyword-aware routing queries, each considering different criteria when finding an optimal route. However, none of these studies explicitly constraint the time budget for the route. In this paper, we propose a new problem formulation TIKRQ that considers the time needed for a user to complete the route, in addition to other criteria such as static cost and textual relevance. A set-based search algorithm and effective pruning strategies are proposed for TIKRQ. We conduct extensive experiments to verify the efficiency of our proposals.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470895",
        "category": "Databases"
    },
    {
        "title": "SPRIG: A Learned Spatial Index for Range and kNN Queries",
        "authors": "['Songnian Zhang', 'Suprio Ray', 'Rongxing Lu', 'Yandong Zheng']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "A corpus of recent work has revealed that the learned index can improve query performance while reducing the storage overhead. It potentially offers an opportunity to address the spatial query processing challenges caused by the surge in location-based services. Although several learned indexes have been proposed to process spatial data, the main idea behind these approaches is to utilize the existing one-dimensional learned models, which requires either converting the spatial data into one-dimensional data or applying the learned model on individual dimensions separately. As a result, these approaches cannot fully utilize or take advantage of the information regarding the spatial distribution of the original spatial data. To this end, in this paper, we exploit it by using the spatial (multi-dimensional) interpolation function as the learned model, which can be directly employed on the spatial data. Specifically, we design an efficient SPatial inteRpolation functIon based Grid index (SPRIG) to process the range and kNN queries. Detailed experiments are conducted on real-world datasets. The results indicate that, compared to the traditional spatial indexes, our proposed learned index can significantly improve the index building and query processing performance with less storage overhead. Moreover, in the best case, our index achieves up to an order of magnitude better performance than ZM-index in range queries and is about 2.7 × , 3 × , and 9 × faster than the multi-dimensional learned index Flood in terms of index building, range queries, and kNN queries, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470892",
        "category": "Databases"
    },
    {
        "title": "Metro Maps on Flexible Base Grids",
        "authors": "['Hannah Bast', 'Patrick Brosi', 'Sabine Storandt']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "We present new generic methods to efficiently draw schematized metro maps for a wide variety of layouts, including octilinear, hexalinear, and orthoradial maps. The maps are drawn by mapping the input graph to a suitable grid graph. Previous work was restricted to regular octilinear grids. In this work, we investigate a variety of grids, including triangular grids and orthoradial grids. In particular, we also construct sparse grids where the local node density adapts to the input graph (e.g. octilinear Hanan grids, which we introduce in this work). For octilinear maps, this reduces the grid size by a factor of up to 5 compared to previous work, while still achieving close-to-optimal layouts. For many maps, this reduction also leads to up to 5 times faster solution times of the underlying optimization problem. We evaluate our approach on five maps. All octilinear maps can be computed in under 0.5 seconds, all hexalinear and orthoradial maps can be computed in under 2.5 seconds.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470899",
        "category": "Databases"
    },
    {
        "title": "Worst-Case Optimal Graph Joins in Almost No Space",
        "authors": "['Diego Arroyuelo', 'Aidan Hogan', 'Gonzalo Navarro', 'Juan L. Reutter', 'Javiel Rojas-Ledesma', 'Adrián Soto']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We present an indexing scheme that supports worst-case optimal (wco) joins over graphs within compact space. Supporting all possible wco joins using conventional data structures - based on B(+)-Trees, tries, etc. - requires 6 index orders in the case of graphs represented as triples. We rather propose a form of index, which we call a ring, that indexes each triple as a set of cyclic bidirectional strings of length 3. Rather than maintaining 6 orderings, we can use one ring to index them all. This ring replaces the graph and uses only sublinear extra space on top of the graph; in order words, the ring supports worst-case optimal graph joins in almost no space beyond storing the graph itself. We perform experiments using our representation to index a large graph (Wikidata) in memory, over which wco join algorithms are implemented. Our experiments show that the ring offers the best overall performance for query times while using only a small fraction of the space when compared with several state-of-the-art approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457256",
        "category": "Databases"
    },
    {
        "title": "Synthesizing Linked Data Under Cardinality and Integrity Constraints",
        "authors": "['Amir Gilad', 'Shweta Patwa', 'Ashwin Machanavajjhala']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The generation of synthetic data is useful in multiple aspects, from testing applications to benchmarking to privacy preservation. Generating thelinks between relations, subject tocardinality constraints (CCs) andintegrity constraints (ICs) is an important aspect of this problem. Given instances of two relations, where one has a foreign key dependence on the other and is missing its foreign key ($FK$) values, and two types of constraints: (1) CCs that apply to the join view and (2) ICs that apply to the table with missing $FK$ values, our goal is to impute the missing $FK$ values such that the constraints are satisfied. We provide a novel framework for the problem based on declarative CCs and ICs. We further show that the problem is NP-hard and propose a novel two-phase solution that guarantees the satisfaction of the ICs. Phase I yields an intermediate solution accounting for the CCs alone, and relies on a hybrid approach based on CC types. For one type, the problem is modeled as an Integer Linear Program. For the others, we describe an efficient and accurate solution. We then combine the two solutions. Phase II augments this solution by incorporating the ICs and uses a coloring of the conflict hypergraph to infer the values of the $FK$ column. Our extensive experimental study shows that our solution scales well when the data and number of constraints increases. We further show that our solution maintains low error rates for the CCs.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457242",
        "category": "Databases"
    },
    {
        "title": "Spatial Skyline Queries on Triangulated Irregular Networks",
        "authors": "['Yuta Kasai', 'Kento Sugiura', 'Yoshiharu Ishikawa']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "A spatial skyline query is a query to find a set of data points that are not spatially dominated by other data points, given a set of data points P and query points Q in a multidimensional space. The query enumerates the skyline points based on distance in a multidimensional space. However, existing spatial skyline queries can lead to large errors with actual travel distances in geo-spaces because the query is based on the Euclidean distance.  We propose a spatial skyline query on triangulated irregular networks (TINs), which are frequently used to represent the surfaces of terrain. We define a new spatial skyline query based on more accurate travel distances considering the TIN distance instead of the Euclidean distance. We also propose an efficient solution method using indexes to find nearest-neighbor points in TIN space and reduce the numbers of unnecessary data points and TIN vertices.  The proposed method achieves a computational complexity of O(|P′||Q|N′2 + |P′|2|Q|), where P′ and N′ are the reduced sets of data points and number of TIN vertices, respectively, based on the range of query points. The proposed method can process a query faster than the naive method with Θ(|P||Q|N2 + |P|2|Q|), where N is the number of TIN vertices. Moreover, experiments verify that the proposed method is faster than the naive method by using a spatial index to reduce the numbers of unnecessary data points and TIN vertices.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470901",
        "category": "Databases"
    },
    {
        "title": "Knowledge Graphs",
        "authors": "['Aidan Hogan', 'Eva Blomqvist', 'Michael Cochez', 'Claudia D’amato', 'Gerard De Melo', 'Claudio Gutierrez', 'Sabrina Kirrane', 'José Emilio Labra Gayo', 'Roberto Navigli', 'Sebastian Neumaier', 'Axel-Cyrille Ngonga Ngomo', 'Axel Polleres', 'Sabbir M. Rashid', 'Anisa Rula', 'Lukas Schmelzeisen', 'Juan Sequeda', 'Steffen Staab', 'Antoine Zimmermann']",
        "date": "None",
        "source": "ACM Computing Surveys",
        "abstract": "In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.",
        "link": "https://dl.acm.org/doi/10.1145/3447772",
        "category": "Databases"
    },
    {
        "title": "Reconstructing with Less: Leakage Abuse Attacks in Two Dimensions",
        "authors": "['Evangelia Anna Markatou', 'Francesca Falzon', 'Roberto Tamassia', 'William Schor']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Access and search pattern leakage from range queries are detrimental to the security of encrypted databases, as evidenced by a large body of work on attacks that reconstruct one-dimensional databases. Recently, the first attack from 2D range queries showed that higher-dimensional databases are also in danger (Falzon et al. CCS 2020). Their attack requires the access and search pattern of all possible queries. We present an order reconstruction attack that only depends on access pattern leakage, and empirically show that the order allows the attacker to infer the geometry of the underlying data. Notably, this attack also achieves full database reconstruction when the 1D horizontal and vertical projections of the points are dense. We also give an approximate database reconstruction attack that is distribution-agnostic and works with any sample of queries, given the search pattern and access pattern leakage of those queries, and the order of the database records. Finally, we show how to improve the reconstruction given knowledge of auxiliary information (e.g., the centroid of a related dataset). We support our results with formal analysis and experiments on real-world databases with queries drawn from various distributions.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484552",
        "category": "Databases"
    },
    {
        "title": "Spatial Dimensions of Algorithmic Transparency: A Summary",
        "authors": "['Jayant Gupta', 'Alexander Long', 'Corey Kewei Xu', 'Tian Tang', 'Shashi Shekhar']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "Spatial data brings an important dimension to AI’s quest for algorithmic transparency. For example, data driven computer-aided policy-decisions use measures of segregation (e.g., dissimilarity index) or income-inequality (e.g., Gini index), and these measures are affected by space partitioning choice. This may lead policymakers to underestimate the level of inequality or segregation within a region. The problem stems from the fact that many segregation based analyses use aggregated census data but do not report result sensitivity to choice of spatial partitioning (e.g., census block, tract). Beyond the well-known Modifiable Areal Unit Problem, this paper shows (via mathematical proofs as well as case studies with census data and census based synthetic micro-population data) that values of many measures (e.g., Gini index, dissimilarity index) diminish monotonically with increasing spatial-unit size in a hierarchical space partitioning (e.g., block, block-group, tract), however the ranking based on spatially aggregated measures remain sensitive to the scale of spatial partitions (e.g., block, block group). This paper highlights the need for social scientists to report how rankings of inequality are affected by the choice of spatial partitions.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470898",
        "category": "Databases"
    },
    {
        "title": "Precise No-Reference Image Quality Evaluation Based on Distortion Identification",
        "authors": "['Chenggang Yan', 'Tong Teng', 'Yutao Liu', 'Yongbing Zhang', 'Haoqian Wang', 'Xiangyang Ji']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "The difficulty of no-reference image quality assessment (NR IQA) often lies in the lack of knowledge about the distortion in the image, which makes quality assessment blind and thus inefficient. To tackle such issue, in this article, we propose a novel scheme for precise NR IQA, which includes two successive steps, i.e., distortion identification and targeted quality evaluation. In the first step, we employ the well-known Inception-ResNet-v2 neural network to train a classifier that classifies the possible distortion in the image into the four most common distortion types, i.e., Gaussian white noise (WN), Gaussian blur (GB), jpeg compression (JPEG), and jpeg2000 compression (JP2K). Specifically, the deep neural network is trained on the large-scale Waterloo Exploration database, which ensures the robustness and high performance of distortion classification. In the second step, after determining the distortion type of the image, we then design a specific approach to quantify the image distortion level, which can estimate the image quality specially and more precisely. Extensive experiments performed on LIVE, TID2013, CSIQ, and Waterloo Exploration databases demonstrate that (1) the accuracy of our distortion classification is higher than that of the state-of-the-art distortion classification methods, and (2) the proposed NR IQA method outperforms the state-of-the-art NR IQA methods in quantifying the image quality.",
        "link": "https://dl.acm.org/doi/10.1145/3468872",
        "category": "Databases"
    },
    {
        "title": "Explainable Data Analytics for Disease and Healthcare Informatics",
        "authors": "['Carson K. Leung', 'Daryl L.x. Fung', 'Daniel Mai', 'Qi Wen', 'Jason Tran', 'Joglas Souza']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "With advancements in technology, huge volumes of valuable data have been generated and collected at a rapid velocity from a wide variety of rich data sources. Examples of these valuable data include healthcare and disease data such as privacy-preserving statistics on patients who suffered from diseases like the coronavirus disease 2019 (COVID-19). Analyzing these data can be for social good. For instance, data analytics on the healthcare and disease data often leads to the discovery of useful information and knowledge about the disease. Explainable artificial intelligence (XAI) further enhances the interpretability of the discovered knowledge. Consequently, the explainable data analytics helps people to get a better understanding of the disease, which may inspire them to take part in preventing, detecting, controlling and combating the disease. In this paper, we present an explainable data analytics system for disease and healthcare informatics. Our system consists of two key components. The predictor component analyzes and mines historical disease and healthcare data for making predictions on future data. Although huge volumes of disease and healthcare data have been generated, volumes of available data may vary partially due to privacy concerns. So, the predictor makes predictions with different methods. It uses random forest With sufficient data and neural network-based few-shot learning (FSL) with limited data. The explainer component provides the general model reasoning and a meaningful explanation for specific predictions. As a database engineering application, we evaluate our system by applying it to real-life COVID-19 data. Evaluation results show the practicality of our system in explainable data analytics for disease and healthcare informatics.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472175",
        "category": "Databases"
    },
    {
        "title": "The Use of Open Electronic Scientific and Educational Systems to Support the Professional Activities of Research and Teaching Staff of Ukrainian Universities and Scientific Institutions",
        "authors": "['Oleg M. Spirin', 'Olga V. Matviienko', 'Svitlana M. Ivanova', 'Oksana V. Ovcharuk', 'Iryna S. Mintii', 'Iryna V. Ivaniuk', 'Liliia A. Luparenko']",
        "date": "December 2021",
        "source": "DHW 2021: Digital Humanities Workshop",
        "abstract": "The article is devoted to the analysis and description of open electronic scientific and educational systems (OESES) and their use by scientific and pedagogical staff in Ukrainian universities and research institutions. The contribution of the use of open electronic systems by scientists and professors into the professional activity is considered. The results of experimental verification of the use of OESES and their impact on the research competence of teachers and researchers are presented. Based on the analysis of domestic and international research, the authors’ own experience, the concept of open electronic educational systems designed to effectively organize and support research in education, pedagogy, social and behavioral sciences. The results of experimental research on the development of information and research competence of Ukrainian teachers and researchers during the use of open electronic systems are presented. The necessity of creating an environment for the development of information and research competence of university teachers and scientists is substantiated. The scientific novelty is based on the obtained results and is that it is proposed to include in the structure of such environment the following elements: scientific electronic libraries, electronic open journal systems (EOJS), scientometric databases, electronic social networks, and quality assessment systems for pedagogical tests, digital identification systems for scientists and scientific publications, software verification uniqueness of texts. Today, these tools are in demand and widely used for the organization of scientific and educational activities in educational institutions and research institutions around the world.",
        "link": "https://dl.acm.org/doi/10.1145/3526242.3526261",
        "category": "Databases"
    },
    {
        "title": "A review of the Text to SQL Frameworks",
        "authors": "['Karam Ahkouk', 'Machkour Mustapha', 'Majhadi Khadija', 'Mama Rachid']",
        "date": "April 2021",
        "source": "NISS '21: Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security",
        "abstract": "The use of relational databases and the extraction of the stored data are generally carried out using queries expressed in a query language such as SQL (Structured Query Language). In particular, people with limited knowledge on databases are unable to write such requests. To resolve this problem, the use of natural language (NL) to interact with these systems will be the easiest alternative. Using natural language interfaces for databases (NLIDB) is the key solution to make the translation of natural languages like English to SQL queries possible. This can help to generalize access to databases for different types of users regardless their technical level of knowledge on SQL. This paper will present a short review of our already proposed framework that deal with Natural Language translation to Database queries.",
        "link": "https://dl.acm.org/doi/10.1145/3454127.3457619",
        "category": "Databases"
    },
    {
        "title": "PigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus",
        "authors": "['Aleksey Charapko', 'Ailidani Ailijiang', 'Murat Demirbas']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Strongly consistent replication helps keep application logic simple and provides significant benefits for correctness and manageability. Unfortunately, the adoption of strongly-consistent replication protocols has been curbed due to their limited scalability and performance. To alleviate the leader bottleneck in strongly-consistent replication protocols, we introduce Pig, an in-protocol communication aggregation and piggybacking technique. Pig employs randomly selected nodes from follower subgroups to relay the leader's message to the rest of the followers in the subgroup, and to perform in-network aggregation of acknowledgments back from these followers. By randomly alternating the relay nodes across replication operations, Pig shields the relay nodes as well as the leader from becoming hotspots and improves throughput scalability. We showcase Pig in the context of classical Paxos protocols employed for strongly consistent replication by many cloud computing services and databases. We implement and evaluate PigPaxos, in comparison to Paxos and EPaxos protocols under various workloads over clusters of size 5 to 25 nodes. We show that the aggregation at the relay has little latency overhead, and PigPaxos can provide more than 3 folds improved throughput over Paxos and EPaxos with little latency deterioration. We support our experimental observations with the analytical modeling of the bottlenecks and show that the communication bottlenecks are minimized when employing only one randomly rotating relay node.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452834",
        "category": "Databases"
    },
    {
        "title": "Citus: Distributed PostgreSQL for Data-Intensive Applications",
        "authors": "['Umur Cubukcu', 'Ozgun Erdogan', 'Sumedh Pathak', 'Sudhakar Sannakkayala', 'Marco Slot']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Citus is an open source distributed database engine for PostgreSQL that is implemented as an extension. Citus gives users the ability to distribute data, queries, and transactions in PostgreSQL across a cluster of PostgreSQL servers to handle the needs of data-intensive applications. The development of Citus has largely been driven by conversations with companies looking to scale PostgreSQL beyond a single server and their workload requirements. This paper describes the requirements of four common workload patterns and how Citus addresses those requirements. It also shares benchmark results demonstrating the performance and scalability of Citus in each of the workload patterns and describes how Microsoft uses Citus to address one of its most challenging data problems.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457551",
        "category": "Databases"
    },
    {
        "title": "Model-theoretic Characterizations of Rule-based Ontologies",
        "authors": "['Marco Console', 'Phokion G. Kolaitis', 'Andreas Pieris']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "An ontology specifies an abstract model of a domain of interest via a formal language that is typically based on logic. Although description logics are popular formalisms for modeling ontologies, tuple-generating dependencies (tgds), originally introduced as a unifying framework for database integrity constraints, and later on used in data exchange and integration, are also well suited for modeling ontologies that are intended for data-intensive tasks. The reason is that, unlike description logics, tgds can easily handle higher-arity relations that naturally occur in relational databases. In recent years, there has been an extensive study of tgd-ontologies and of their applications to several different data-intensive tasks. However, the fundamental question of whether the expressive power of tgd-ontologies can be characterized in terms of model-theoretic properties remains largely unexplored. We establish several characterizations of tgd-ontologies, including characterizations of ontologies specified by such central classes of tgds as full, linear, guarded, and frontier-guarded tgds. Our characterizations use the well-known notions of critical instance and direct product, as well as a novel locality property for tgd-ontologies. We further use this locality property to decide whether an ontology expressed by frontier-guarded (respectively, guarded) tgds can be expressed by tgds in the weaker class of guarded (respectively, linear) tgds, and effectively construct such an equivalent ontology if one exists.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458310",
        "category": "Databases"
    },
    {
        "title": "The Case for SIMDified Analytical Query Processing on GPUs",
        "authors": "['Johannes Fett', 'Annett Ungethüm', 'Dirk Habich', 'Wolfgang Lehner']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "Data-level parallelism (DLP) is a heavily used hardware-driven parallelization technique to optimize the analytical query processing, especially in in-memory column stores. This kind of parallelism is characterized by executing essentially the same operation on different data elements simultaneously. Besides Single Instruction Multiple Data (SIMD) extensions on common x86-processors, GPUs also provide DLP but with a different execution model called Single Instruction Multiple Threads (SIMT), where multiple scalar threads are executed in a SIMD manner. Unfortunately, a complete GPU-specific implementation of all query operators has to be set up, since the state of the vectorized implementations cannot be ported from x86-processors to GPUs right now. To avoid this implementation effort, we present our vision to virtualize GPUs as virtual vector engines with software-defined SIMD instructions and to specialize hardware-oblivious vectorized operators to GPUs using our Template Vector Library (TVL) in this paper.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466015",
        "category": "Databases"
    },
    {
        "title": "SAND: a static analysis approach for detecting SQL antipatterns",
        "authors": "['Yingjun Lyu', 'Sasha Volokh', 'William G. J. Halfond', 'Omer Tripp']",
        "date": "July 2021",
        "source": "ISSTA 2021: Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "abstract": "Local databases underpin important features in many mobile applications, such as responsiveness in the face of poor connectivity. However, failure to use such databases correctly can lead to high resource consumption or even security vulnerabilities. We present SAND, an extensible static analysis approach that checks for misuse of local databases, also known as SQL antipatterns, in mobile apps. SAND features novel abstractions for common forms of application/database interactions, which enables concise and precise specification of the antipatterns that SAND checks for. To validate the efficacy of SAND, we have experimented with a diverse suite of 1,000 Android apps. We show that the abstractions that power SAND allow concise specification of all the known antipatterns from the literature (12-74 LOC), and that the antipatterns are modeled accurately (99.4-100% precision). As for performance, SAND requires on average 41 seconds to complete a scan on a mobile app.",
        "link": "https://dl.acm.org/doi/10.1145/3460319.3464818",
        "category": "Databases"
    },
    {
        "title": "Bayesian Covariance Representation with Global Informative Prior for 3D Action Recognition",
        "authors": "['Jianhai Zhang', 'Zhiyong Feng', 'Yong Su', 'Meng Xing']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "For the merits of high-order statistics and Riemannian geometry, covariance matrix has become a generic feature representation for action recognition. An independent action can be represented by an empirical statistics over all of its pose samples. Two major problems of covariance include the following: (1) it is prone to be singular so that actions fail to be represented properly, and (2) it is short of global action/pose-aware information so that expressive and discriminative power is limited. In this article, we propose a novel Bayesian covariance representation by a prior regularization method to solve the preceding problems. Specifically, covariance is viewed as a parametric maximum likelihood estimate of Gaussian distribution over local poses from an independent action. Then, a Global Informative Prior (GIP) is generated over global poses with sufficient statistics to regularize covariance. In this way, (1) singularity is greatly relieved due to sufficient statistics, (2) global pose information of GIP makes Bayesian covariance theoretically equivalent to a saliency weighting covariance over global action poses so that discriminative characteristics of actions can be represented more clearly. Experimental results show that our Bayesian covariance with GIP efficiently improves the performance of action recognition. In some databases, it outperforms the state-of-the-art variant methods that are based on kernels, temporal-order structures, and saliency weighting attentions, among others.",
        "link": "https://dl.acm.org/doi/10.1145/3460235",
        "category": "Databases"
    },
    {
        "title": "Insights from Student Solutions to MongoDB Homework Problems",
        "authors": "['Ridha Alkhabaz', 'Seth Poulsen', 'Mei Chen', 'Abdussalam Alawini']",
        "date": "June 2021",
        "source": "ITiCSE '21: Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1",
        "abstract": "We analyze submissions for homework assignments of 527 students in an upper-level database course offered at the University of Illinois at Urbana-Champaign. The ability to query databases is becoming a crucial skill for technology professionals and academics. Although we observe a large demand for teaching database skills, there is little research on database education. Also, despite the industry's continued demand for NoSQL databases, we have virtually no research on the matter of how students learn NoSQL databases, such as MongoDB. In this paper, we offer an in-depth analysis of errors committed by students working on MongoDB homework assignments over the course of two semesters. We show that as students use more advanced MongoDB operators, they make more Reference errors. Additionally, when students face a new functionality of MongoDB operators, such as \\texttt\\$group operator, they usually take time to understand it but do not make the same errors again in later problems. Finally, our analysis suggests that students struggle with advanced concepts for a comparable amount of time. Our results suggest that instructors should allocate more time and effort for the discussed topics in our paper.",
        "link": "https://dl.acm.org/doi/10.1145/3430665.3456308",
        "category": "Databases"
    },
    {
        "title": "Interpretable Graph Similarity Computation via Differentiable Optimal Alignment of Node Embeddings",
        "authors": "['Khoa D. Doan', 'Saurav Manchanda', 'Suchismit Mahapatra', 'Chandan K. Reddy']",
        "date": "July 2021",
        "source": "SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "abstract": "Computing graph similarity is an important task in many graph-related applications such as retrieval in graph databases or graph clustering. While numerous measures have been proposed to capture the similarity between a pair of graphs, Graph Edit Distance (GED) and Maximum Common Subgraphs (MCS) are the two widely used measures in practice. GED and MCS are domain-agnostic measures of structural similarity between the graphs and define the similarity as a function of pairwise alignment of different entities (such as nodes, edges, and subgraphs) in the two graphs. The explicit explainability offered by the pairwise alignment provides transparency and justification of the similarity score, thus, GED and MCS have important practical applications. However, their exact computations are known to be NP-hard. While recently proposed neural-network based approximations have been shown to accurately compute these similarity scores, they have limited ability in providing comprehensive explanations compared to classical combinatorial algorithms, e.g., Beam search. This paper aims at efficiently approximating these domain-agnostic similarity measures through a neural network, and simultaneously learning the alignments (i.e., explanations) similar to those of classical intractable methods. Specifically, we formulate the similarity between a pair of graphs as the minimal \"transformation\" cost from one graph to another in the learnable node-embedding space. We show that, if node embedding is able to capture its neighborhood context closely, our proposed similarity function closely approximates both the alignment and the similarity score of classical methods. Furthermore, we also propose an efficient differentiable computation of our proposed objective for model training. Empirically, we demonstrate that the proposed method achieves up to 50%-100% reduction in the Mean Squared Error for the graph similarity approximation task and up to 20% improvement in the retrieval evaluation metrics for the graph retrieval task. The source code is available at https://github.com/khoadoan/GraphOTSim.",
        "link": "https://dl.acm.org/doi/10.1145/3404835.3462960",
        "category": "Databases"
    },
    {
        "title": "METRO: a generic graph neural network framework for multivariate time series forecasting",
        "authors": "['Yue Cui', 'Kai Zheng', 'Dingshan Cui', 'Jiandong Xie', 'Liwei Deng', 'Feiteng Huang', 'Xiaofang Zhou']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Multivariate time series forecasting has been drawing increasing attention due to its prevalent applications. It has been commonly assumed that leveraging latent dependencies between pairs of variables can enhance prediction accuracy. However, most existing methods suffer from static variable relevance modeling and ignorance of correlation between temporal scales, thereby failing to fully retain the dynamic and periodic interdependencies among variables, which are vital for long- and short-term forecasting. In this paper, we propose METRO, a generic framework with multi-scale temporal graphs neural networks, which models the dynamic and cross-scale variable correlations simultaneously. By representing the multivariate time series as a series of temporal graphs, both intra- and inter-step correlations can be well preserved via message-passing and node embedding update. To enable information propagation across temporal scales, we design a novel sampling strategy to align specific steps between higher and lower scales and fuse the cross-scale information efficiently. Moreover, we provide a modular interpretation of existing GNN-based time series forecasting works as specific instances under our framework. Extensive experiments conducted on four benchmark datasets demonstrate the effectiveness and efficiency of our approach. METRO has been successfully deployed onto the time series analytics platform of Huawei Cloud, where a one-month online test demonstrated that up to 20% relative improvement over state-of-the-art models w.r.t. RSE can be achieved.",
        "link": "https://dl.acm.org/doi/10.14778/3489496.3489503",
        "category": "Databases"
    },
    {
        "title": "MIDAS: Towards Efficient and Effective Maintenance of Canned Patterns in Visual Graph Query Interfaces",
        "authors": "['Kai Huang', 'Huey Eng Chua', 'Sourav S. Bhowmick', 'Byron Choi', 'Shuigeng Zhou']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Several visual graph query interfaces (a.k.a gui) expose a set of canned patterns (i.e., small subgraph patterns) to expedite subgraph query formulation by enabling pattern-at-a-time construction. Unfortunately, manual generation of canned patterns is not only labour intensive but also may lack diversity to support efficient visual formulation of a wide range of subgraph queries. Recent efforts have taken a data-driven approach to select high-quality canned patterns for a gui automatically from the underlying graph database. However, as the underlying database evolves, these selected patterns may become stale and adversely impact efficient query formulation. In this paper, we present a novel framework called Midas for efficient and effective maintenance of the canned patterns as the database evolves. Specifically, it adopts a selective maintenance strategy that guarantees progressive gain of coverage of the patterns without sacrificing their diversity and cognitive load. Experimental study with real-world datasets and visual graph interfaces demonstrates the effectiveness of Midas compared to static guis.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457251",
        "category": "Databases"
    },
    {
        "title": "Categorical Management of Multi-Model Data",
        "authors": "['Irena Holubova', 'Pavel Contos', 'Martin Svoboda']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "In this vision paper, we introduce an idea of a framework that would enable us to model, represent, and manage multi-model data in a unified and abstract way. Its core idea exploits constructs provided by category theory, which is sufficiently general but still simple enough to cover any of the logical data models used in contemporary databases. Focusing on promising features and taking into account mature and verified principles, we overview the key parts of the framework and outline open questions and research directions that need to be further investigated. The ultimate objective is to pursue the idea of a self-tuning system that would permit us to collapse the traditionally understood conceptual and logical layers into just a single model allowing for unified handling of schemas, data instances, as well as queries.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472166",
        "category": "Databases"
    },
    {
        "title": "Achieving low tail-latency and high scalability for serializable transactions in edge computing",
        "authors": "['Xusheng Chen', 'Haoze Song', 'Jianyu Jiang', 'Chaoyi Ruan', 'Cheng Li', 'Sen Wang', 'Gong Zhang', 'Reynold Cheng', 'Heming Cui']",
        "date": "April 2021",
        "source": "EuroSys '21: Proceedings of the Sixteenth European Conference on Computer Systems",
        "abstract": "A distributed database utilizing the wide-spread edge computing servers to provide low-latency data access with the serializability guarantee is highly desirable for emerging edge computing applications. In an edge database, nodes are divided into regions, and a transaction can be categorized as intra-region (IRT) or cross-region (CRT) based on whether it accesses data in different regions. In addition to serializability, we insist that a practical edge database should provide low tail latency for both IRTs and CRTs, and such low latency must be scalable to a large number of regions. Unfortunately, none of existing geo-replicated serializable databases or edge databases can meet such requirements. In this paper, we present Dast (Decentralized Anticipate and STretch), the first edge database that can meet the stringent performance requirements with serializability. Our key idea is to order transactions by anticipating when they are ready to execute: Dast binds an IRT to the latest timestamp and binds a CRT to a future timestamp to avoid the coordination of CRTs blocking IRTs. Dast also carries a new stretchable clock abstraction to tolerate inaccurate anticipations and to handle cross-region data reads. Our evaluation shows that, compared to three relevant serializable databases, Dast's 99-percentile latency was 87.9%~93.2% lower for IRTs and 27.7%~70.4% lower for CRTs; Dast's low latency is scalable to a large number of regions.",
        "link": "https://dl.acm.org/doi/10.1145/3447786.3456238",
        "category": "Databases"
    },
    {
        "title": "Design and implementation of CTD profile observation data accumulation system based on MySQL",
        "authors": "['Xing-min Li', 'Tao Dong', 'Xin-peng Wang', 'Li-shan Ma']",
        "date": "December 2021",
        "source": "CSAI '21: Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence",
        "abstract": "In order to realize the accumulation of sea temperature, conductivity, pressure, depth, salinity, density and sound profile observation data, a ocean observation data storage system based on database is designed. For efficiently realizing accumulation of ocean observation data, the system makes full use of the advantages of MYSQL database management platform, and orderly stores tens of thousands ofConductivity-Temperature-Depth(CTD) profile observation data. At the same time, the data quality control will be applied to the received hydrological observation data, which improves the quality of the data stored in the database and enhances the data usability. After the system was developed, a simulation environment was set up to test the system. The results show that the system hasrational function design and stable operation, which can well realize the accumulation of CTD profile observation data. The realization of ocean hydrologic profile observation data accumulation provides reliable data source for the subsequent in-depth data-mining and utilization of ocean hydrologic observation data.",
        "link": "https://dl.acm.org/doi/10.1145/3507548.3507590",
        "category": "Databases"
    },
    {
        "title": "FastVer: Making Data Integrity a Commodity",
        "authors": "['Arvind Arasu', 'Badrish Chandramouli', 'Johannes Gehrke', 'Esha Ghosh', 'Donald Kossmann', 'Jonathan Protzenko', 'Ravi Ramamurthy', 'Tahina Ramananandro', 'Aseem Rastogi', 'Srinath Setty', 'Nikhil Swamy', 'Alexander van Renen', 'Min Xu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We present FastVer, a high-performance key-value store with strong data integrity guarantees. FastVer is built as an extension of FASTER, an open-source, high-performance key-value store. It offers the same key-value API as FASTER plus an additional verify() method that detects if an unauthorized attacker tampered with the database and checks whether results of all read operations are consistent with historical updates. FastVer is based on a novel approach that combines the advantages of Merkle trees and deferred memory verification. We show that this approach achieves one to two orders of magnitudes higher throughputs than traditional approaches based on either Merkle trees or memory verification. We have formally proven the correctness of our approach in a proof assistant, ensuring that verify() detects any inconsistencies, except if a collision can be found on a cryptographic hash.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457312",
        "category": "Databases"
    },
    {
        "title": "Building Advanced SQL Analytics From Low-Level Plan Operators",
        "authors": "['André Kohn', 'Viktor Leis', 'Thomas Neumann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Analytical queries virtually always involve aggregation and statistics. SQL offers a wide range of functionalities to summarize data such as associative aggregates, distinct aggregates, ordered-set aggregates, grouping sets, and window functions. In this work, we propose a unified framework for advanced statistics that composes all flavors of complex SQL aggregates from low-level plan operators. These operators can reuse materialized intermediate results, which decouples monolithic aggregation logic and speeds up complex multi-expression queries. The contribution is therefore twofold: our framework modularizes aggregate implementations, and outperforms traditional systems whenever multiple aggregates are combined. We integrated our approach into the high-performance database system Umbra and experimentally show that we compute complex aggregates faster than the state-of-the-art HyPer system.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457288",
        "category": "Databases"
    },
    {
        "title": "Facial-expression-aware Emotional Color Transfer Based on Convolutional Neural Network",
        "authors": "['Shiguang Liu', 'Huixin Wang', 'Min Pei']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Emotional color transfer aims to change the evoked emotion of a source image to that of a target image by adjusting color distribution. Most of existing emotional color transfer methods only consider the low-level visual features of an image and ignore the facial expression features when the image contains a human face, which would cause incorrect emotion evaluation for the given image. In addition, previous emotional color transfer methods may easily result in ambiguity between the emotion of resulting image and target image. For example, if the background of the target image is dark while the facial expression is happiness, then previous methods would directly transfer dark color to the source image, neglecting the facial emotion in the image. To solve this problem, we propose a new facial-expression-aware emotional color transfer framework. Given a target image with facial expression features, we first predict the facial emotion label of the image through the emotion classification network. Then, facial emotion labels are matched with pre-trained emotional color transfer models. Finally, we use the matched emotion model to transfer the color of the target image to the source image. Considering none of the existing emotion image databases, which focus on images that contain face and background, we built an emotion database for our new emotional color transfer framework that is called “Face-Emotion database.” Experiments demonstrate that our method can successfully capture and transfer facial emotions, outperforming state-of-the-art methods.",
        "link": "https://dl.acm.org/doi/10.1145/3464382",
        "category": "Databases"
    },
    {
        "title": "A Comparative Study of MongoDB, ArangoDB and CouchDB for Big Data Storage",
        "authors": "['Konstanitnos Mavrogiorgos', 'Athanasios Kiourtis', 'Argyro Mavrogiorgou', 'Dimosthenis Kyriazis']",
        "date": "August 2021",
        "source": "ICCBDC '21: Proceedings of the 2021 5th International Conference on Cloud and Big Data Computing",
        "abstract": "A distinctive aspect of the current era is the ferocious amount of data that is generated and processed in a daily basis. There is no wonder that this epoch is generally characterized as the “Era of Big Data”. Thus, many enterprises and research initiatives strive to find a way to effectively and efficiently collect, store and analyze Big Data in order to improve their services and make efficient decisions. Those approaches refer to several domains such as healthcare, transportation, governance, or insurance. Towards this direction, in this paper we contribute into the selection of the most appropriate database for efficiently storing and retrieving Big Data. More specifically, taking into account the nature of Big Data and the main categories of databases that currently exist, three (3) NoSQL document-based databases were considered for this comparative study, namely the ArangoDB, the MongoDB and the CouchDB. The performance of these databases was measured based on specific metrics and criteria, including the total execution time for the same CRUD operations and their corresponding demands for resources, concluding to the most suitable database for storing Big Data.",
        "link": "https://dl.acm.org/doi/10.1145/3481646.3481648",
        "category": "Databases"
    },
    {
        "title": "Self-Tuning Query Scheduling for Analytical Workloads",
        "authors": "['Benjamin Wagner', 'André Kohn', 'Thomas Neumann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Most database systems delegate scheduling decisions to the operating system. While such an approach simplifies the overall database design, it also entails problems. Adaptive resource allocation becomes hard in the face of concurrent queries. Furthermore, incorporating domain knowledge to improve query scheduling is difficult. To mitigate these problems, many modern systems employ forms of task-based parallelism. The execution of a single query is broken up into small, independent chunks of work (tasks). Now, fine-grained scheduling decisions based on these tasks are the responsibility of the database system. Despite being commonplace, little work has focused on the opportunities arising from this execution model. In this paper, we show how task-based scheduling in database systems opens up new areas for optimization. We present a novel lock-free, self-tuning stride scheduler that optimizes query latencies for analytical workloads. By adaptively managing query priorities and task granularity, we provide high scheduling elasticity. By incorporating domain knowledge into the scheduling decisions, our system is able to cope with workloads that other systems struggle with. Even at high load, we retain near optimal latencies for short running queries. Compared to traditional database systems, our design often improves tail latencies by more than 10x.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457260",
        "category": "Databases"
    },
    {
        "title": "All in One: Design, Verification, and Implementation of SNOW-optimal Read Atomic Transactions",
        "authors": "['Si Liu']",
        "date": "None",
        "source": "ACM Transactions on Software Engineering and Methodology",
        "abstract": "Distributed read atomic transactions are important building blocks of modern cloud databases that magnificently bridge the gap between data availability and strong data consistency. The performance of their transactional reads is particularly critical to the overall system performance, as many real-world database workloads are dominated by reads. Following the SNOW design principle for optimal reads, we develop LORA, a novel SNOW-optimal algorithm for distributed read atomic transactions. LORA completes its reads in exactly one round trip, even in the presence of conflicting writes, without imposing additional overhead to the communication, and it outperforms the state-of-the-art read atomic algorithms.To guide LORA’s development, we present a rewriting-logic-based framework and toolkit for design, verification, implementation, and evaluation of distributed databases. Within the framework, we formalize LORA and mathematically prove its data consistency guarantees. We also apply automatic model checking and statistical verification to validate our proofs and to estimate LORA’s performance. We additionally generate from the formal model a correct-by-construction distributed implementation for testing and performance evaluation under realistic deployments. Our design-level and implementation-based experimental results are consistent, which together demonstrate LORA’s promising data consistency and performance achievement.",
        "link": "https://dl.acm.org/doi/10.1145/3494517",
        "category": "Databases"
    },
    {
        "title": "Gene Ranking based on Paths from Phenotypes to Genes on Knowledge Graph",
        "authors": "['Atsuko Yamaguchi', 'Jae-Moon Shin', 'Toyofumi Fujiwara']",
        "date": "December 2021",
        "source": "IJCKG '21: Proceedings of the 10th International Joint Conference on Knowledge Graphs",
        "abstract": "Whole exome sequencing has been widely used to make a diagnosis of a genetic disease efficiently, especially for rare diseases. To support the identification of the gene that causes the disease, a gene ranking system from symptoms is essential to narrow down many genes obtained from exome analysis. Therefore, this paper proposed a method for ranking genes by symptoms based on paths from phenotypes to genes on a knowledge graph. First, we constructed a knowledge graph from resources about phenotypes, diseases, and genes. Then, we designed an algorithm to rank genes from phenotypes by following paths from phenotypes to genes on the knowledge graph. Furthermore, we evaluated the performance of the proposed method by comparing it with similar existing tools.",
        "link": "https://dl.acm.org/doi/10.1145/3502223.3502240",
        "category": "Databases"
    },
    {
        "title": "DIY: Assessing the Correctness of Natural Language to SQL Systems",
        "authors": "['Arpit Narechania', 'Adam Fourney', 'Bongshin Lee', 'Gonzalo Ramos']",
        "date": "April 2021",
        "source": "IUI '21: 26th International Conference on Intelligent User Interfaces",
        "abstract": "Designing natural language interfaces for querying databases remains an important goal pursued by researchers in natural language processing, databases, and HCI. These systems receive natural language as input, translate it into a formal database query, and execute the query to compute a result. Because the responses from these systems are not always correct, it is important to provide people with mechanisms to assess the correctness of the generated query and computed result. However, this assessment can be challenging for people who lack expertise in query languages. We present Debug-It-Yourself (DIY), an interactive technique that enables users to assess the responses from a state-of-the-art natural language to SQL (NL2SQL) system for correctness and, if possible, fix errors. DIY provides users with a sandbox where they can interact with (1) the mappings between the question and the generated query, (2) a small-but-relevant subset of the underlying database, and (3) a multi-modal explanation of the generated query. End-users can then employ a back-of-the-envelope calculation debugging strategy to evaluate the system’s response. Through an exploratory study with 12 users, we investigate how DIY helps users assess the correctness of the system’s answers and detect & fix errors. Our observations reveal the benefits of DIY while providing insights about end-user debugging strategies and underscore opportunities for further improving the user experience.",
        "link": "https://dl.acm.org/doi/10.1145/3397481.3450667",
        "category": "Databases"
    },
    {
        "title": "An Agile Data Warehouse Virtualization Framework for ROLAP Server",
        "authors": "['André Andrade Menolli', 'Ricardo Gonçalves Coelho', 'Glauco Carlos Silva', 'Elielson Barbosa']",
        "date": "June 2021",
        "source": "SBSI '21: Proceedings of the XVII Brazilian Symposium on Information Systems",
        "abstract": "In order to adapt to a competitive business scenario, the decision-making needs to be fast and reliable. In this panorama, agile business intelligence emerges as a resource to provide agile solutions. To achieve agile business intelligence solutions, organizations consider real-time data warehousing a powerful technique. Thus, we propose in this paper a framework based on data warehouse virtualization and real-time data warehousing concepts, called Agile ROLAP. The framework is comprised of an approach designed to be compatible with the main consolidated DW concepts. Furthermore, a set of components that enable the deployment of each step of the Agile ROLAP process was implemented. We evaluated our proposed approach in an experimental study where we deployed dimensional models from three distinct databases. It was analyzed the approach viability and the performance through query performance. The results indicate that the approach is viable, and the performance is satisfactory for no very large databases.",
        "link": "https://dl.acm.org/doi/10.1145/3466933.3466980",
        "category": "Databases"
    },
    {
        "title": "Privacy-Preserving Synthetic Location Data in the Real World",
        "authors": "['Teddy Cunningham', 'Graham Cormode', 'Hakan Ferhatosmanoglu']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "Sharing sensitive data is vital in enabling many modern data analysis and machine learning tasks. However, current methods for data release are insufficiently accurate or granular to provide meaningful utility, and they carry a high risk of deanonymization or membership inference attacks. In this paper, we propose a differentially private synthetic data generation solution with a focus on the compelling domain of location data. We present two methods with high practical utility for generating synthetic location data from real locations, both of which protect the existence and true location of each individual in the original dataset. Our first, partitioning-based approach introduces a novel method for privately generating point data using kernel density estimation, in addition to employing private adaptations of classic statistical techniques, such as clustering, for private partitioning. Our second, network-based approach incorporates public geographic information, such as the road network of a city, to constrain the bounds of synthetic data points and hence improve the accuracy of the synthetic data. Both methods satisfy the requirements of differential privacy, while also enabling accurate generation of synthetic data that aims to preserve the distribution of the real locations. We conduct experiments using three large-scale location datasets to show that the proposed solutions generate synthetic location data with high utility and strong similarity to the real datasets. We highlight some practical applications for our work by applying our synthetic data to a range of location analytics queries, and we demonstrate that our synthetic data produces near-identical answers to the same queries compared to when real data is used. Our results show that the proposed approaches are practical solutions for sharing and analyzing sensitive location data privately.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470893",
        "category": "Databases"
    },
    {
        "title": "NIR-Tree: A Non-Intersecting R-Tree",
        "authors": "['Kyle Langendoen', 'Brad Glasbergen', 'Khuzaima Daudjee']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Indexes for multidimensional data based on the R-Tree are popularly used by databases for a wide range of applications. Such index trees support point and range queries but are costly to construct over datasets of millions of points. We present the Non-Intersecting R-Tree (NIR-Tree), a novel insert-efficient, in-memory, multidimensional index that uses bounding polygons to provide efficient point and range query performance while indexing data at least an order of magnitude faster. The NIR-Tree leverages non-intersecting bounding polygons to reduce the number of nodes accessed during queries, compared to existing R-family indexes. Our experiments demonstrate that inserting into a NIR-Tree is 27 × faster than the ubiquitous R*-Tree, with point queries completing 2 × faster and range queries executing just as quickly.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468818",
        "category": "Databases"
    },
    {
        "title": "Compressed Oblivious Encoding for Homomorphically Encrypted Search",
        "authors": "['Seung Geol Choi', 'Dana Dachman-Soled', 'S. Dov Gordon', 'Linsheng Liu', 'Arkady Yerukhimovich']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Fully homomorphic encryption (FHE) enables a simple, attractive framework for secure search. Compared to other secure search systems, no costly setup procedure is necessary; it is sufficient for the client merely to upload the encrypted database to the server. Confidentiality is provided because the server works only on the encrypted query and records. While the search functionality is enabled by the full homomorphism of the encryption scheme. For this reason, researchers have been paying increasing attention to this problem. Since Akavia et al. (CCS 2018) presented a framework for secure search on FHE encrypted data and gave a working implementation called SPiRiT, several more efficient realizations have been proposed. In this paper, we identify the main bottlenecks of this framework and show how to significantly improve the performance of FHE-base secure search. In particular, To retrieve l matching items, the existing framework needs to repeat the protocol l times sequentially. In our new framework, all matching items are retrieved in parallel in a single protocol execution. The most recent work by Wren et al. (CCS 2020) requires O(n) multiplications to compute the first matching index. Our solution requires no homomorphic multiplication, instead using only additions and scalar multiplications to encode all matching indices. Our implementation and experiments show that to fetch 16 matching records, our system gives an 1800X speed-up over the state of the art in fetching the query results resulting in a 26X speed-up for the full search functionality.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484792",
        "category": "Databases"
    },
    {
        "title": "Spitfire: A Three-Tier Buffer Manager for Volatile and Non-Volatile Memory",
        "authors": "['Xinjing Zhou', 'Joy Arulraj', 'Andrew Pavlo', 'David Cohen']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The design of the buffer manager in database management systems (DBMSs) is influenced by the performance characteristics of volatile memory (i.e., DRAM) and non-volatile storage (e.g., SSD). The key design assumptions have been that the data must be migrated to DRAM for the DBMS to operate on it and that storage is orders of magnitude slower than DRAM. But the arrival of new non-volatile memory (NVM) technologies that are nearly as fast as DRAM invalidates these previous assumptions. Researchers have recently designed Hymem, a novel buffer manager for a three-tier storage hierarchy comprising of DRAM, NVM, and SSD. Hymem supports cache-line-grained loading and an NVM-aware data migration policy. While these optimizations improve its throughput, Hymem suffers from two limitations. First, it is a single-threaded buffer manager. Second, it is evaluated on an NVM emulation platform. These limitations constrain the utility of the insights obtained using Hymem. In this paper, we present Spitfire, a multi-threaded, three-tier buffer manager that is evaluated on Optane Persistent Memory Modules, an NVM technology that is now being shipped by Intel. We introduce a general framework for reasoning about data migration in a multi-tier storage hierarchy. We illustrate the limitations of the optimizations used in Hymem on Optane and then discuss how Spitfire circumvents them. We demonstrate that the data migration policy has to be tailored based on the characteristics of the devices and the workload. Given this, we present a machine learning technique for automatically adapting the policy for an arbitrary workload and storage hierarchy. Our experiments show that Spitfire works well across different workloads and storage hierarchies.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452819",
        "category": "Databases"
    },
    {
        "title": "Probabilistic Deep Learning for Electric-Vehicle Energy-Use Prediction",
        "authors": "['Linas Petkevicius', 'Simonas Saltenis', 'Alminas Civilis', 'Kristian Torp']",
        "date": "August 2021",
        "source": "SSTD '21: Proceedings of the 17th International Symposium on Spatial and Temporal Databases",
        "abstract": "The continued spread of electric vehicles raises new challenges for the supporting digital infrastructure. For example, long-distance route planning for such vehicles relies on the prediction of both the expected travel time as well as energy use. We envision a two-tier architecture to produce such predictions. First, a routing and travel-time-prediction subsystem generates a suggested route and predicts how the speed will vary along the route. Next, the expected energy use is predicted from the speed profile and other contextual characteristics, such as weather information and slope.  To this end, the paper proposes deep-learning models that are built from EV tracking data. First, as the speed profile of a route is one of the main predictors for energy use, different simple ways to build speed profiles are explored. Next, eight different deep-learning models for energy-use prediction are proposed. Four of the models are probabilistic in that they predict not a single-point estimate but parameters of a probability distribution of energy use on the route. This is particularly relevant when predicting EV energy use, which is highly sensitive to many input characteristics and, thus, can hardly be predicted precisely. Extensive experiments with two real-world EV tracking datasets validate the proposed methods. The code for this research has been made available on GitHub.",
        "link": "https://dl.acm.org/doi/10.1145/3469830.3470915",
        "category": "Databases"
    },
    {
        "title": "Optimizing Transaction Schedules on Universal Quantum Computers via Code Generation for Grover’s Search Algorithm",
        "authors": "['Sven Groppe', 'Jinghua Groppe']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Quantum computers are known to be efficient for solving combinatorial problems like finding optimal schedules for processing transactions in parallel without blocking. We show how Grover’s search algorithm for quantum computers can be applied for finding an optimal transaction schedule via generating code from the problem instance. We compare our approach with existing approaches for traditional computers and quantum annealers in terms of preprocessing, runtime, space and code length complexity. Furthermore, we show by experiments the expected number of optimal solutions of this problem as well as suboptimal ones. With the help of an estimator of the number of solutions, we further speed up our optimizer for optimal and suboptimal transaction schedules.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472164",
        "category": "Databases"
    },
    {
        "title": "Structure and Complexity of Bag Consistency",
        "authors": "['Albert Atserias', 'Phokion G. Kolaitis']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Since the early days of relational databases, it was realized that acyclic hypergraphs give rise to database schemas with desirable structural and algorithmic properties. In a by-now classical paper, Beeri, Fagin, Maier, and Yannakakis established several different equivalent characterizations of acyclicity; in particular, they showed that the sets of attributes of a schema form an acyclic hypergraph if and only if the local-to-global consistency property for relations over that schema holds, which means that every collection of pairwise consistent relations over the schema is globally consistent. Even though real-life databases consist of bags (multisets), there has not been a study of the interplay between local consistency and global consistency for bags. We embark on such a study here and we first show that the sets of attributes of a schema form an acyclic hypergraph if and only if the local-to-global consistency property for bags over that schema holds. After this, we explore algorithmic aspects of global consistency for bags by analyzing the computational complexity of the global consistency problem for bags: given a collection of bags, are these bags globally consistent? We show that this problem is in NP, even when the schema is part of the input. We then establish the following dichotomy theorem for fixed schemas: if the schema is acyclic, then the global consistency problem for bags is solvable in polynomial time, while if the schema is cyclic, then the global consistency problem for bags is NP-complete. The latter result contrasts sharply with the state of affairs for relations, where, for each fixed schema, the global consistency problem for relations is solvable in polynomial time.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458329",
        "category": "Databases"
    },
    {
        "title": "Shedding Light on Opaque Application Queries",
        "authors": "['Kapil Khurana', 'Jayant R. Haritsa']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We investigate a new query reverse-engineering problem of unmasking SQL queries hidden within database applications. The diverse use-cases for this problem range from resurrecting legacy code to query rewriting. As a first step in addressing the unmasking challenge, we present UNMASQUE, an active-learning extraction algorithm that can expose a basal class of hidden warehouse queries. A special feature of our design is that the extraction is non-invasive wrt the application, examining only the results obtained from repeated executions on databases derived with a combination of data mutation and data generation techniques. Further, potent optimizations are incorporated to minimize the extraction overheads. A detailed evaluation over applications hosting hidden SQL queries, or their imperative versions, demonstrates that UNMASQUE correctly and efficiently extracts these queries.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457252",
        "category": "Databases"
    },
    {
        "title": "Expressiveness within Sequence Datalog",
        "authors": "['Heba Aamer', 'Jan Hidders', 'Jan Paredaens', 'Jan Van den Bussche']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Motivated by old and new applications, we investigate Datalog as a language for sequence databases. We reconsider classical features of Datalog programs, such as negation, recursion, intermediate predicates, and relations of higher arities. We also consider new features that are useful for sequences, notably, equations between path expressions, and \"packing''. Our goal is to clarify the relative expressiveness of all these different features, in the context of sequences. Towards our goal, we establish a number of redundancy and primitivity results, showing that certain features can, or cannot, be expressed in terms of other features. These results paint a complete picture of the expressiveness relationships among all possible Sequence Datalog fragments that can be formed using the six features that we consider.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458327",
        "category": "Databases"
    },
    {
        "title": "Secure Big Data Management based on Secret Sharing and Verifiable Computing",
        "authors": "['Momoko Shiraishi']",
        "date": "August 2021",
        "source": "ICCBDC '21: Proceedings of the 2021 5th International Conference on Cloud and Big Data Computing",
        "abstract": "Of late, more and more sensitive data have been connected with cyberspace, which has led to an increase in the importance of secure cloud computing. This paper shows an overall scheme of securely managing big data in a cloud supported by two technological methods, secret sharing and verifiable computing. While each of them was implemented and analyzed individually in previous works, this paper shows a comprehensive framework combining them to realize both security and efficiency in cloud data management. Specifically, secret sharing enables hiding the outsourced encrypted data from any unauthorized entity, and verifiable computing enable resource-constraint users to access the database in which the correctness is undoubtedly confirmed. This paper also compares the database scheme based on blockchain and the proposed method in terms of performance. The analysis shows that the proposed scheme entirely based on the conventional Internet computes the operations lighter than the blockchain scheme.",
        "link": "https://dl.acm.org/doi/10.1145/3481646.3481650",
        "category": "Databases"
    },
    {
        "title": "Language-agnostic integrated queries in a managed polyglot runtime",
        "authors": "['Filippo Schiavio', 'Daniele Bonetta', 'Walter Binder']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Language-integrated query (LINQ) frameworks offer a convenient programming abstraction for processing in-memory collections of data, allowing developers to concisely express declarative queries using general-purpose programming languages. Existing LINQ frameworks rely on the well-defined type system of statically-typed languages such as C# or Java to perform query compilation and execution. As a consequence of this design, they do not support dynamic languages such as Python, R, or JavaScript. Such languages are however very popular among data scientists, who would certainly benefit from LINQ frameworks in data analytics applications.In this work we bridge the gap between dynamic languages and LINQ frameworks. We introduce DynQ, a novel query engine designed for dynamic languages. DynQ is language-agnostic, since it is able to execute SQL queries in a polyglot language runtime. Moreover, DynQ can execute queries combining data from multiple sources, namely in-memory object collections as well as on-file data and external database systems. Our evaluation of DynQ shows performance comparable with equivalent hand-optimized code, and in line with common data-processing libraries and embedded databases, making DynQ an appealing query engine for standalone analytics applications and for data-intensive server-side workloads.",
        "link": "https://dl.acm.org/doi/10.14778/3457390.3457405",
        "category": "Databases"
    },
    {
        "title": "Leveraging NVMe SSDs for Building a Fast, Cost-effective, LSM-tree-based KV Store",
        "authors": "['Cheng Li', 'Hao Chen', 'Chaoyi Ruan', 'Xiaosong Ma', 'Yinlong Xu']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "Key-value (KV) stores support many crucial applications and services. They perform fast in-memory processing but are still often limited by I/O performance. The recent emergence of high-speed commodity non-volatile memory express solid-state drives (NVMe SSDs) has propelled new KV system designs that take advantage of their ultra-low latency and high bandwidth. Meanwhile, to switch to entirely new data layouts and scale up entire databases to high-end SSDs requires considerable investment. As a compromise, we propose SpanDB, an LSM-tree-based KV store that adapts the popular RocksDB system to utilize selective deployment of high-speed SSDs. SpanDB allows users to host the bulk of their data on cheaper and larger SSDs (and even hard disc drives with certain workloads), while relocating write-ahead logs (WAL) and the top levels of the LSM-tree to a much smaller and faster NVMe SSD. To better utilize this fast disk, SpanDB provides high-speed, parallel WAL writes via SPDK, and enables asynchronous request processing to mitigate inter-thread synchronization overhead and work efficiently with polling-based I/O. To ease the live data migration between fast and slow disks, we introduce TopFS, a stripped-down file system providing familiar file interface wrappers on top of SPDK I/O. Our evaluation shows that SpanDB simultaneously improves RocksDB's throughput by up to 8.8\\(\\times\\) and reduces its latency by 9.5–58.3%. Compared with KVell, a system designed for high-end SSDs, SpanDB achieves 96–140% of its throughput, with a 2.3–21.6\\(\\times\\) lower latency, at a cheaper storage configuration.",
        "link": "https://dl.acm.org/doi/10.1145/3480963",
        "category": "Databases"
    },
    {
        "title": "MonkeyDB: effectively testing correctness under weak isolation levels",
        "authors": "['Ranadeep Biswas', 'Diptanshu Kakwani', 'Jyothi Vedurada', 'Constantin Enea', 'Akash Lal']",
        "date": "October 2021",
        "source": "Proceedings of the ACM on Programming Languages",
        "abstract": "Modern applications, such as social networking systems and e-commerce platforms are centered around using large-scale storage systems for storing and retrieving data. In the presence of concurrent accesses, these storage systems trade off isolation for performance. The weaker the isolation level, the more behaviors a storage system is allowed to exhibit and it is up to the developer to ensure that their application can tolerate those behaviors. However, these weak behaviors only occur rarely in practice and outside the control of the application, making it difficult for developers to test the robustness of their code against weak isolation levels.  This paper presents MonkeyDB, a mock storage system for testing storage-backed applications. MonkeyDB supports a key-value interface as well as SQL queries under multiple isolation levels. It uses a logical specification of the isolation level to compute, on a read operation, the set of all possible return values. MonkeyDB then returns a value randomly from this set. We show that MonkeyDB provides good coverage of weak behaviors, which is complete in the limit. We test a variety of applications for assertions that fail only under weak isolation. MonkeyDB is able to break each of those assertions in a small number of attempts.",
        "link": "https://dl.acm.org/doi/10.1145/3485546",
        "category": "Databases"
    },
    {
        "title": "R2GSync and edge views: practical RDBMS to GDBMS synchronization",
        "authors": "['Nafisa Anzum', 'Semih Salihoglu']",
        "date": "June 2021",
        "source": "GRADES-NDA '21: Proceedings of the 4th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)",
        "abstract": "Graph databases that are used in enterprises are primarily extracted from a main transactional store that is often an RDBMS. This data infrastructure set up raises the challenge of keeping the extracted graph in a graph database management system (GDBMS) in sync with the source RDBMS. When the extracted graphs contain edge types that are results of join queries, this synchronization requires incrementally maintaining these join queries. In this paper, we investigate an alternative design where we can map the individual relations in these joins to virtual nodes and edges to keep the synchronization very efficient and instead support view-based querying in the GDBMS. We present a system called R2GSync, that synchronizes an RDBMS with a GDBMS and our accompanying edge view design for a GDBMS. We describe our implementation of edge views in GraphflowDB and query optimization techniques for improving the performance of queries that involve edge views.",
        "link": "https://dl.acm.org/doi/10.1145/3461837.3464515",
        "category": "Databases"
    },
    {
        "title": "Design and Development of Heuristic Utility Management Algorithm for Chinese Library Management System",
        "authors": "['Xiaodong Yang', 'Xiaoxia Lin']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "Utility Management in a library is the programmatic tool with the synthetic mental program ability, along with Artificial Intelligence capacities, headed to manage a high volume of books, articles, and assignments, which help to ease the manual significance of librarians. This computerized machine code helps librarians to deal with various databases of the library management system. This framework keeps the records of all the resource details in an optimized manner. It uses a utility management software code with an optimized search classifier that helps to deal with the resource of the library. In this work, the Heuristic Utility Management Algorithm (HUMA) has been used to keep track of resources in the library using mathematical modeling and standardized programmatic computation on tags, which relates the decode scanner to parse the input information. HUMA helps to reduce the manual routine work done by the librarians, and it has been analyzed in this research with prominent survey outcomes based on experimental validation.",
        "link": "https://dl.acm.org/doi/10.1145/3397968",
        "category": "Databases"
    },
    {
        "title": "Scalable Multi-Query Execution using Reinforcement Learning",
        "authors": "['Panagiotis Sioulas', 'Anastasia Ailamaki']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The growing demand for data-intensive decision support and the migration to multi-tenant infrastructures put databases under the stress of high analytical query load. The requirement for high throughput contradicts the traditional design of query-at-a-time databases that optimize queries for efficient serial execution. Sharing work across queries presents an opportunity to reduce the total cost of processing and therefore improve throughput with increasing query load. Systems can share work either by assessing all opportunities and restructuring batches of queries ahead of execution, or by inspecting opportunities in individual incoming queries at runtime: the former strategy scales poorly to large query counts, as it requires expensive sharing-aware optimization, whereas the latter detects only a subset of the opportunities. Both strategies fail to minimize the cost of processing for large and ad-hoc workloads. This paper presents RouLette, a specialized intelligent engine for multi-query execution that addresses, through runtime adaptation, the shortcomings of existing work-sharing strategies. RouLette scales by replacing sharing-aware optimization with adaptive query processing, and it chooses opportunities to explore and exploit by using reinforcement learning. RouLette also includes optimizations that reduce the adaptation overhead. RouLette increases throughput by 1.6-28.3x, compared to a state-of-the-art query-at-a-time engine, and up to 6.5x, compared to sharing-enabled prototypes, for multi-query workloads based on the schema of TPC-DS.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452799",
        "category": "Databases"
    },
    {
        "title": "Hu-Fu: efficient and secure spatial queries over data federation",
        "authors": "['Yongxin Tong', 'Xuchen Pan', 'Yuxiang Zeng', 'Yexuan Shi', 'Chunbo Xue', 'Zimu Zhou', 'Xiaofei Zhang', 'Lei Chen', 'Yi Xu', 'Ke Xu', 'Weifeng Lv']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Data isolation has become an obstacle to scale up query processing over big data, since sharing raw data among data owners is often prohibitive due to security concerns. A promising solution is to perform secure queries over a federation of multiple data owners leveraging secure multi-party computation (SMC) techniques, as evidenced by recent federation work over relational data. However, existing solutions are highly inefficient on spatial queries due to excessive secure distance operations for query processing and their usage of general-purpose SMC libraries for secure operation implementation. In this paper, we propose Hu-Fu, the first system for efficient and secure spatial query processing on a data federation. The idea is to decompose the secure processing of a spatial query into as many plaintext operations and as few secure operations as possible, where fewer secure operators are involved and all secure operators are implemented dedicatedly. As a working system, Hu-Fu supports not only query input in native SQL, but also heterogeneous spatial databases (e.g., PostGIS, Simba, GeoMesa, and SpatialHadoop) at the backend. Extensive experiments show that Hu-Fu usually outperforms the state-of-the-arts in running time and communication cost while guaranteeing security.",
        "link": "https://dl.acm.org/doi/10.14778/3514061.3514064",
        "category": "Databases"
    },
    {
        "title": "\"What makes my queries slow?\": subgroup discovery for SQL workload analysis",
        "authors": "['Youcef Remil', 'Anes Bendimerad', 'Romain Mathonat', 'Philippe Chaleat', 'Mehdi Kaytoue']",
        "date": "November 2021",
        "source": "ASE '21: Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering",
        "abstract": "Among daily tasks of database administrators (DBAs), the analysis of query workloads to identify schema issues and improving performances is crucial. Although DBAs can easily pinpoint queries repeatedly causing performance issues, it remains challenging to automatically identify subsets of queries that share some properties only (a pattern) and simultaneously foster some target measures, such as execution time. Patterns are defined on combinations of query clauses, environment variables, database alerts and metrics and help answer questions like what makes SQL queries slow? What makes I/O communications high? Automatically discovering these patterns in a huge search space and providing them as hypotheses for helping to localize issues and root-causes is important in the context of explainable AI. To tackle it, we introduce an original approach rooted on Subgroup Discovery. We show how to instantiate and develop this generic data-mining framework to identify potential causes of SQL workloads issues. We believe that such data-mining technique is not trivial to apply for DBAs. As such, we also provide a visualization tool for interactive knowledge discovery. We analyse a one week workload from hundreds of databases from our company, make both the dataset and source code available, and experimentally show that insightful hypotheses can be discovered.",
        "link": "https://dl.acm.org/doi/10.1109/ASE51524.2021.9678915",
        "category": "Databases"
    },
    {
        "title": "FlexPushdownDB: hybrid pushdown and caching in a cloud DBMS",
        "authors": "['Yifei Yang', 'Matt Youill', 'Matthew Woicik', 'Yizhou Liu', 'Xiangyao Yu', 'Marco Serafini', 'Ashraf Aboulnaga', 'Michael Stonebraker']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Modern cloud databases adopt a storage-disaggregation architecture that separates the management of computation and storage. A major bottleneck in such an architecture is the network connecting the computation and storage layers. Two solutions have been explored to mitigate the bottleneck: caching and computation pushdown. While both techniques can significantly reduce network traffic, existing DBMSs consider them as orthogonal techniques and support only one or the other, leaving potential performance benefits unexploited.In this paper we present FlexPushdownDB (FPDB), an OLAP cloud DBMS prototype that supports fine-grained hybrid query execution to combine the benefits of caching and computation pushdown in a storage-disaggregation architecture. We build a hybrid query executor based on a new concept called separable operators to combine the data from the cache and results from the pushdown processing. We also propose a novel Weighted-LFU cache replacement policy that takes into account the cost of pushdown computation. Our experimental evaluation on the Star Schema Benchmark shows that the hybrid execution outperforms both the conventional caching-only architecture and pushdown-only architecture by 2.2X. In the hybrid architecture, our experiments show that Weighted-LFU can outperform the baseline LFU by 37%.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476265",
        "category": "Databases"
    },
    {
        "title": "Compliant geo-distributed data processing in action",
        "authors": "['Kaustubh Beedkar', 'David Brekardin', 'Jorge-Anulfo Quiané-Ruiz', 'Volker Markl']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In this paper we present our work on compliant geo-distributed data processing. Our work focuses on the new dimension of dataflow constraints that regulate the movement of data across geographical or institutional borders. For example, European directives may regulate transferring only certain information fields (such as non personal information) or aggregated data. Thus, it is crucial for distributed data processing frameworks to consider compliance with respect to dataflow constraints derived from these regulations. We have developed a compliance-based data processing framework, which (i) allows for the declarative specification of dataflow constraints, (ii) determines if a query can be translated into a compliant distributed query execution plan, and (iii) executes the compliant plan over distributed SQL databases. We demonstrate our framework using a geo-distributed adaptation of the TPC-H benchmark data. Our framework provides an interactive dashboard, which allows users to specify dataflow constraints, and analyze and execute compliant distributed query execution plans.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476359",
        "category": "Databases"
    },
    {
        "title": "Unsupervised time series outlier detection with diversity-driven convolutional ensembles",
        "authors": "['David Campos', 'Tung Kieu', 'Chenjuan Guo', 'Feiteng Huang', 'Kai Zheng', 'Bin Yang', 'Christian S. Jensen']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "With the sweeping digitalization of societal, medical, industrial, and scientific processes, sensing technologies are being deployed that produce increasing volumes of time series data, thus fueling a plethora of new or improved applications. In this setting, outlier detection is frequently important, and while solutions based on neural networks exist, they leave room for improvement in terms of both accuracy and efficiency. With the objective of achieving such improvements, we propose a diversity-driven, convolutional ensemble. To improve accuracy, the ensemble employs multiple basic outlier detection models built on convolutional sequence-to-sequence autoencoders that can capture temporal dependencies in time series. Further, a novel diversity-driven training method maintains diversity among the basic models, with the aim of improving the ensemble's accuracy. To improve efficiency, the approach enables a high degree of parallelism during training. In addition, it is able to transfer some model parameters from one basic model to another, which reduces training time. We report on extensive experiments using real-world multivariate time series that offer insight into the design choices underlying the new approach and offer evidence that it is capable of improved accuracy and efficiency.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494142",
        "category": "Databases"
    },
    {
        "title": "Ranked enumeration of join queries with projections",
        "authors": "['Shaleen Deep', 'Xiao Hu', 'Paraschos Koutris']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Join query evaluation with ordering is a fundamental data processing task in relational database management systems. SQL and custom graph query languages such as Cypher offer this functionality by allowing users to specify the order via the ORDER BY clause. In many scenarios, the users also want to see the first k results quickly (expressed by the LIMIT clause), but the value of k is not predetermined as user queries are arriving in an online fashion. Recent work has made considerable progress in identifying optimal algorithms for ranked enumeration of join queries that do not contain any projections. In this paper, we initiate the study of the problem of enumerating results in ranked order for queries with projections. Our main result shows that for any acyclic query, it is possible to obtain a near-linear (in the size of the database) delay algorithm after only a linear time preprocessing step for two important ranking functions: sum and lexicographic ordering. For a practical subset of acyclic queries known as star queries, we show an even stronger result that allows a user to obtain a smooth tradeoff between faster answering time guarantees using more preprocessing time. Our results are also extensible to queries containing cycles and unions. We also perform a comprehensive experimental evaluation to demonstrate that our algorithms, which are simple to implement, improve up to three orders of magnitude in the running time over state-of-the-art algorithms implemented within open-source RDBMS and specialized graph databases.",
        "link": "https://dl.acm.org/doi/10.14778/3510397.3510401",
        "category": "Databases"
    },
    {
        "title": "BullFrog: Online Schema Evolution via Lazy Evaluation",
        "authors": "['Souvik Bhattacherjee', 'Gang Liao', 'Michael Hicks', 'Daniel J. Abadi']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "BullFrog is a relational DBMS that supports single-step schema migrations --- even those that are backwards incompatible --- without downtime, and without need for advanced warning. When a schema migration is submitted, BullFrog initiates a logical switch to the new schema, but physically migrates affected data lazily, as it is accessed by incoming transactions. BullFrog's internal concurrency control algorithms and data structures enable concurrent processing of schema migration operations with post-migration transactions, while ensuring exactly-once migration of all old data into the physical layout required by the new schema. BullFrog is implemented as an open source extension to PostgreSQL. Experiments using this prototype over a TPC-C based workload (supplemented to include schema migrations) show that BullFrog can achieve zero-downtime migration to non-trivial new schemas with near-invisible impact on transaction throughput and latency.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452842",
        "category": "Databases"
    },
    {
        "title": "The Model Counting Competition 2020",
        "authors": "['Johannes K. Fichte', 'Markus Hecher', 'Florim Hamiti']",
        "date": "None",
        "source": "ACM Journal of Experimental Algorithmics",
        "abstract": "Many computational problems in modern society account to probabilistic reasoning, statistics, and combinatorics. A variety of these real-world questions can be solved by representing the question in (Boolean) formulas and associating the number of models of the formula directly with the answer to the question. Since there has been an increasing interest in practical problem solving for model counting over the past years, the Model Counting Competition was conceived in fall 2019. The competition aims to foster applications, identify new challenging benchmarks, and promote new solvers and improve established solvers for the model counting problem and versions thereof. We hope that the results can be a good indicator of the current feasibility of model counting and spark many new applications. In this article, we report on details of the Model Counting Competition 2020, about carrying out the competition, and the results. The competition encompassed three versions of the model counting problem, which we evaluated in separate tracks. The first track featured the model counting problem, which asks for the number of models of a given Boolean formula. On the second track, we challenged developers to submit programs that solve the weighted model counting problem. The last track was dedicated to projected model counting. In total, we received a surprising number of nine solvers in 34 versions from eight groups.",
        "link": "https://dl.acm.org/doi/10.1145/3459080",
        "category": "Databases"
    },
    {
        "title": "Session-Recommendation Based on Gated Neural Network to Extract Item Structure Feature Graph",
        "authors": "['Wei Zhang', 'Yan Yang', 'Yingli Zhong']",
        "date": "February 2022",
        "source": "DSDE '22: 2022 the 5th International Conference on Data Storage and Data Engineering",
        "abstract": "The issue of inaccurate user portrait recommendations for short-term conversations is solved. The classical recommendation is based on an assumption: the user's historical behavior can represent the user's stable long-term preferences, and the result is generated based on the user's historical behavior. However, there is sparseness in the user's historical behavior information record in the actual data set, and the obtained data cannot accurately represent the user's preferences. Research on a session recommendation focuses on user behavior data in the short term, and aims at predict anonymous user behavior based on in-session behavior information. It plays a vital role in application scenarios where user behavior information is scarce. Feature extraction is performed on the user's anonymous behavior information graph generated in a fixed period of time as the embedding representation of user's interest preference; the graph structure of the same item in the global click interaction graph is combined with the input a gated graph neural network as the item attribute embedding representation. The prediction part adopts simulated interactive calculation to produce results. With the help of gated graph neural network, features extraction and analysis of data in short-term sessions: session-based user behavior data and in-session item attributes are obtained from the graph data structure, which reduces errors caused by inconsistent data processing methods. The user's current interest is captured based on the behavior information in the session, the global inter-item conversion graph is generated based on the sequence of click moments, and the feature vector is obtained through graph neural network processing. The generation of recommendation results is more time-sensitive, complete the anonymity problem due to the scarcity of user's information, and improves the accuracy of recommendations.",
        "link": "https://dl.acm.org/doi/10.1145/3528114.3528115",
        "category": "Databases"
    },
    {
        "title": "Monitoring Method for Archival Data Change of Electrical Information Acquisition System Based On Data Middle Office and Its Application",
        "authors": "['Weiqing Yao', 'Qiang Rao', 'Jiajun Yuan', 'Deyi Liu', 'Jie Xu']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "In order to improve the \"last kilometer\" integrity rate of operation data of distribution transformers for the PIS system, a method is designed to monitor the abnormal changes of the archival data of the measuring points for the Electrical Information Acquisition system based on the Data Middle Office. Through flexible use of Dataworks, the change data is divided into the Addition/Deletion and Change data to monitor individually. The change record of source systems can be distinguished, so as to achieve accurate positioning of abnormal problems. The practical application proved that this new method can effectively monitor the change problem. After timely handling according to the monitoring feedback results, the integrity rate of operation data accessed in the PIS system increases to about 96.12% on average every month. Moreover, the rate of good load index of distribution transformers improves to about 93.99% on average every month. This result of practical application meets the standard of State Grid, and lays a foundation for the deepening application of the PIS system.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495287",
        "category": "Databases"
    },
    {
        "title": "Fast Key-Value Lookups with Node Tracker",
        "authors": "['Mustafa Cavus', 'Mohammed Shatnawi', 'Resit Sendag', 'Augustus K. Uht']",
        "date": "None",
        "source": "ACM Transactions on Architecture and Code Optimization",
        "abstract": "Lookup operations for in-memory databases are heavily memory bound, because they often rely on pointer-chasing linked data structure traversals. They also have many branches that are hard-to-predict due to random key lookups. In this study, we show that although cache misses are the primary bottleneck for these applications, without a method for eliminating the branch mispredictions only a small fraction of the performance benefit is achieved through prefetching alone. We propose the Node Tracker (NT), a novel programmable prefetcher/pre-execution unit that is highly effective in exploiting inter key-lookup parallelism to improve single-thread performance. We extend NT with branch outcome streaming (BOS) to reduce branch mispredictions and show that this achieves an extra 3× speedup. Finally, we evaluate the NT as a pre-execution unit and demonstrate that we can further improve the performance in both single- and multi-threaded execution modes. Our results show that, on average, NT improves single-thread performance by 4.1× when used as a prefetcher; 11.9× as a prefetcher with BOS; 14.9× as a pre-execution unit and 18.8× as a pre-execution unit with BOS. Finally, with 24 cores of the latter version, we achieve a speedup of 203× and 11× over the single-core and 24-core baselines, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3452099",
        "category": "Databases"
    },
    {
        "title": "CADRE: A Cloud-Based Data Service for Big Bibliographic Data",
        "authors": "['Xiaoran Yan', 'Guangchen Ruan', 'Dimitar Nikolov', 'Matthew Hutchinson', 'Chathuri Peli Kankanamalage', 'Ben Serrette', 'James McCombs', 'Alan Walsh', 'Esen Tuna', 'Valentin Pentchev']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Large bibliographic data sets hold the promise of revolutionizing the scientific enterprise when combined with state-of-the-science computational capabilities. Providing high-quality data services for large network datasets such as the Microsoft Academic Graph, which contains more than two billion citation links, poses significant difficulties for universities. Data systems based on the property graph model are capable of delivering efficient graph query services for large networks. However, real-life queries often combine multiple types of data models. To satisfy the needs of different user groups, we developed and deployed a cloud-based data system consisting of scalable graph and text-indexed query engines. For non-expert users, the property graph model also presents a technological barrier. To alleviate the steep learning curve, we designed an intuitive graphical user interface for query-building. For advanced users, a scalable notebook service in our platform provides a more flexible computing environments where the query results can be further analyzed. These systems form the data-backbone of the Collaborative Archive and Data Research Environment (CADRE), which provides efficient and high-quality bibliographic data services to eleven large public universities in North America.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3481898",
        "category": "Databases"
    },
    {
        "title": "Chiller: Contention-centric Transaction Execution and Data Partitioning for Modern Networks",
        "authors": "['Erfan Zamanian', 'Julian Shun', 'Carsten Binnig', 'Tim Kraska']",
        "date": "March 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Distributed transactions on high-overhead TCP/IP-based networks were conventionally considered to be prohibitively expensive. In fact, the primary goal of existing partitioning schemes is to minimize the number of cross-partition transactions. However, with the new generation of fast RDMAenabled networks, this assumption is no longer valid.In this paper, we first make the case that the new bottleneck which hinders truly scalable transaction processing in modern RDMA-enabled databases is data contention, and that optimizing for data contention leads to different partitioning layouts than optimizing for the number of distributed transactions. We then present Chiller, a new approach to data partitioning and transaction execution, which aims to minimize data contention for both local and distributed transactions.",
        "link": "https://dl.acm.org/doi/10.1145/3471485.3471490",
        "category": "Databases"
    },
    {
        "title": "Cloud-Based Smart Energy Framework for Accelerated Data Analytics with Parallel Computing of Orchestrated Containers: Study Case of CU-BEMS",
        "authors": "['Kittipat Saengkaenpetch', 'Chaodit Aswakul']",
        "date": "November 2021",
        "source": "AISS '21: Proceedings of the 3rd International Conference on Advanced Information Science and System",
        "abstract": "This paper proposes a practical smart energy framework for data analytic on energy management system at Chulalongkorn University, called CU-BEMS. This serves as an example of demand-sided smart energy application that copes with the challenges of big data analytic and real-time processing needs. The framework is based on the divide and conquer paradigm to accelerate data analytics with parallel computing. The workload is containerized and deployed on the Kubernetes cloud facility of our internationally collaborated IoTcloudServe@TEIN playground. With this playground, the workload scalability and portability can be achieved. Applying the proposed framework, this paper reports on a practical data log analysis to determine the wasted energy consumption. Based on the experimental result, the wasted energy consumption of the whole data set of CU-BEMS's communication research laboratory area from March 2014 to August 2017 can be computed within 81 seconds by using 32 cores running in parallel. The framework is expected to serve as a basis template for further research ongoing at CU-BEMS and smart energy applications that can be computationally enhanced by data analytic pipelining with containerized services as orchestrated by Kubernetes.",
        "link": "https://dl.acm.org/doi/10.1145/3503047.3503088",
        "category": "Databases"
    },
    {
        "title": "Effective Community Detection Algorithm Based on Edge Influence Weight",
        "authors": "['Chang Wang', 'Yan Yang']",
        "date": "March 2022",
        "source": "ICIAI '22: Proceedings of the 2022 6th International Conference on Innovation in Artificial Intelligence",
        "abstract": "Connections strength between nodes are fundamental and important components in social networks, and connection strength determines the community structure of the network to a large extent. Edge weight is a meaningful representative of connection strength or data credibility, which can be applied to social network analysis. Aiming at the problems of insufficient research on the relationship between nodes and unreasonable initial selection of community centers, a community detection algorithm based on edge influence weight (CDP-EW) was proposed in this research. Specifically, to solve the initial community center selection problem, the degree centrality of nodes was used to calculate node influence. Then, the edge influence weight was redefined to calculate similarity based on the link relationships between nodes. Moreover, CDP-EW was compared with some community detection algorithms on real complex network datasets in experiments, where the proposed algorithm performed well on complex networks.",
        "link": "https://dl.acm.org/doi/10.1145/3529466.3529495",
        "category": "Databases"
    },
    {
        "title": "PeriodicMove: Shift-aware Human Mobility Recovery with Graph Neural Network",
        "authors": "['Hao Sun', 'Changjie Yang', 'Liwei Deng', 'Fan Zhou', 'Feiteng Huang', 'Kai Zheng']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Human mobility recovery is of great importance for a wide range of location-based services. However, recovering human mobility is not trivial because of three challenges: 1) complex transition patterns among locations; 2) multi-level periodicity and shifting periodicity of human mobility; 3) sparsity of the collected trajectory data. In this paper, we propose PeriodicMove, a neural attention model based on graph neural network for human mobility recovery from lengthy and sparse trajectories. In PeriodicMove, we first construct a directed graph for each trajectory and capture complex location transition patterns using graph neural network. Then, we design two attention mechanisms which capture multi-level periodicity and shifting periodicity of human mobility respectively. Finally, a spatial-aware loss function is proposed to incorporate spatial proximity into the model optimization, which alleviates the data sparsity problem. We perform extensive experiments and the evaluation results demonstrate that PeriodicMove yields significant improvements over the competitors on two representative real-life mobility datasets. In addition, by providing high-quality mobility data, our model can benefit a variety of mobility-oriented downstream applications.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3482284",
        "category": "Databases"
    },
    {
        "title": "Aurochs: an architecture for dataflow threads",
        "authors": "['Matthew Vilim', 'Alexander Rucker', 'Kunle Olukotun']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "Data analytics pipelines increasingly rely on databases to select, filter, and pre-process reams of data. These databases use data structures with irregular control flow like trees and hash tables which map poorly to existing database accelerators, leaving architects with a choice between CPUS-with stagnant performance---or accelerators that handle this complexity by relying on simpler but asymptotically sub-optimal algorithms. To bridge this gap, we propose Aurochs: a reconfigurable dataflow accelerator (RDA) that matches a CPU asymptotically but outperforms it by over 100 × on constant factors. We introduce a threading model for vector dataflow accelerators that extracts massive parallelism from irregular data structures using lightweight thread contexts. To implement this model, we add only a sparse scratchpad to an existing database accelerator---increasing area by 5%. We reformulate common data structures using dataflow threads and evaluate Aurochs on ridesharing queries---outperforming a GPU by 8x.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00039",
        "category": "Databases"
    },
    {
        "title": "DICE: data discovery by example",
        "authors": "['El Kindi Rezig', 'Anshul Bhandari', 'Anna Fariha', 'Benjamin Price', 'Allan Vanterpool', 'Vijay Gadepally', 'Michael Stonebraker']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In order to conduct analytical tasks, data scientists often need to find relevant data from an avalanche of sources (e.g., data lakes, large organizational databases). This effort is typically made in an ad hoc, non-systematic manner, which makes it a daunting endeavour. Current data discovery systems typically require the users to find relevant tables manually, usually by issuing multiple queries (e.g., using SQL). However, expressing such queries is nontrivial, as it requires knowledge of the underlying structure (schema) of the data organization in advance. This issue is further exacerbated when data resides in data lakes, where there is no predefined schema that data must conform to. On the other hand, data scientists can often come up with a few example records of interest quickly. Motivated by this observation, we developed DICE---a human-in-the-loop system for <u>D</u>ata d<u>I</u>s<u>C</u>overy by <u>E</u>xample---that takes user-provided example records as input and returns more records that satisfy the user intent. DICE's key idea is to synthesize a SQL query that captures the user intent, specified via examples. To this end, DICE follows a three-step process: (1) DICE first discovers a few candidate queries by finding join paths across tables within the data lake. (2) Then DICE consults with the user for validation by presenting a few records to them, and, thus, eliminating spurious queries. (3) Based on the user feedback, DICE refines the search and repeats the process until the user is satisfied with the results. We will demonstrate how DICE can help in data discovery through an interactive, example-based interaction.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476353",
        "category": "Databases"
    },
    {
        "title": "SQL Scrolls - A Reusable and Extensible DGBL Experiment",
        "authors": "['Ela Pustulka', 'Kai Krause', 'Lucia de Espona', 'Andrea Kennel']",
        "date": "November 2021",
        "source": "CSERC '21: Proceedings of the 10th Computer Science Education Research Conference",
        "abstract": "The teaching of databases and SQL is an active research area. We contribute by presenting a reusable and extensible SQL teaching experiment which uses a game and fits the paradigm of digital game based learning (DGBL). Although DGBL is hampered partly by the difficulty of obtaining statistically significant empirical results, the research shows that it may be an effective learning method and that it is in demand. We investigate the acceptance and effectiveness of an SQL learning game and focus on two areas: student reaction to games as a vehicle for teaching, and educational effectiveness. We designed a game prototype and administered a pre test, post test and an acceptance survey, with seven part-time and sixteen full-time students. A statistical analysis of effect sizes revealed a moderate intervention effect for the game group (d= -0.562) and a small one for the traditional group (d= -0.234). The acceptance survey means were between 4.43 and 4.70 out of 5, which shows that the game is highly acceptable. Our experiment demonstrated positive student attitudes towards DGBL in SQL teaching and showed the game to be as effective as exercises done using a workbench. We further observed interesting differences in teaching using a game and a natural workbench environment and had excellent course feedback. We have released the game as open source in the hope that other researchers will replicate or contradict our findings or simply use it in teaching.",
        "link": "https://dl.acm.org/doi/10.1145/3507923.3507932",
        "category": "Databases"
    },
    {
        "title": "Napa: powering scalable data warehousing with robust query performance at Google",
        "authors": "['Ankur Agiwal', 'Kevin Lai', 'Gokul Nath Babu Manoharan', 'Indrajit Roy', 'Jagan Sankaranarayanan', 'Hao Zhang', 'Tao Zou', 'Min Chen', 'Zongchang (Jim) Chen', 'Ming Dai', 'Thanh Do', 'Haoyu Gao', 'Haoyan Geng', 'Raman Grover', 'Bo Huang', 'Yanlai Huang', 'Zhi (Adam) Li', 'Jianyi Liang', 'Tao Lin', 'Li Liu', 'Yao Liu', 'Xi Mao', 'Yalan (Maya) Meng', 'Prashant Mishra', 'Jay Patel', 'Rajesh S. R.', 'Vijayshankar Raman', 'Sourashis Roy', 'Mayank Singh Shishodia', 'Tianhang Sun', 'Ye (Justin) Tang', 'Junichi Tatemura', 'Sagar Trehan', 'Ramkumar Vadali', 'Prasanna Venkatasubramanian', 'Gensheng Zhang', 'Kefei Zhang', 'Yupu Zhang', 'Zeleng Zhuang', 'Goetz Graefe', 'Divyakant Agrawal', 'Jeff Naughton', 'Sujata Kosalge', 'Hakan Hacıgümüş']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Google services continuously generate vast amounts of application data. This data provides valuable insights to business users. We need to store and serve these planet-scale data sets under the extremely demanding requirements of scalability, sub-second query response times, availability, and strong consistency; all this while ingesting a massive stream of updates from applications used around the globe. We have developed and deployed in production an analytical data management system, Napa, to meet these requirements. Napa is the backend for numerous clients in Google. These clients have a strong expectation of variance-free, robust query performance. At its core, Napa's principal technologies for robust query performance include the aggressive use of materialized views, which are maintained consistently as new data is ingested across multiple data centers. Our clients also demand flexibility in being able to adjust their query performance, data freshness, and costs to suit their unique needs. Robust query processing and flexible configuration of client databases are the hallmark of Napa design.Most of the related work in this area takes advantage of full flexibility to design the whole system without the need to support a diverse set of preexisting use cases. In comparison, a particular challenge we faced is that Napa needs to deal with hard constraints from existing applications and infrastructure, so we could not do a \"green field\" system, but rather had to satisfy existing constraints. These constraints led us to make particular design decisions and also devise new techniques to meet the challenges. In this paper, we share our experiences in designing, implementing, deploying, and running Napa in production with some of Google's most demanding applications.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476377",
        "category": "Databases"
    },
    {
        "title": "Quantum Machine Learning Algorithm for Knowledge Graphs",
        "authors": "['Yunpu Ma', 'Volker Tresp']",
        "date": "None",
        "source": "ACM Transactions on Quantum Computing",
        "abstract": "Semantic knowledge graphs are large-scale triple-oriented databases for knowledge representation and reasoning. Implicit knowledge can be inferred by modeling the tensor representations generated from knowledge graphs. However, as the sizes of knowledge graphs continue to grow, classical modeling becomes increasingly computationally resource intensive. This article investigates how to capitalize on quantum resources to accelerate the modeling of knowledge graphs. In particular, we propose the first quantum machine learning algorithm for inference on tensorized data, i.e., on knowledge graphs. Since most tensor problems are NP-hard [18], it is challenging to devise quantum algorithms to support the inference task. We simplify the modeling task by making the plausible assumption that the tensor representation of a knowledge graph can be approximated by its low-rank tensor singular value decomposition, which is verified by our experiments. The proposed sampling-based quantum algorithm achieves speedup with a polylogarithmic runtime in the dimension of knowledge graph tensor.",
        "link": "https://dl.acm.org/doi/10.1145/3467982",
        "category": "Databases"
    },
    {
        "title": "View selection over knowledge graphs in triple stores",
        "authors": "['Theofilos Mailis', 'Yannis Kotidis', 'Stamatis Christoforidis', 'Evgeny Kharlamov', 'Yannis Ioannidis']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Knowledge Graphs (KGs) are collections of interconnected and annotated entities that have become powerful assets for data integration, search enhancement, and other industrial applications. Knowledge Graphs such as DBPEDIA may contain billion of triple relations and are intensively queried with millions of queries per day. A prominent approach to enhance query answering on Knowledge Graph databases is View Materialization, ie., the materialization of an appropriate set of computations that will improve query performance.We study the problem of view materialization and propose a view selection methodology for processing query workloads with more than a million queries. Our approach heavily relies on subgraph pattern mining techniques that allow to create efficient summarizations of massive query workloads while also identifying the candidate views for materialization. In the core of our work is the correspondence between the view selection problem to that of Maximizing a Nondecreasing Submodular Set Function Subject to a Knapsack Constraint. The latter leads to a tractable view-selection process for native triple stores that allows a (1 - e---1)-approximation of the optimal selection of views. Our experimental evaluation shows that all the steps of the view-selection process are completed in a few minutes, while the corresponding rewritings accelerate 67.68% of the queries in the DBPEDIA query workload. Those queries are executed in 2.19% of their initial time on average.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484227",
        "category": "Databases"
    },
    {
        "title": "Compliant Geo-distributed Query Processing",
        "authors": "['Kaustubh Beedkar', 'Jorge-Arnulfo Quiané-Ruiz', 'Volker Markl']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In this paper, we address the problem of compliant geo-distributed query processing. In particular, we focus on dataflow policies that impose restrictions on movement of data across geographical or institutional borders. Traditional ways to distributed query processing do not consider such restrictions and therefore in geo-distributed environments may lead to non-compliant query execution plans. For example, an execution plan for a query over data sources from Europe, North America, and Asia, which may otherwise be optimal, may not comply with dataflow policies as a result of shipping some restricted (intermediate) data. We pose this problem of compliance in the setting of geo-distributed query processing. We propose a compliance-based query optimizer that takes into account dataflow policies, which are declaratively specified using our policy expressions, to generate compliant geo-distributed execution plans. Our experimental study using a geo-distributed adaptation of the TPC-H benchmark data indicates that our optimization techniques are effective in generating efficient compliant plans and incur low overhead on top of traditional query optimizers.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3453687",
        "category": "Databases"
    },
    {
        "title": "ByShard: sharding in a byzantine environment",
        "authors": "['Jelle Hellings', 'Mohammad Sadoghi']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The emergence of blockchains has fueled the development of resilient systems that can deal with Byzantine failures due to crashes, bugs, or even malicious behavior. Recently, we have also seen the exploration of sharding in these resilient systems, this to provide the scalability required by very large data-based applications. Unfortunately, current sharded resilient systems all use system-specific specialized approaches toward sharding that do not provide the flexibility of traditional sharded data management systems.To improve on this situation, we fundamentally look at the design of sharded resilient systems. We do so by introducing BYSHARD, a unifying framework for the study of sharded resilient systems. Within this framework, we show how two-phase commit and two-phase locking---two techniques central to providing atomicity and isolation in traditional sharded databases---can be implemented efficiently in a Byzantine environment, this with a minimal usage of costly Byzantine resilient primitives. Based on these techniques, we propose eighteen multi-shard transaction processing protocols. Finally, we practically evaluate these protocols and show that each protocol supports high transaction throughput and provides scalability while each striking its own trade-off between throughput, isolation level, latency, and abort rate. As such, our work provides a strong foundation for the development of ACID-compliant general-purpose and flexible sharded resilient data management systems.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476275",
        "category": "Databases"
    },
    {
        "title": "#NFA Admits an FPRAS: Efficient Enumeration, Counting, and Uniform Generation for Logspace Classes",
        "authors": "['Marcelo Arenas', 'Luis Alberto Croquevielle', 'Rajesh Jayaram', 'Cristian Riveros']",
        "date": "None",
        "source": "Journal of the ACM",
        "abstract": "In this work, we study two simple yet general complexity classes, based on logspace Turing machines, that provide a unifying framework for efficient query evaluation in areas such as information extraction and graph databases, among others. We investigate the complexity of three fundamental algorithmic problems for these classes: enumeration, counting, and uniform generation of solutions, and show that they have several desirable properties in this respect.Both complexity classes are defined in terms of non-deterministic logspace transducers (NL-transducers). For the first class, we consider the case of unambiguous NL-transducers, and we prove constant delay enumeration and both counting and uniform generation of solutions in polynomial time. For the second class, we consider unrestricted NL-transducers, and we obtain polynomial delay enumeration, approximate counting in polynomial time, and polynomial-time randomized algorithms for uniform generation. More specifically, we show that each problem in this second class admits a fully polynomial-time randomized approximation scheme (FPRAS) and a polynomial-time Las Vegas algorithm (with preprocessing) for uniform generation. Remarkably, the key idea to prove these results is to show that the fundamental problem # NFA admits an FPRAS, where # NFA is the problem of counting the number of strings of length n (given in unary) accepted by a non-deterministic finite automaton (NFA). While this problem is known to be P-complete and, more precisely, SpanL-complete, it was open whether this problem admits an FPRAS. In this work, we solve this open problem and obtain as a welcome corollary that every function in SpanL admits an FPRAS.",
        "link": "https://dl.acm.org/doi/10.1145/3477045",
        "category": "Databases"
    },
    {
        "title": "Privacy-preserving Dynamic Symmetric Searchable Encryption with Controllable Leakage",
        "authors": "['Shujie Cui', 'Xiangfu Song', 'Muhammad Rizwan Asghar', 'Steven D. Galbraith', 'Giovanni Russello']",
        "date": "None",
        "source": "ACM Transactions on Privacy and Security",
        "abstract": "Searchable Encryption (SE) is a technique that allows Cloud Service Providers to search over encrypted datasets without learning the content of queries and records. In recent years, many SE schemes have been proposed to protect outsourced data. However, most of them leak sensitive information, from which attackers could still infer the content of queries and records by mounting leakage-based inference attacks, such as the count attack and file-injection attack.In this work, first we define the leakage in searchable encrypted databases and analyse how the leakage is leveraged in existing leakage-based attacks. Second, we propose a <underline>P</underline>rivacy-preserving <underline>M</underline>ulti-<underline>c</underline>loud based dynamic symmetric SE scheme for relational <underline>D</underline>ata<underline>b</underline>ase (P-McDb). P-McDb has minimal leakage, which not only ensures confidentiality of queries and records but also protects the search, intersection, and size patterns. Moreover, P-McDb ensures both forward and backward privacy of the database. Thus, P-McDb could resist existing leakage-based attacks, e.g., active file/record-injection attacks. We give security definition and analysis to show how P-McDb hides the aforementioned patterns. Finally, we implemented a prototype of P-McDb and tested it using the TPC-H benchmark dataset. Our evaluation results show that users can get the required records in 2.16 s when searching over 4.1 million records.",
        "link": "https://dl.acm.org/doi/10.1145/3446920",
        "category": "Databases"
    },
    {
        "title": "Flow-loss: learning cardinality estimates that matter",
        "authors": "['Parimarjan Negi', 'Ryan Marcus', 'Andreas Kipf', 'Hongzi Mao', 'Nesime Tatbul', 'Tim Kraska', 'Mohammad Alizadeh']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Recently there has been significant interest in using machine learning to improve the accuracy of cardinality estimation. This work has focused on improving average estimation error, but not all estimates matter equally for downstream tasks like query optimization. Since learned models inevitably make mistakes, the goal should be to improve the estimates that make the biggest difference to an optimizer. We introduce a new loss function, Flow-Loss, for learning cardinality estimation models. Flow-Loss approximates the optimizer's cost model and search algorithm with analytical functions, which it uses to optimize explicitly for better query plans. At the heart of Flow-Loss is a reduction of query optimization to a flow routing problem on a certain \"plan graph\", in which different paths correspond to different query plans. To evaluate our approach, we introduce the Cardinality Estimation Benchmark (CEB) which contains the ground truth cardinalities for sub-plans of over 16K queries from 21 templates with up to 15 joins. We show that across different architectures and databases, a model trained with Flow-Loss improves the plan costs and query runtimes despite having worse estimation accuracy than a model trained with Q-Error. When the test set queries closely match the training queries, models trained with both loss functions perform well. However, the Q-Error-trained model degrades significantly when evaluated on slightly different queries (e.g., similar but unseen query templates), while the Flow-Loss-trained model generalizes better to such situations, achieving 4 -- 8× better 99th percentile runtimes on unseen templates with the same model architecture and training data.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476259",
        "category": "Databases"
    },
    {
        "title": "Multi-Model Data Modeling and Representation: State of the Art and Research Challenges",
        "authors": "['Irena Holubova', 'Pavel Contos', 'Martin Svoboda']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472267",
        "category": "Databases"
    },
    {
        "title": "Heterogeneous Feature Fusion and Cross-modal Alignment for Composed Image Retrieval",
        "authors": "['Gangjian Zhang', 'Shikui Wei', 'Huaxin Pang', 'Yao Zhao']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Composed image retrieval aims at performing image retrieval task by giving a reference image and a complementary text piece. Since composing both image and text information can accurately model the users' search intent, composed image retrieval can perform target-specific image retrieval task and be potentially applied to many scenarios such as interactive product search. However, two key challenging issues must be addressed in composed image retrieval occasion. One of them is how to fuse heterogeneous image and text piece in the query into a complementary feature space. The other is how to bridge the heterogeneous gap between text pieces in the query and images in the database. To address the issues, we propose an end-to-end framework for composed image retrieval, which consists of three key components including Multi-modal Complementary Fusion (MCF), Cross-modal Guided Pooling (CGP), and Relative Caption-aware Consistency (RCC). By incorporating MCF and CGP modules, we can fully integrate the complementary information of image and text piece in the query through multiple deep interactions and aggregate obtained local features into an embedding vector. To bridge the heterogeneous gap, we introduce the RCC constraint to align text pieces in the query and images in the database. Extensive experiments on four public benchmark datasets show that the proposed composed image retrieval framework achieves outstanding performance against the state-of-the-art methods.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475659",
        "category": "Databases"
    },
    {
        "title": "Hierarchical Semantics Matching For Heterogeneous Spatio-temporal Sources",
        "authors": "['Daniel Glake', 'Norbert Ritter', 'Florian Ocker', 'Nima Ahmady-Moghaddam', 'Daniel Osterholz', 'Ulfia Lenfers', 'Thomas Clemen']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Spatio-temporal data are semantically valuable information used for various analytical tasks to identify spatially relevant and temporally limited correlations within a domain. The increasing availability and data acquisition from multiple sources with their typically high heterogeneity are getting more and more attention. However, these sources often lack interconnecting shared keys, making their integration a challenging problem. For example, publicly available parking data that consist of point data on parking facilities with fluctuating occupancy and static location data on parking spaces cannot be directly correlated. Both data sets describe two different aspects from distinct sources in which parking spaces and fluctuating occupancy are part of the same semantic model object. Especially for ad hoc analytical tasks on integrated models, these missing relationships cannot be handled using join operations as usual in relational databases. The reason lies in the lack of equijoin relationships, comparing for equality of strings and additional overhead in loading data up before processing. This paper addresses the optimization problem of finding suitable partners in the absence of equijoin relations for heterogeneous spatio-temporal data, applicable to ad hoc analytics. We propose a graph-based approach that achieves good recall and performance scaling via hierarchically separating the semantics along spatial, temporal, and domain-specific dimensions. We evaluate our approach using public data, showing that it is suitable for many standard join scenarios and highlighting its limitations.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3482350",
        "category": "Databases"
    },
    {
        "title": "Scaling replicated state machines with compartmentalization",
        "authors": "['Michael Whittaker', 'Ailidani Ailijiang', 'Aleksey Charapko', 'Murat Demirbas', 'Neil Giridharan', 'Joseph M. Hellerstein', 'Heidi Howard', 'Ion Stoica', 'Adriana Szekeres']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "State machine replication protocols, like MultiPaxos and Raft, are a critical component of many distributed systems and databases. However, these protocols offer relatively low throughput due to several bottlenecked components. Numerous existing protocols fix different bottlenecks in isolation but fall short of a complete solution. When you fix one bottleneck, another arises. In this paper, we introduce compartmentalization, the first comprehensive technique to eliminate state machine replication bottlenecks. Compartmentalization involves decoupling individual bottlenecks into distinct components and scaling these components independently. Compartmentalization has two key strengths. First, compartmentalization leads to strong performance. In this paper, we demonstrate how to compartmentalize MultiPaxos to increase its throughput by 6× on a write-only workload and 16× on a mixed read-write workload. Unlike other approaches, we achieve this performance without the need for specialized hardware. Second, compartmentalization is a technique, not a protocol. Industry practitioners can apply compartmentalization to their protocols incrementally without having to adopt a completely new protocol.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476273",
        "category": "Databases"
    },
    {
        "title": "Towards a Pragmatic Interoperability on the MIDAS Middleware",
        "authors": "['Elivaldo Lozer Fracalossi Ribeiro', 'Luis Emanuel Neves de Jesus', 'Daniela Barreiro Claro', 'Natan Moura']",
        "date": "November 2021",
        "source": "WebMedia '21: Proceedings of the Brazilian Symposium on Multimedia and the Web",
        "abstract": "Nowadays, many organizations store and publish their data and services based on the Cloud Computing paradigm. In this scenario, cloud consumers access these resources anytime and anywhere. Software as a Service (SaaS) and Data as a Service are examples of cloud services. While DaaS delivers and manages data on-demand, SaaS is a delivery model of applications in a cloud environment. However, the vast amount of social data and applications enable different formats of DaaS, such as non-structured (e.g., text), semi-structured (e.g., JSON), and structured format (e.g., Relational Database). The lack of standardization makes users dependent on a system due to the lack of interoperability among different providers. Interoperability is heterogeneous systems' ability to communicate transparently, and it is classified into syntactic, semantic, and pragmatic levels. Middleware for SaaS and DaaS (MIDAS) is a solution to provide interoperability among cloud services. Although the latest version of MIDAS promotes a semantic approach, pragmatic aspects are not addressed. This paper enhances MIDAS to provide pragmatic interoperability in a cloud environment. Our approach presents the necessary elements that MIDAS must consider to provide pragmatic interoperability among cloud services. We conduct a set of experiments to validate our pragmatic MIDAS. We evaluate the overhead of our approach, the correctness of our novel MIDAS, and the effort to implement the MIDAS middleware with dynamic pragmatic information. Results evidence that our approach is towards pragmatic interoperability among cloud services.",
        "link": "https://dl.acm.org/doi/10.1145/3470482.3479459",
        "category": "Databases"
    },
    {
        "title": "Context-free path querying with all-path semantics by matrix multiplication",
        "authors": "['Rustam Azimov', 'Ilya Epelbaum', 'Semyon Grigorev']",
        "date": "June 2021",
        "source": "GRADES-NDA '21: Proceedings of the 4th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)",
        "abstract": "Context-Free Path Querying (CFPQ) allows one to use context-free grammars as path constraints in navigational graph queries. Many algorithms for CFPQ were proposed but recently showed that the state-of-the-art CFPQ algorithms are still not performant enough for practical use. One promising way to achieve high-performance solutions for graph querying problems is to reduce them to linear algebra operations. Recently, there are two CFPQ solutions formulated in terms of linear algebra: the one based on the Boolean matrix multiplication operation proposed by Azimov et al. (2018) and the Kronecker product-based CFPQ algorithm proposed by Orachev et al. (2020). However, the algorithm based on matrix multiplication still does not support the most expressive all-path query semantics and cannot be truly compared with Kronecker product-based CFPQ algorithm. In this work, we introduce a new matrix-based CFPQ algorithm with all-path query semantics that allows us to extract all found paths for each pair of vertices. Also, we implement our algorithm by using appropriate high-performance libraries for linear algebra. Finally, we provide a comparison of the most performant linear algebra-based CFPQ algorithms for different query semantics.",
        "link": "https://dl.acm.org/doi/10.1145/3461837.3464513",
        "category": "Databases"
    },
    {
        "title": "Basil: Breaking up BFT with ACID (transactions)",
        "authors": "['Florian Suri-Payer', 'Matthew Burke', 'Zheng Wang', 'Yunhao Zhang', 'Lorenzo Alvisi', 'Natacha Crooks']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "This paper presents Basil, the first transactional, leaderless Byzantine Fault Tolerant key-value store. Basil leverages ACID transactions to scalably implement the abstraction of a trusted shared log in the presence of Byzantine actors. Unlike traditional BFT approaches, Basil executes non-conflicting operations in parallel and commits transactions in a single round-trip during fault-free executions. Basil improves throughput over traditional BFT systems by four to five times, and is only four times slower than TAPIR, a non-Byzantine replicated system. Basil's novel recovery mechanism further minimizes the impact of failures: with 30% Byzantine clients, throughput drops by less than 25% in the worst-case.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483552",
        "category": "Databases"
    },
    {
        "title": "Cross-Modal Recipe Embeddings by Disentangling Recipe Contents and Dish Styles",
        "authors": "['Yu Sugiyama', 'Keiji Yanai']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Nowadays, cooking recipe sharing sites on the Web are widely used, and play a major role in everyday home cooking. Since cooking recipes consist of dish photos and recipe texts, cross-modal recipe search is being actively explored. To enable cross-modal search, both food image features and cooking text recipe features are embedded into the same shared space in general. However, in most of the existing studies, a one-to-one correspondence between a recipe text and a dish image in the embedding space is assumed, although an unlimited number of photos with different serving styles and different plates can be associated with the same recipe. In this paper, we propose a RDE-GAN (Recipe Disentangled Embedding GAN) which separates food image information into a recipe image feature and a non-recipe shape feature. In addition, we generate a food image by integrating both the recipe embedding and a shape feature. Since the proposed embedding is free from serving and plate styles which are unrelated to cooking recipes, the experimental results showed that it outperformed the existing methods on cross-modal recipe search. We also confirmed that only either shape or recipe elements can be changed at the time of food image generation.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475422",
        "category": "Databases"
    },
    {
        "title": "PhotoCube at the Lifelog Search Challenge 2021",
        "authors": "['Jihye Shin', 'Alexandra Waldau', 'Aaron Duane', 'Björn Þór Jónsson']",
        "date": "August 2021",
        "source": "LSC '21: Proceedings of the 4th Annual on Lifelog Search Challenge",
        "abstract": "The Lifelog Search Challenge (LSC) is a venue where retrieval system researchers compete in solving tasks to retrieve the correct image from a lifelog collection. At LSC 2021, we introduce the PhotoCube system as a new competitor. PhotoCube is an interactive media retrieval system that considers media items to exist in a hypercube in multidimensional metadata space. To solve tasks, users explore the contents of the hypercube by dynamically (a) applying a variety of filters and (b) projecting the hypercube to a three-dimensional cube that is visualised on screen.",
        "link": "https://dl.acm.org/doi/10.1145/3463948.3469073",
        "category": "Databases"
    },
    {
        "title": "PerfGuard: deploying ML-for-systems without performance regressions, almost!",
        "authors": "['Remmelt Ammerlaan', 'Gilbert Antonius', 'Marc Friedman', 'H M Sajjad Hossain', 'Alekh Jindal', 'Peter Orenberg', 'Hiren Patel', 'Shi Qiao', 'Vijay Ramani', 'Lucas Rosenblatt', 'Abhishek Roy', 'Irene Shaffer', 'Soundarajan Srinivasan', 'Markus Weimer']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484233",
        "category": "Databases"
    },
    {
        "title": "Synchronization Schemas",
        "authors": "['Rajeev Alur', 'Phillip Hilliard', 'Zachary G. Ives', 'Konstantinos Kallas', 'Konstantinos Mamouras', 'Filip Niksic', 'Caleb Stanford', 'Val Tannen', 'Anton Xue']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "We present a type-theoretic framework for data stream processing for real-time decision making, where the desired computation involves a mix of sequential computation, such as smoothing and detection of peaks and surges, and naturally parallel computation, such as relational operations, key-based partitioning, and map-reduce. Our framework unifies sequential (ordered) and relational (unordered) data models. In particular, we define synchronization schemas as types, and series-parallel streams (SPS) as objects of these types. A synchronization schema imposes a hierarchical structure over relational types that succinctly captures ordering and synchronization requirements among different kinds of data items. Series-parallel streams naturally model objects such as relations, sequences, sequences of relations, sets of streams indexed by key values, time-based and event-based windows, and more complex structures obtained by nesting of these. We introduce series-parallel stream transformers (SPST) as a domain-specific language for modular specification of deterministic transformations over such streams. SPSTs provably specify only monotonic transformations allowing streamability, have a modular structure that can be exploited for correct parallel implementation, and are composable allowing specification of complex queries as a pipeline of transformations.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458317",
        "category": "Databases"
    },
    {
        "title": "De-anonymization Attacks on Neuroimaging Datasets",
        "authors": "['Vikram Ravindra', 'Ananth Grama']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Advances in imaging technologies, combined with inexpensive storage, have led to an explosion in the volume of publicly available neuroimaging datasets. Effective analyses of these images hold the potential for uncovering mechanisms that govern functioning of the human brain, and understanding various neurological diseases and disorders. The potential significance of these studies notwithstanding, a growing concern relates to the protection of privacy and confidentiality of subjects who participate in these studies. In this paper, we present a de-anonymization attack rooted in the innate uniqueness of the structure and function of the human brain. We show that the attack reveals not only the identity of an individual, but also the efficacy with which they performing cognitive tasks. Our attack relies on novel matrix analyses techniques that are used to extract discriminating features in neuroimages. These features correspond to individual-specific signatures that can be matched across datasets for highly accurate identification. We present data preprocessing, signature extraction, and matching techniques that are computationally inexpensive, and can scale to large datasets. We characterize the efficacy of our de-anonymization attacks on publicly available databases. Finally, we discuss implications of the attack and challenges associated with defending against such attacks.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457234",
        "category": "Databases"
    },
    {
        "title": "Bag Query Containment and Information Theory",
        "authors": "['Mahmoud Abo Khamis', 'Phokion G. Kolaitis', 'Hung Q. Ngo', 'Dan Suciu']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "The query containment problem is a fundamental algorithmic problem in data management. While this problem is well understood under set semantics, it is by far less understood under bag semantics. In particular, it is a long-standing open question whether or not the conjunctive query containment problem under bag semantics is decidable. We unveil tight connections between information theory and the conjunctive query containment under bag semantics. These connections are established using information inequalities, which are considered to be the laws of information theory. Our first main result asserts that deciding the validity of a generalization of information inequalities is many-one equivalent to the restricted case of conjunctive query containment in which the containing query is acyclic; thus, either both these problems are decidable or both are undecidable. Our second main result identifies a new decidable case of the conjunctive query containment problem under bag semantics. Specifically, we give an exponential-time algorithm for conjunctive query containment under bag semantics, provided the containing query is chordal and admits a simple junction tree.",
        "link": "https://dl.acm.org/doi/10.1145/3472391",
        "category": "Databases"
    },
    {
        "title": "QuiCK: A Queuing System in CloudKit",
        "authors": "['Kfir Lev-Ari', 'Yizuo Tian', 'Alexander Shraer', 'Chris Douglas', 'Hao Fu', 'Andrey Andreev', 'Kevin Beranek', 'Scott Dugas', 'Alec Grieser', 'Jeremy Hemmo']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We present QuiCK, a queuing system built for managing asynchronous tasks in CloudKit, Apple's storage backend service. QuiCK stores queued messages along with user data in CloudKit, and supports CloudKit's tenancy model including isolation, fair resource allocation, observability, and tenant migration. QuiCK is built on the FoundationDB Record Layer, an open source transactional DBMS. It employs massive two-level sharding, with tens of billions of queues on the first level (separately storing the queued items for each user of every CloudKit app), and hundreds of queues on a second level (one per FoundationDB cluster used by CloudKit). Our evaluation demonstrates that QuiCK scales linearly with additional consumer resources, effectively avoids contention, provides fairness across CloudKit tenants, and executes deferred tasks with low latency.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457567",
        "category": "Databases"
    },
    {
        "title": "Extraction of LMS Student Engagement and Behavioral Patterns in Online Education Using Decision Tree and K-Means Algorithm",
        "authors": "['Ma.Corazon Fernando Raguro', 'Ace Carpio Lagman', 'Lalaine P. Abad', 'Paul Lawrenz S. Ong']",
        "date": "January 2022",
        "source": "APIT '22: Proceedings of the 2022 4th Asia Pacific Information Technology Conference",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3512353.3512373",
        "category": "Databases"
    },
    {
        "title": "Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data",
        "authors": "['Ana Claudia Sima', 'Tarcisio Mendes de Farias', 'Maria Anisimova', 'Christophe Dessimoz', 'Marc Robinson-Rechavi', 'Erich Zbinden', 'Kurt Stockinger']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available.  In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3469119",
        "category": "Databases"
    },
    {
        "title": "Milvus: A Purpose-Built Vector Data Management System",
        "authors": "['Jianguo Wang', 'Xiaomeng Yi', 'Rentong Guo', 'Hai Jin', 'Peng Xu', 'Shengjun Li', 'Xiangyu Wang', 'Xiangzhou Guo', 'Chengming Li', 'Xiaohai Xu', 'Kun Yu', 'Yuxing Yuan', 'Yinghao Zou', 'Jiquan Long', 'Yudong Cai', 'Zhenxiang Li', 'Zhifeng Zhang', 'Yihua Mo', 'Jun Gu', 'Ruiyi Jiang', 'Yi Wei', 'Charles Xie']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457550",
        "category": "Databases"
    },
    {
        "title": "Bringing Common Subexpression Problem from the Dark to Light: Towards Large-Scale Workload Optimizations",
        "authors": "['Mohamed Kechar', 'Ladjel Bellatreche', 'Safia Nait-bahloul']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Nowadays large-scale data-centric systems have become an essential element for companies to store, manipulate and derive value from large volumes of data. Capturing this value depends on the ability of these systems in managing large-scale workloads including complex analytical queries. One of the main characteristics of these queries is that they share computations in terms of selections and joins. Materialized views (MV) have shown their force in speeding up queries by exploiting these redundant computations. MV selection problem (VSP) is one of the most studied problems in the database field. A large majority of the existing solutions follow workload-driven approaches since they facilitate the identification of shared computations. Interesting algorithms have been proposed and implemented in commercial DBMSs. But they fail in managing large-scale workloads. In this paper, we presented a comprehensive framework to select the most beneficial materialized views based on the detection of the common subexpressions shared between queries. This framework gives the right place of the problem of selection of common subexpressions representing the causes of the redundancy. The utility of final MV depends strongly on the selected subexpressions. Once selected, a heuristic is given to select the most beneficial materialized views by considering different query ordering. Finally, experiments have been conducted to evaluate the effectiveness and efficiency of our proposal by considering large workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472180",
        "category": "Databases"
    },
    {
        "title": "Multi-caption Text-to-Face Synthesis: Dataset and Algorithm",
        "authors": "['Jianxin Sun', 'Qi Li', 'Weining Wang', 'Jian Zhao', 'Zhenan Sun']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Text-to-Face synthesis with multiple captions is still an important yet less addressed problem because of the lack of effective algorithms and large-scale datasets. We accordingly propose a Semantic Embedding and Attention (SEA-T2F) network that allows multiple captions as input to generate highly semantically related face images. With a novel Sentence Features Injection Module, SEA-T2F can integrate any number of captions into the network. In addition, an attention mechanism named Attention for Multiple Captions is proposed to fuse multiple word features and synthesize fine-grained details. Considering text-to-face generation is an ill-posed problem, we also introduce an attribute loss to guide the network to generate sentence-related attributes. Existing datasets for text-to-face are either too small or roughly generated according to attribute labels, which is not enough to train deep learning based methods to synthesize natural face images. Therefore, we build a large-scale dataset named CelebAText-HQ, in which each image is manually annotated with 10 captions. Extensive experiments demonstrate the effectiveness of our algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475391",
        "category": "Databases"
    },
    {
        "title": "Cluster and Scatter: A Multi-grained Active Semi-supervised Learning Framework for Scalable Person Re-identification",
        "authors": "['Bingyu Hu', 'Zheng-Jun Zha', 'Jiawei Liu', 'Xierong Zhu', 'Hongtao Xie']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Active learning has recently attracted increasing attention in the task of person re-identification, due to its unique scalability that not only maximally reduces the annotation cost but also retains the satisfying performance. Although some preliminary active learning methods have been explored in scalable person re-identification task, they have the following two problems: 1) the inefficiency in the selection process of image pairs due to the huge search space, and 2) the ineffectiveness caused by ignoring the impact of unlabeled data in model training. Considering that, we propose a Multi-grained Active Semi-Supervised learning framework, named MASS, to address the scalable person re-identification problem existing in the practical scenarios. Specifically, we firstly design a cluster-scatter procedure to alleviate the inefficiency problem, which consists of two components: cluster step and scatter step. The cluster step shrinks the search space into individual small clusters by a coarse-grained clustering method, and the subsequent scatter step further mines the hard distinguished image pairs from unlabelled set to purify the learned clusters by a novel centrality-based adaptive purification strategy. Afterward, we introduce a customized purification loss for the purified clustering, which utilizes the complementary information in both labeled and unlabeled data to optimize the model for solving the ineffectiveness problem. The cluster-scatter procedure and the model optimization are performed in an iterative fashion to achieve the promising performance while greatly reducing the annotation cost. Extensive experimental results have demonstrated that MASS can even achieve a competitive performance with fully supervised methods in the case of extremely less annotation requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475435",
        "category": "Databases"
    },
    {
        "title": "Interactive Re-ranking via Object Entropy-Guided Question Answering for Cross-Modal Image Retrieval",
        "authors": "['Rintaro Yanagi', 'Ren Togo', 'Takahiro Ogawa', 'Miki Haseyama']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Cross-modal image-retrieval methods retrieve desired images from a query text by learning relationships between texts and images. Such a retrieval approach is one of the most effective ways of achieving the easiness of query preparation. Recent cross-modal image-retrieval methods are convenient and accurate when users input a query text that can be used to uniquely identify the desired image. However, in reality, users frequently input ambiguous query texts, and these ambiguous queries make it difficult to obtain desired images. To overcome these difficulties, in this study, we propose a novel interactive cross-modal image-retrieval method based on question answering. The proposed method analyzes candidate images and asks users questions to obtain information that can narrow down retrieval candidates. By only answering questions generated by the proposed method, users can reach their desired images, even when using an ambiguous query text. Experimental results show the proposed method’s effectiveness.",
        "link": "https://dl.acm.org/doi/10.1145/3485042",
        "category": "Databases"
    },
    {
        "title": "PGMJoins: Random Join Sampling with Graphical Models",
        "authors": "['Ali Mohammadi Shanghooshabad', 'Meghdad Kurmanji', 'Qingzhi Ma', 'Michael Shekelyan', 'Mehrdad Almasi', 'Peter Triantafillou']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Modern databases face formidable challenges when called to join (several) massive tables. Joins (especially when entailing many-to-many joins) are very time- and resource-consuming, join results can be too big to keep in memory, and performing analytics/learning tasks over them costs dearly in terms of time, resources, and money (in the cloud). Moreover, although random sampling is a promising idea to mitigate the above problems, the current state of the art leaves lots of room for improvements. With this paper we contribute a principled solution, coined PGMJoins. PGMJoins adapts Probabilistic Graphical Models to deriving provably random samples of the join result for (n-way) key joins, many-to-many joins, and cyclic and acyclic joins. PGMJoins contributes optimizations both for deriving the structure of the graph and for PGM inference. It also contributes a novel Sum-Product Message Passing Algorithm (SP-MPA) to make a uniform sample of the joint distribution (join result) efficiently and a novel way to deal with cyclic joins. Despite the use of PGMs, the learned joint distribution is not approximated, and the uniform samples are drawn from the true distribution. Our experimentation using queries and datasets from TPC-H, JOB, TPC-DS, and Twitter shows PGMJoins to outperform the state of the art (by 2X-28X).",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457302",
        "category": "Databases"
    },
    {
        "title": "Penalty- and Locality-aware Memory Allocation in Redis Using Enhanced AET",
        "authors": "['Cheng Pan', 'Xiaolin Wang', 'Yingwei Luo', 'Zhenlin Wang']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "Due to large data volume and low latency requirements of modern web services, the use of an in-memory key-value (KV) cache often becomes an inevitable choice (e.g., Redis and Memcached). The in-memory cache holds hot data, reduces request latency, and alleviates the load on background databases. Inheriting from the traditional hardware cache design, many existing KV cache systems still use recency-based cache replacement algorithms, e.g., least recently used or its approximations. However, the diversity of miss penalty distinguishes a KV cache from a hardware cache. Inadequate consideration of penalty can substantially compromise space utilization and request service time. KV accesses also demonstrate locality, which needs to be coordinated with miss penalty to guide cache management. In this article, we first discuss how to enhance the existing cache model, the Average Eviction Time model, so that it can adapt to modeling a KV cache. After that, we apply the model to Redis and propose pRedis, Penalty- and Locality-aware Memory Allocation in Redis, which synthesizes data locality and miss penalty, in a quantitative manner, to guide memory allocation and replacement in Redis. At the same time, we also explore the diurnal behavior of a KV store and exploit long-term reuse. We replace the original passive eviction mechanism with an automatic dump/load mechanism, to smooth the transition between access peaks and valleys. Our evaluation shows that pRedis effectively reduces the average and tail access latency with minimal time and space overhead. For both real-world and synthetic workloads, our approach delivers an average of 14.0%∼52.3% latency reduction over a state-of-the-art penalty-aware cache management scheme, Hyperbolic Caching (HC), and shows more quantitative predictability of performance. Moreover, we can obtain even lower average latency (1.1%∼5.5%) when dynamically switching policies between pRedis and HC.",
        "link": "https://dl.acm.org/doi/10.1145/3447573",
        "category": "Databases"
    },
    {
        "title": "Distributed transactions on serverless stateful functions",
        "authors": "['Martijn de Heus', 'Kyriakos Psarakis', 'Marios Fragkoulis', 'Asterios Katsifodimos']",
        "date": "June 2021",
        "source": "DEBS '21: Proceedings of the 15th ACM International Conference on Distributed and Event-based Systems",
        "abstract": "Serverless computing is currently the fastest-growing cloud services segment. The most prominent serverless offering is Function-as-a-Service (FaaS), where users write functions and the cloud automates deployment, maintenance, and scalability. Although FaaS is a good fit for executing stateless functions, it does not adequately support stateful constructs like microservices and scalable, low-latency cloud applications, mainly because it lacks proper state management support and the ability to perform function-to-function calls. Most importantly, executing transactions across stateful functions remains an open problem. In this paper, we introduce a programming model and implementation for transaction orchestration of stateful serverless functions. Our programming model supports serializable distributed transactions with two-phase commit, as well as relaxed transactional guarantees with Sagas. We design and implement our programming model on Apache Flink StateFun. We choose to build our solution on top of StateFun in order to leverage Flink's exactly-once processing and state management guarantees. We base our evaluation on the YCSB benchmark, which we extended with transactional operations and adapted for the SFaaS programming model. Our experiments show that our transactional orchestration adds 10% overhead to the original system and that Sagas can achieve up to 34% more transactions per second than two-phase commit transactions at a sub-200ms latency.",
        "link": "https://dl.acm.org/doi/10.1145/3465480.3466920",
        "category": "Databases"
    },
    {
        "title": "Clustering-based Efficient Privacy-preserving Face Recognition Scheme without Compromising Accuracy",
        "authors": "['Meng Liu', 'Hongsheng Hu', 'Haolong Xiang', 'Chi Yang', 'Lingjuan Lyu', 'Xuyun Zhang']",
        "date": "None",
        "source": "ACM Transactions on Sensor Networks",
        "abstract": "Recently, biometric identification has been extensively used for border control. Some face recognition systems have been designed based on Internet of Things. But the rich personal information contained in face images can cause severe privacy breach and abuse issues during the process of identification if a biometric system has compromised by insiders or external security attacks. Encrypting the query face image is the state-of-the-art solution to protect an individual’s privacy but incurs huge computational cost and poses a big challenge on time-critical identification applications. However, due to their high computational complexity, existing methods fail to handle large-scale biometric repositories where a target face is searched. In this article, we propose an efficient privacy-preserving face recognition scheme based on clustering. Concretely, our approach innovatively matches an encrypted face query against clustered faces in the repository to save computational cost while guaranteeing identification accuracy via a novel multi-matching scheme. To the best of our knowledge, our scheme is the first to reduce the computational complexity from O(M) in existing methods to approximate O(√M), where M is the size of a face repository. Extensive experiments on real-world datasets have shown the effectiveness and efficiency of our scheme.",
        "link": "https://dl.acm.org/doi/10.1145/3448414",
        "category": "Databases"
    },
    {
        "title": "Resource-efficient Shared Query Execution via Exploiting Time Slackness",
        "authors": "['Dixin Tang', 'Zechao Shang', 'William W. Ma', 'Aaron J. Elmore', 'Sanjay Krishnan']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Shared query execution can reduce resource consumption by sharing common sub-expressions across concurrent queries. We show that this is not always the case when regularly querying a dataset under change. Depending on latency goals, how eagerly to incrementally process the new data differs. Naively sharing the execution of queries with different latency goals will push the whole shared plan to meet the lowest latency goal and execute more eagerly than each participating query. The overhead introduced by the eager execution can even offset the benefit of shared query execution. We propose an optimization framework iShare to exploit the benefit of shared execution and avoid the overhead of eager execution. iShare judiciously shares queries with different latency goals and selectively executes parts of the share plan lazily. iShare can significantly reduce resource consumption compared to eagerly executing share plans from the state-of-the-art multi-query optimizer or approaches that execute queries separately.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457282",
        "category": "Databases"
    },
    {
        "title": "The Complexity and Expressive Power of Limit Datalog",
        "authors": "['Mark Kaminski', 'Egor V. Kostylev', 'Bernardo Cuenca Grau', 'Boris Motik', 'Ian Horrocks']",
        "date": "None",
        "source": "Journal of the ACM",
        "abstract": "Motivated by applications in declarative data analysis, in this article, we study DatalogZ—an extension of Datalog with stratified negation and arithmetic functions over integers. This language is known to be undecidable, so we present the fragment of limit DatalogZ programs, which is powerful enough to naturally capture many important data analysis tasks. In limit DatalogZ, all intensional predicates with a numeric argument are limit predicates that keep maximal or minimal bounds on numeric values. We show that reasoning in limit DatalogZ is decidable if a linearity condition restricting the use of multiplication is satisfied. In particular, limit-linear DatalogZ is complete for Δ2EXP and captures Δ2P over ordered datasets in the sense of descriptive complexity. We also provide a comprehensive study of several fragments of limit-linear DatalogZ. We show that semi-positive limit-linear programs (i.e., programs where negation is allowed only in front of extensional atoms) capture coNP over ordered datasets; furthermore, reasoning becomes coNEXP-complete in combined and coNP-complete in data complexity, where the lower bounds hold already for negation-free programs. In order to satisfy the requirements of data-intensive applications, we also propose an additional stability requirement, which causes the complexity of reasoning to drop to EXP in combined and to P in data complexity, thus obtaining the same bounds as for usual Datalog. Finally, we compare our formalisms with the languages underpinning existing Datalog-based approaches for data analysis and show that core fragments of these languages can be encoded as limit programs; this allows us to transfer decidability and complexity upper bounds from limit programs to other formalisms. Therefore, our article provides a unified logical framework for declarative data analysis which can be used as a basis for understanding the impact on expressive power and computational complexity of the key constructs available in existing languages.",
        "link": "https://dl.acm.org/doi/10.1145/3495009",
        "category": "Databases"
    },
    {
        "title": "Probabilistic Data with Continuous Distributions",
        "authors": "['Martin Grohe', 'Benjamin Lucien Kaminski', 'Joost-Pieter Katoen', 'Peter Lindner']",
        "date": "March 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Statistical models of real world data typically involve continuous probability distributions such as normal, Laplace, or exponential distributions. Such distributions are supported by many probabilistic modelling formalisms, including probabilistic database systems. Yet, the traditional theoretical framework of probabilistic databases focuses entirely on finite probabilistic databases.Only recently, we set out to develop the mathematical theory of infinite probabilistic databases. The present paper is an exposition of two recent papers which are cornerstones of this theory. In (Grohe, Lindner; ICDT 2020) we propose a very general framework for probabilistic databases, possibly involving continuous probability distributions, and show that queries have a well-defined semantics in this framework. In (Grohe, Kaminski, Katoen, Lindner; PODS 2020) we extend the declarative probabilistic programming language Generative Datalog, proposed by (B´ar´any et al. 2017) for discrete probability distributions, to continuous probability distributions and show that such programs yield generative models of continuous probabilistic databases.",
        "link": "https://dl.acm.org/doi/10.1145/3471485.3471502",
        "category": "Databases"
    },
    {
        "title": "LifeMon: A MongoDB-Based Lifelog Retrieval Prototype",
        "authors": "['Alexander Christian Faisst', 'Björn Þór Jónsson']",
        "date": "August 2021",
        "source": "LSC '21: Proceedings of the 4th Annual on Lifelog Search Challenge",
        "abstract": "We present LifeMon, a new lifelog retrieval prototype targeting LSC. LifeMon is based around the MongoDB document store, which is one of a host of scalable NoSQL systems developed over the last two decades, with a semi-structured data model that seems well matched with lifelog requirements. Preliminary results indicate that the system is efficient and that novice users can successfully use it to solve some LSC tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3463948.3469066",
        "category": "Databases"
    },
    {
        "title": "Blockchain Extension for PostgreSQL Data Storage",
        "authors": "['Yash Madhwal', 'Darkhan Nurlybay', 'Yury Yanovich']",
        "date": "July 2021",
        "source": "BIOTC '21: Proceedings of the 2021 3rd Blockchain and Internet of Things Conference",
        "abstract": "Blockchain is an emerging technology with the potential to resolve auditing issues. Implementing a new blockchain-related feature implies moving to a platform with another database or duplicating its parts in a blockchain system. Both ways are difficult to migrate and maintain. The alternative is to implement blockchain features within the existing database, including consensus mechanisms and specific data structures for audit needs. The paper describes and evaluates a database extension with blockchain-related structures, leaving consensus beyond the scope. We use an account-based prototype of cryptocurrency as a model example. The proposed extension allows provably checking transaction content and user balance without a full database lookup. The numerical experiments to study the overhead of the proposed extension are provided.",
        "link": "https://dl.acm.org/doi/10.1145/3475992.3476002",
        "category": "Databases"
    },
    {
        "title": "Flexible Interactive Retrieval SysTem 2.0 for Visual Lifelog Exploration at LSC 2021",
        "authors": "['Hoang-Phuc Trang-Trung', 'Thanh-Cong Le', 'Mai-Khiem Tran', 'Van-Tu Ninh', 'Tu-Khiem Le', 'Cathal Gurrin', 'Minh-Triet Tran']",
        "date": "August 2021",
        "source": "LSC '21: Proceedings of the 4th Annual on Lifelog Search Challenge",
        "abstract": "With a huge collection of photos and video clips, it is essential to provide an efficient and easy-to-use system for users to retrieve moments of interest with a wide variation of query types. This motivates us to develop and upgrade our flexible interactive retrieval system for visual lifelog exploration. In this paper, we briefly introduce version 2 of our system with the following main features. Our system supports multiple modalities for interaction and query processing, including visual query by meta-data, text query and visual information matching based on a joint embedding model, scene clustering based on visual and location information, flexible temporal event navigation, and query expansion with visual examples. With the flexibility in system architecture, we expect our system can easily integrate new modules to enhance its functionalities.",
        "link": "https://dl.acm.org/doi/10.1145/3463948.3469072",
        "category": "Databases"
    },
    {
        "title": "Technical Perspective: Probabilistic Data with Continuous Distributions",
        "authors": "['Dan Olteanu']",
        "date": "March 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "The paper entitled \"Probabilistic Data with Continuous Distributions\" overviews recent work on the foundations of infinite probabilistic databases [3, 2]. Prior work on probabilistic databases (PDBs) focused almost exclusively on the finite case: A finite PDB represents a discrete probability distribution over a finite set of possible worlds [4]. In contrast, an infinite PDB models a continuous probability distribution over an infinite set of possible worlds. In both cases, each world is a finite relational database instance. Continuous distributions are essential and commonplace tools for reasoning under uncertainty in practice. Accommodating them in the framework of probabilistic databases brings us closer to applications that naturally rely on both continuous distributions and relational databases.",
        "link": "https://dl.acm.org/doi/10.1145/3471485.3471501",
        "category": "Databases"
    },
    {
        "title": "LifeSeeker 3.0: An Interactive Lifelog Search Engine for LSC'21",
        "authors": "['Thao-Nhu Nguyen', 'Tu-Khiem Le', 'Van-Tu Ninh', 'Minh-Triet Tran', 'Nguyen Thanh Binh', 'Graham Healy', 'Annalina Caputo', 'Cathal Gurrin']",
        "date": "August 2021",
        "source": "LSC '21: Proceedings of the 4th Annual on Lifelog Search Challenge",
        "abstract": "In this paper, we present the interactive lifelog retrieval engine developed for the LSC'21 comparative benchmarking challenge. The LifeSeeker 3.0 interactive lifelog retrieval engine is an enhanced version of our previous system participating in LSC'20 - LifeSeeker 2.0. The system is developed by both Dublin City University and the Ho Chi Minh City University of Science. The implementation of LifeSeeker 3.0 focuses on searching and filtering by text query using a weighted Bag-of-Words model with visual concept augmentation and three weighted vocabularies. The visual similarity search is improved using a bag of local convolutional features; while improving the previous version's performance, enhancing query processing time, result displaying, and browsing support.",
        "link": "https://dl.acm.org/doi/10.1145/3463948.3469065",
        "category": "Databases"
    },
    {
        "title": "Perceptual Quality Assessment of Low-light Image Enhancement",
        "authors": "['Guangtao Zhai', 'Wei Sun', 'Xiongkuo Min', 'Jiantao Zhou']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Low-light image enhancement algorithms (LIEA) can light up images captured in dark or back-lighting conditions. However, LIEA may introduce various distortions such as structure damage, color shift, and noise into the enhanced images. Despite various LIEAs proposed in the literature, few efforts have been made to study the quality evaluation of low-light enhancement. In this article, we make one of the first attempts to investigate the quality assessment problem of low-light image enhancement. To facilitate the study of objective image quality assessment (IQA), we first build a large-scale low-light image enhancement quality (LIEQ) database. The LIEQ database includes 1,000 light-enhanced images, which are generated from 100 low-light images using 10 LIEAs. Rather than evaluating the quality of light-enhanced images directly, which is more difficult, we propose to use the multi-exposure fused (MEF) image and stack-based high dynamic range (HDR) image as a reference and evaluate the quality of low-light enhancement following a full-reference (FR) quality assessment routine. We observe that distortions introduced in low-light enhancement are significantly different from distortions considered in traditional image IQA databases that are well-studied, and the current state-of-the-art FR IQA models are also not suitable for evaluating their quality. Therefore, we propose a new FR low-light image enhancement quality assessment (LIEQA) index by evaluating the image quality from four aspects: luminance enhancement, color rendition, noise evaluation, and structure preserving, which have captured the most key aspects of low-light enhancement. Experimental results on the LIEQ database show that the proposed LIEQA index outperforms the state-of-the-art FR IQA models. LIEQA can act as an evaluator for various low-light enhancement algorithms and systems. To the best of our knowledge, this article is the first of its kind comprehensive low-light image enhancement quality assessment study.",
        "link": "https://dl.acm.org/doi/10.1145/3457905",
        "category": "Databases"
    },
    {
        "title": "Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification",
        "authors": "['Weiming Zhuang', 'Yonggang Wen', 'Shuai Zhang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Person re-identification (ReID) aims to re-identify a person from non-overlapping camera views. Since person ReID data contains sensitive personal information, researchers have adopted federated learning, an emerging distributed training method, to mitigate the privacy leakage risks. However, existing studies rely on data labels that are laborious and time-consuming to obtain. We present FedUReID, a federated unsupervised person ReID system to learn person ReID models without any labels while preserving privacy. FedUReID enables in-situ model training on edges with unlabeled data. A cloud server aggregates models from edges instead of centralizing raw data to preserve data privacy. Moreover, to tackle the problem that edges vary in data volumes and distributions, we personalize training in edges with joint optimization of cloud and edge. Specifically, we propose personalized epoch to reassign computation throughout training, personalized clustering to iteratively predict suitable labels for unlabeled data, and personalized update to adapt the server aggregated model to each edge. Extensive experiments on eight person ReID datasets demonstrate that FedUReID not only achieves higher accuracy but also reduces computation cost by 29%. Our FedUReID system with the joint optimization will shed light on implementing federated learning to more multimedia tasks without data labels.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475182",
        "category": "Databases"
    },
    {
        "title": "Efficient Indexing of 3D Human Motions",
        "authors": "['Petra Budikova', 'Jan Sedmidubsky', 'Pavel Zezula']",
        "date": "August 2021",
        "source": "ICMR '21: Proceedings of the 2021 International Conference on Multimedia Retrieval",
        "abstract": "Digitization of human motion using 2D or 3D skeleton representations offers exciting possibilities for many applications but, at the same time, requires scalable content-based retrieval techniques to make such data reusable. Although a lot of research effort focuses on extracting content-preserving motion features, there is a lack of techniques that support efficient similarity search on a large scale. In this paper, we introduce a new indexing scheme for organizing large collections of spatio-temporal skeleton sequences. Specifically, we apply the motion-word concept to transform skeleton sequences into structured text-like motion documents, and index such documents using an extended inverted-file approach. Over this index, we design a new similarity search algorithm that exploits the properties of the motion-word representation and provides efficient retrieval with a variable level of approximation, possibly reaching constant search costs disregarding the collection size. Experimental results confirm the usefulness of the proposed approach.",
        "link": "https://dl.acm.org/doi/10.1145/3460426.3463646",
        "category": "Databases"
    },
    {
        "title": "Debugging of Wrong and Missing Answers in SPARQL✱",
        "authors": "['Jesús M. Almendros-Jiménez', 'Antonio Becerra-Terón']",
        "date": "August 2021",
        "source": "DBPL '21: The 18th International Symposium on Database Programming Languages",
        "abstract": "The debugging of database queries is a research topic of increasing interest in recent years. The Semantic Web query language SPARQL should be equipped with a debugger for helping users to detect bugs which usually cause empty results as well as wrong and missing answers. Declarative debugging is a well-known debugging method successfully used in other database query languages. In this paper we present the elements of a declarative debugger for SPARQL in which debugging is based on the construction of a debugging tree for a given answer, and the detection of buggy and failure nodes in the debugging tree causing empty results as well as wrong and missing answers. The debugger has been implemented and it is available as web tool.",
        "link": "https://dl.acm.org/doi/10.1145/3475726.3475727",
        "category": "Databases"
    },
    {
        "title": "Predictive replica placement for mobile users in distributed fog data stores with client-side markov models",
        "authors": "['Malte Bellmann', 'Tobias Pfandzelter', 'David Bermbach']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion",
        "abstract": "Mobile clients that consume and produce data are abundant in fog environments and low latency access to this data can only be achieved by storing it in their close physical proximity. To adapt data replication in fog data stores in an efficient manner and make client data available at the fog node that is closest to the client, the systems need to predict both client movement and pauses in data consumption. In this paper, we present variations of Markov model algorithms that can run on clients to increase the data availability while minimizing excess data. In a simulation, we find the availability of data at the closest node can be improved by 35% without incurring the storage and communication overheads of global replication.",
        "link": "https://dl.acm.org/doi/10.1145/3492323.3495595",
        "category": "Databases"
    },
    {
        "title": "The evolution of Amazon redshift",
        "authors": "['Ippokratis Pandis']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In 2013, Amazon Web Services revolutionized the data warehousing industry by launching Amazon Redshift [7], the first fully managed, petabyte-scale enterprise-grade cloud data warehouse. Amazon Redshift made it simple and cost-effective to efficiently analyze large volumes of data using existing business intelligence tools. This launch was a significant leap from the traditional on-premise data warehousing solutions, which were expensive, not elastic, and required significant expertise to tune and operate. Customers embraced Amazon Redshift and it became the fastest growing service in AWS. Today, tens of thousands of customers use Amazon Redshift in AWS's global infrastructure of 25 launched Regions and 81 Availability Zones (AZs), to process exabytes of data daily.The success of Amazon Redshift inspired a lot of innovation in the analytics segment, e.g. [1, 2, 4, 10], which in turn has benefited customers. In the last few years, the use cases for Amazon Redshift have evolved and in response, Amazon Redshift continues to deliver a series of innovations that delight customers.In this paper, we give an overview of Amazon Redshift's system architecture. Amazon Redshift is a columnar MPP data warehouse [7]. As shown in Figure 1, an Amazon Redshift compute cluster consists of a coordinator node, called the leader node, and multiple compute nodes. Data is stored on Redshift Managed Storage, backed by Amazon S3, and cached in compute nodes on locally-attached SSDs in compressed columnar fashion. Tables are either replicated on every compute node or partitioned into multiple buckets that are distributed among all compute nodes. AQUA is a query acceleration layer that leverages FPGAs to improve performance. CaaS is a caching microservice of optimized generated code for the various query fragments executed in the Amazon Redshift fleet.The innovation at Amazon Redshift continues at accelerated pace. Its development is centered around four streams. First, Amazon Redshift strives to provide industry-leading data warehousing performance. Amazon Redshift's query execution blends database operators in each query fragment via code generation. It combines prefetching and vectorized execution with code generation to achieve maximum efficiency. This allows Amazon Redshift to scale linearly when processing from a few terabytes to petabytes of data. Figure 2 depicts the total execution time of the Cloud Data Warehouse Benchmark Derived from TPC-DS 2.13 [6] while scaling dataset size and hardware simultaneously. Amazon Redshift's performance remains nearly flat for a given ratio of data to hardware, as data volume increases from 30TB to 1PB. This linear scaling to the petabyte scale makes it easy, predictable and cost-efficient for customers to on-board new datasets and workloads.Second, customers needed to process more data and wanted to support an increasing number of concurrent users or independent compute clusters that are operating over the Redshift-managed data and the data in Amazon S3. We present Redshift Managed Storage, Redshift's high-performance transactional storage layer, which is disaggregated from the Redshift compute layer and allows a single database to grow to tens of petabytes. We also describe Redshift's compute scaling capabilities. In particular, we present how Redshift can scale up by elastically resizing the size of each cluster, and how Redshift can scale out and increase its throughput via multi-cluster autoscaling, called Concurrency Scaling. With Concurrency Scaling, customers can have thousands of concurrent users executing queries on the same Amazon Redshift endpoint. We also talk about data sharing, which allows users to have multiple isolated compute clusters consume the same datasets in Redshift Managed Storage. Elastic resizing, concurrency scaling and data sharing can be combined giving multiple compute scaling options to the Amazon Redshift customers.Third, as Amazon Redshift became the most widely used cloud data warehouse, its users wanted it to be even easier to use. For that, Redshift introduced ML-based autonomics. We present how Redshift automated among others workload management, physical tuning, the refresh of materialized views (MVs), along with automated MVs-based optimization that rewrites queries to use MVs. We also present how we leverage ML to improve the operational health of the service and deal with gray failures [8].Finally, as AWS offers a wide range of purpose-built services, Amazon Redshift provides seamless integration with the AWS ecosystem and novel abilities in ingesting and ELTing semistructured data (e.g., JSON) using the PartiQL extension of SQL [9]. AWS purpose-built services include the Amazon S3 object storage, transactional databases (e.g., DynamoDB [5] and Aurora [11]) and the ML services of Amazon Sagemaker. We present how AWS and Redshift make it easy for their customers to use the best service for each job and seamlessly take advantage of Redshift's best of class analytics capabilities. For example, we talk about Redshift Spectrum [3] that allows Redshift to query data in open-file formats in Amazon S3. We present how Redshift facilitates both the in-place querying of data in OLTP services, using Redshift's Federated Querying, as well as the copy of data to Redshift, using Glue Elastic Views. We also present how Redshift can leverage the catabilities of Amazon Sagemaker through SQL and without data movement.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476391",
        "category": "Databases"
    },
    {
        "title": "DBNet: a novel deep learning framework for mechanical ventilation prediction using electronic health records",
        "authors": "['Kai Zhang', 'Xiaoqian Jiang', 'Mahboubeh Madadi', 'Luyao Chen', 'Sean Savitz', 'Shayan Shams']",
        "date": "August 2021",
        "source": "BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "abstract": "The outbreak of the Coronavirus disease (COVID-19) pandemic has caused millions of deaths and put immense pressure on the health care system, especially the supply of mechanical ventilators. It is critical for clinicians to identify the patients in a timely manner whose status may deteriorate in the near future and therefore need mechanical ventilators. We propose a prediction model to estimate the probability of requiring mechanical ventilation for in-hospital patients at least 24 hours after their admission. Our model is a multi-modal encoder-decoder attention model that takes full usages of the electronic health record (EHR) database. The EHR database consists of heterogeneous data tables of different formats (diagnosis, drug administrations, medicine prescriptions, lab tests, vital sign observations, clinical procedures, and demographics). We leverage the attention mechanism to increase model performance and promote result interpretability. The attention mechanism also serves the role of the missing data imputation technique, which is often used on irregularly sampled temporal data. We name the model as DBNet as the model takes the database as input. DBNet is evaluated on a large cohort of COVID-19 patients and the result shows it outperforms the state-of-the-art baseline deep learning models in predicting the future requirement of mechanical ventilation. It also outperforms several machine learning models even with sophisticated feature engineering. Due to its ability to handle multiple tables and also longitudinal data, the DBNet can also is not limited to this single application and can be generalized to other healthcare prediction tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3459930.3469551",
        "category": "Databases"
    },
    {
        "title": "Unsupervised Vehicle Search in the Wild: A New Benchmark",
        "authors": "['Xian Zhong', 'Shilei Zhao', 'Xiao Wang', 'Kui Jiang', 'Wenxuan Liu', 'Wenxin Huang', 'Zheng Wang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "In urban surveillance systems, finding a specific vehicle in video frames efficiently and accurately has always been an essential part of traffic supervision and criminal investigation. Existing studies focus on vehicle re-identification (re-ID), but vehicle search is still underexploited. These methods depend on the locations of many vehicles (bounding boxes) that are not available in most real-world applications. Therefore, the unsupervised joint study of vehicle location and identification for the observed scene is a pressing need. Inspired by person search, we conduct a study on the vehicle search while considering four main discrepancies among them, summarized as: 1) It is challenging to select the candidate regions for the observed vehicle due to the perspective differences (front or side); 2) The sides of the same type of vehicles are almost the same, resulting in smaller inter-class; 3) Lacking satisfied dataset for vehicle search to meet the practical scenarios; 4) Supervised search publishing methods rely on datasets with expensive annotations. To address these issues, we have established a new vehicle search dataset. We design an unsupervised framework on this benchmark dataset to generate pseudo labels for further training existing vehicle re-ID or person search models. Experimental results reveal that these methods turn less effective on vehicle search tasks. Therefore, the vehicle search task needs to be further developed, and this dataset can advance the research of vehicle search. Https://github.com/zsl1997/VSW.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475654",
        "category": "Databases"
    },
    {
        "title": "Data management in microservices: state of the practice, challenges, and research directions",
        "authors": "['Rodrigo Laigner', 'Yongluan Zhou', 'Marcos Antonio Vaz Salles', 'Yijian Liu', 'Marcos Kalinowski']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Microservices have become a popular architectural style for data-driven applications, given their ability to functionally decompose an application into small and autonomous services to achieve scalability, strong isolation, and specialization of database systems to the workloads and data formats of each service. Despite the accelerating industrial adoption of this architectural style, an investigation of the state of the practice and challenges practitioners face regarding data management in microservices is lacking. To bridge this gap, we conducted a systematic literature review of representative articles reporting the adoption of microservices, we analyzed a set of popular open-source microservice applications, and we conducted an online survey to cross-validate the findings of the previous steps with the perceptions and experiences of over 120 experienced practitioners and researchers.Through this process, we were able to categorize the state of practice of data management in microservices and observe several foundational challenges that cannot be solved by software engineering practices alone, but rather require system-level support to alleviate the burden imposed on practitioners. We discuss the shortcomings of state-of-the-art database systems regarding microservices and we conclude by devising a set of features for microservice-oriented database systems.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484232",
        "category": "Databases"
    },
    {
        "title": "IP Geolocation through Geographic Clicks",
        "authors": "['Ovidiu Dan', 'Vaibhav Parikh', 'Brian D. Davison']",
        "date": "None",
        "source": "ACM Transactions on Spatial Algorithms and Systems",
        "abstract": "IP geolocation databases map IP addresses to their physical locations. They are used to determine the location of online users when their precise location is unavailable. These databases are vital for a number of online services, including search engine personalization, content delivery, local ads, and fraud detection. However, IP geolocation databases are often inaccurate. In this work we present two novel approaches to improving IP geolocation by mining search engine click logs. First, we show that we can derive which URLs have local affinity by clustering clicks from IPs with known locations. We demonstrate that we can further propagate these URL locations to IP addresses with unknown locations. Our approach significantly outperforms two state-of-the-art commercial IP geolocation databases by 25 and 36 percentage points at a distance error of 10 kilometers, respectively. Second, we present an alternative method of assigning locations to URLs when IP location training data is not available, by instead extracting locations from the body of web documents. This second approach also outperforms the baselines by 7 and 17 percentage points, respectively, and has higher coverage than the first method. Finally, we also demonstrate that our two approaches outperform the academic state of the art based on mining query logs.",
        "link": "https://dl.acm.org/doi/10.1145/3476774",
        "category": "Databases"
    },
    {
        "title": "An evolutionary multi-objective feature selection approach for detecting music segment boundaries of specific types",
        "authors": "['Igor Vatolkin', 'Fabian Ostermann', 'Meinard Müller']",
        "date": "June 2021",
        "source": "GECCO '21: Proceedings of the Genetic and Evolutionary Computation Conference",
        "abstract": "The goal of music segmentation is to identify boundaries between parts of music pieces which are perceived as entities. Segment boundaries often go along with a change in musical properties including instrumentation, key, and tempo (or a combination thereof). One can consider different types (or classes) of boundaries according to these musical properties. In contrast to existing datasets with missing specifications of changing properties for annotated boundaries, we have created a set of artificial music tracks with precise annotations for boundaries of different types. This allows for a profound analysis and interpretation of annotated and predicted boundaries and a more exhaustive comparison of different segmentation algorithms. For this scenario, we formulate a novel multi-objective optimisation task that identifies boundaries of only a specific type. The optimisation is conducted by means of evolutionary multi-objective feature selection and a novelty-based segmentation approach. Furthermore, we provide lists of audio features from non-dominated fronts which most significantly contribute to the estimation of given boundaries (the first objective) and most significantly reduce the performance of the prediction of other boundaries (the second objective).",
        "link": "https://dl.acm.org/doi/10.1145/3449639.3459374",
        "category": "Databases"
    },
    {
        "title": "In-network support for transaction triaging",
        "authors": "['Theo Jepsen', 'Alberto Lerner', 'Fernando Pedone', 'Robert Soulé', 'Philippe Cudré-Mauroux']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We introduce Transaction Triaging, a set of techniques that manipulate streams of transaction requests and responses while they travel to and from a database server. Compared to normal transaction streams, the triaged ones execute faster once they reach the database. The triaging algorithms do not interfere with the transaction execution nor require adherence to any particular concurrency control method, making them easy to port across database systems.Transaction Triaging leverages recent programmable networking hardware that can perform computations on in-flight data. We evaluate our techniques on an in-memory database system using an actual programmable hardware network switch. Our experimental results show that triaging brings enough performance gains to compensate for almost all networking overheads. In high-overhead network stacks such as UDP/IP, we see throughput improvements from 2.05X to 7.95X. In an RDMA stack, the gains range from 1.08X to 1.90X without introducing significant latency.",
        "link": "https://dl.acm.org/doi/10.14778/3461535.3461551",
        "category": "Databases"
    },
    {
        "title": "PolyFrame: a retargetable query-based approach to scaling dataframes",
        "authors": "['Phanwadee Sinthong', 'Michael J. Carey']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In the last few years, the field of data science has been growing rapidly as various businesses have adopted statistical and machine learning techniques to empower their decision-making and applications. Scaling data analyses to large volumes of data requires the utilization of distributed frameworks. This can lead to serious technical challenges for data analysts and reduce their productivity. AFrame, a data analytics library, is implemented as a layer on top of Apache AsterixDB, addressing these issues by providing the data scientists' familiar interface, Pandas Dataframe, and transparently scaling out the evaluation of analytical operations through a Big Data management system. While AFrame is able to leverage data management facilities (e.g., indexes and query optimization) and allows users to interact with a large volume of data, the initial version only generated SQL++ queries and only operated against AsterixDB. In this work, we describe a new design that retargets AFrame's incremental query formation to other query-based database systems, making it more flexible for deployment against other data management systems with composable query languages.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476281",
        "category": "Databases"
    },
    {
        "title": "Symbolic execution of NoSQL applications using versioned schemas",
        "authors": "['Hendrik Winkelmann', 'Herbert Kuchen']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "NoSQL databases can contain complex and heterogeneous data entries in a non-normalized fashion. Contrary to relational databases, often an explicit schema is optional and differently structured entities can be expected in the same data collection. If an explicit schema is absent, the consuming applications are forced to extract and make sense of the implicit schema without the support of the database itself. Automatic tools for systematic test case-generation currently do not support validating this error-prone procedure. To address this issue, we propose an approach applicable to NoSQL databases. We furthermore present a prototypical glass-box test generation tool, JaSoN, an extension of Symbolic Pathfinder for Java. JaSoN generates test cases for applications using MongoDB and accounts for different schema versions for entities of the same type. For this, the real MongoDB instance is replaced with a symbolic data store by mocking relevant MongoDB interfaces and combining this mock with custom Java bytecode.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3442050",
        "category": "Databases"
    },
    {
        "title": "Kamino: constraint-aware differentially private data synthesis",
        "authors": "['Chang Ge', 'Shubhankar Mohapatra', 'Xi He', 'Ihab F. Ilyas']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Organizations are increasingly relying on data to support decisions. When data contains private and sensitive information, the data owner often desires to publish a synthetic database instance that is similarly useful as the true data, while ensuring the privacy of individual data records. Existing differentially private data synthesis methods aim to generate useful data based on applications, but they fail in keeping one of the most fundamental data properties of the structured data --- the underlying correlations and dependencies among tuples and attributes (i.e., the structure of the data). This structure is often expressed as integrity and schema constraints, or with a probabilistic generative process. As a result, the synthesized data is not useful for any downstream tasks that require this structure to be preserved.This work presents KAMINO, a data synthesis system to ensure differential privacy and to preserve the structure and correlations present in the original dataset. KAMINO takes as input of a database instance, along with its schema (including integrity constraints), and produces a synthetic database instance with differential privacy and structure preservation guarantees. We empirically show that while preserving the structure of the data, KAMINO achieves comparable and even better usefulness in applications of training classification models and answering marginal queries than the state-of-the-art methods of differentially private data synthesis.",
        "link": "https://dl.acm.org/doi/10.14778/3467861.3467876",
        "category": "Databases"
    },
    {
        "title": "Trouble at the source",
        "authors": "['Don Monroe']",
        "date": "December 2021",
        "source": "Communications of the ACM",
        "abstract": "Errors and biases in artificial intelligence systems often reflect the data used to train them.",
        "link": "https://dl.acm.org/doi/10.1145/3490155",
        "category": "Databases"
    },
    {
        "title": "Definition of a Knowledge Base Towards a Benchmark for Experiments with Mutation Testing",
        "authors": "['Alessandro Pizzoleto', 'Giovanni Guarnieri', 'Fabiano Ferrari']",
        "date": "September 2021",
        "source": "SBES '21: Proceedings of the XXXV Brazilian Symposium on Software Engineering",
        "abstract": "Context: Mutation testing has been investigated since the late 70’s. Ever since, researchers have devised dozens of mutation approaches, including ways of generating, executing, and analyzing mutants, as well as ways of reducing the costs for its application. However, research on this field falls short when it comes to producing a representative and manageable set of artifacts to enable experiments with the plethora of existing mutation approaches. Objective: In this paper, we describe the process and current results of creating a knowledge base of mutation-related artifacts to support experiments with mutation testing. Method: We setup the Evosuite tool for generating test cases, and the PIT tool for generating and running mutants. We also created scripts to import the results to a relational database. The database includes some procedures to generate killing matrices for the tested Java classes. Results: Beyond establishing the tooling infrastructure, we populated our database with classes extracted from five Java projects, from which four are open source projects hosted in GitHub and the fifth one is composed by simple Java programs. Currently, the database includes around 2,000 classes, 50,000 test cases, and 195,000 mutants. Conclusion: The database structure eases the addition of other Java programs and the related mutation artifacts. Furthermore, it allows for tasks such as minimizing test sets and mutant sets (e.g. by removing redundant tests and trivial mutants), thus providing researchers with a well-established and extensible basis to support varied experiments.",
        "link": "https://dl.acm.org/doi/10.1145/3474624.3477060",
        "category": "Databases"
    },
    {
        "title": "MEDTO: Medical Data to Ontology Matching Using Hybrid Graph Neural Networks",
        "authors": "['Junheng Hao', 'Chuan Lei', 'Vasilis Efthymiou', 'Abdul Quamar', 'Fatma Özcan', 'Yizhou Sun', 'Wei Wang']",
        "date": "August 2021",
        "source": "KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Medical ontologies are widely used to describe and organize medical terminologies and to support many critical applications on healthcare databases. These ontologies are often manually curated (e.g., UMLS, SNOMED CT, and MeSH) by medical experts. Medical databases, on the other hand, are often created by database administrators, using different terminology and structures. The discrepancies between medical ontologies and databases compromise interoperability between them. Data to ontology matching is the process of finding semantic correspondences between tables in databases to standard ontologies. Existing solutions such as ontology matching have mostly focused on engineering features from terminological, structural, and semantic model information extracted from the ontologies. However, this is often labor intensive and the accuracy varies greatly across different ontologies. Worse yet, the ontology capturing a medical database is often not given in practice. In this paper, we propose MEDTO, a novel end-to-end framework that consists of three innovative techniques: (1) a lightweight yet effective method that bootstrap a semantically rich ontology from a given medical database, (2) a hyperbolic graph convolution layer that encodes hierarchical concepts in the hyperbolic space, and (3) a heterogeneous graph layer that encodes both local and global context information of a concept. Experiments on two real-world medical datasets matching against SNOMED CT show significant improvements compared to the state-of-the-art methods. MEDTO also consistently achieves competitive results on a benchmark from the Ontology Alignment Evaluation Initiative.",
        "link": "https://dl.acm.org/doi/10.1145/3447548.3467138",
        "category": "Databases"
    },
    {
        "title": "Log-structured Protocols in Delos",
        "authors": "['Mahesh Balakrishnan', 'Chen Shen', 'Ahmed Jafri', 'Suyog Mapara', 'David Geraghty', 'Jason Flinn', 'Vidhya Venkat', 'Ivailo Nedelchev', 'Santosh Ghosh', 'Mihir Dharamshi', 'Jingming Liu', 'Filip Gruszczynski', 'Jun Li', 'Rounak Tibrewal', 'Ali Zaveri', 'Rajeev Nagar', 'Ahmed Yossef', 'Francois Richard', 'Yee Jiun Song']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Developers have access to a wide range of storage APIs and functionality in large-scale systems, such as relational databases, key-value stores, and namespaces. However, this diversity comes at a cost: each API is implemented by a complex distributed system that is difficult to develop and operate. Delos amortizes this cost by enabling different APIs on a shared codebase and operational platform. The primary innovation in Delos is a log-structured protocol: a fine-grained replicated state machine executing above a shared log that can be layered into reusable protocol stacks under different databases. We built and deployed two production databases using Delos at Facebook, creating nine different log-structured protocols in the process. We show via experiments and production data that log-structured protocols impose low overhead, while allowing optimizations that can improve latency by up to 100X (e.g., via leasing) and throughput by up to 2X (e.g., via batching).",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483544",
        "category": "Databases"
    },
    {
        "title": "Procedural extensions of SQL: understanding their usage in the wild",
        "authors": "['Surabhi Gupta', 'Karthik Ramachandra']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Procedural extensions of SQL have been in existence for many decades now. However, little is known about their magnitude of usage and their complexity in real-world workloads. Procedural code executing in a RDBMS is known to have inefficiencies and limitations; as a result there have been several efforts to address this problem. However, the lack of understanding of their use in real workloads makes it challenging to (a) motivate new work in this area, (b) identify research challenges and opportunities, and (c) demonstrate impact of novel work. We aim to address these challenges with our work.In this paper, we present the results of our in-depth analysis of thousands of stored procedures, user-defined functions and triggers taken from several real workloads. We introduce SQL-ProcBench, a benchmark for procedural workloads in RDBMSs. SQL-ProcBench has been created using the insights derived from our analysis, and thus represents real workloads. Using SQL-ProcBench, we present an experimental evaluation on several database engines to understand and identify research challenges and opportunities. We emphasize the need to work on these interesting and relevant problems, and encourage researchers to contribute to this area.",
        "link": "https://dl.acm.org/doi/10.14778/3457390.3457402",
        "category": "Databases"
    },
    {
        "title": "Analyses on Multi-sensor Fingerprint Enhancement and Indexing",
        "authors": "['Wei Zhou', 'Yong Ding']",
        "date": "February 2022",
        "source": "ACSW '22: Proceedings of the 2022 Australasian Computer Science Week",
        "abstract": "Fingerprint biometric has been widely applied in both forensic law enforcement and security applications, and the most common application is access control. In recent years, we have witnessed the development of touchless fingerprint acquisition technology, which can generate 3D representation of fingerprints. Although some research has been carried out on 3D fingerprint feature extraction (e.g. 3D minutiae) and identification, it is computationally expensive and time-consuming to compare two sets of 3D minutiae in real time. Therefore, the common way to develop an Automatic Fingerprint Identification System (AFIS) using 3D fingerprints is to unwrap 3D fingerprints into 2D equivalent images, then the existing algorithms for 2D fingerprint processing can be utilized. In this paper, we present some observations of 2D to 3D (unraveled 2D equivalent images from 3D model) fingerprints recognition based on a publicly available database. We tested the performance of 2D to 3D fingerprint verification using enhanced unraveled 2D equivalent images. The results show that the cropped enhanced 3D images based on singular points can achieve the best performance. Since enhanced fingerprint images can result in efficient fingerprint indexing, we also made some analyses on the hash bit selection of MCC indexing scheme using the cropped enhanced 3D images based on singular points.",
        "link": "https://dl.acm.org/doi/10.1145/3511616.3513096",
        "category": "Databases"
    },
    {
        "title": "Pedometer-free Geomagnetic Fingerprinting with Casual Walking Speed",
        "authors": "['Hang Wu', 'Jiajie Tan', 'S.-H. Gary Chan']",
        "date": "None",
        "source": "ACM Transactions on Sensor Networks",
        "abstract": "The geomagnetic field has been wildly advocated as an effective signal for fingerprint-based indoor localization due to its omnipresence and local distinctive features. Prior survey-based approaches to collect magnetic fingerprints often required surveyors to walk at constant speeds or rely on a meticulously calibrated pedometer (step counter) or manual training. This is inconvenient, error-prone, and not highly deployable in practice. To overcome that, we propose Maficon, a novel and efficient pedometer-free approach for geomagnetic fingerprint database construction. In Maficon, a surveyor simply walks at casual (arbitrary) speed along the survey path to collect geomagnetic signals. By correlating the features of geomagnetic signals and accelerometer readings (user motions), Maficon adopts a self-learning approach and formulates a quadratic programming to accurately estimate the walking speed in each signal segment and label these segments with their physical locations. To the best of our knowledge, Maficon is the first piece of work on pedometer-free magnetic fingerprinting with casual walking speed. Extensive experiments show that Maficon significantly reduces walking speed estimation error (by more than 20%) and hence fingerprint error (by 35% in general) as compared with traditional and state-of-the-art schemes.",
        "link": "https://dl.acm.org/doi/10.1145/3470850",
        "category": "Databases"
    },
    {
        "title": "RSS Remeasurement Estimation for Indoor Positioning System with Generative Adversarial Network Model",
        "authors": "['Xiaoqi Ren', 'Wenyuan Tao', 'Chung-Ming Own']",
        "date": "May 2021",
        "source": "CNIOT '21: Proceedings of the 2021 2nd International Conference on Computing, Networks and Internet of Things",
        "abstract": "With the rapid development of computer technology in recent years, the demand for services to obtain location information is increasing day by day. The expansion of mobile computing has prompted wireless fidelity(WiFi)-based indoor localization to be one of the most attractive and promising techniques for ubiquitous application. The main problem with this technology is that received signal strength (RSS) values need to be manually sampled to create a WiFi fingerprint database in the offline database creation stage. As any change in the environment may affect signal propagation, the fingerprint database must be repeatedly updated and maintained, and this is a time-consuming task. In this study, we propose the Adaptive Context Generative Adversarial Networks (ACOGAN) model by adopting random sampling RSS values as the feedbacks and predicting the residual RSS values as the missing fingerprints to solve the remeasurement problem. By employing feedbacks, the missing fingerprints can be rebuilt efficiently and accurately using the ACOGAN model. The experimental results not only illustrate the superiority of the proposed ACOGAN model on field and simulated environments but also demonstrate its robustness to satisfy the time consumption on the learning progress.",
        "link": "https://dl.acm.org/doi/10.1145/3468691.3468742",
        "category": "Databases"
    },
    {
        "title": "Interactive Multimodal Lifelog Retrieval with vitrivr at LSC 2021",
        "authors": "['Silvan Heller', 'Ralph Gasser', 'Mahnaz Parian-Scherb', 'Sanja Popovic', 'Luca Rossetto', 'Loris Sauter', 'Florian Spiess', 'Heiko Schuldt']",
        "date": "August 2021",
        "source": "LSC '21: Proceedings of the 4th Annual on Lifelog Search Challenge",
        "abstract": "The Lifelog Search Challenge (LSC) is an annual benchmarking competition for interactive multimedia retrieval systems, where participating systems compete in finding events based on textual descriptions containing hints about structured, semi-structured, and/or unstructured data. In this paper, we present the multimedia retrieval system vitrivr, a long-time participant to LSC, with a focus on new functionality. Specifically, we introduce the image stabilisation module which is added prior to the feature extraction to reduce the image degradation caused by lifelogger movements, and discuss how geodata is used during query formulation, query execution, and result presentation.",
        "link": "https://dl.acm.org/doi/10.1145/3463948.3469062",
        "category": "Databases"
    },
    {
        "title": "Beyond equi-joins: ranking, enumeration and factorization",
        "authors": "['Nikolaos Tziavelis', 'Wolfgang Gatterbauer', 'Mirek Riedewald']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We study theta-joins in general and join predicates with conjunctions and disjunctions of inequalities in particular, focusing on ranked enumeration where the answers are returned incrementally in an order dictated by a given ranking function. Our approach achieves strong time and space complexity properties: with n denoting the number of tuples in the database, we guarantee for acyclic full join queries with inequality conditions that for every value of k, the k top-ranked answers are returned in O(n polylog n + k log k) time. This is within a polylogarithmic factor of O(n + k log k), i.e., the best known complexity for equi-joins, and even of O(n + k), i.e., the time it takes to look at the input and return k answers in any order. Our guarantees extend to join queries with selections and many types of projections (namely those called \"free-connex\" queries and those that use bag semantics). Remarkably, they hold even when the number of join results is nℓ for a join of ℓ relations. The key ingredient is a novel O(n polylog n)-size factorized representation of the query output, which is constructed on-the-fly for a given query and database. In addition to providing the first nontrivial theoretical guarantees beyond equi-joins, we show in an experimental study that our ranked-enumeration approach is also memory-efficient and fast in practice, beating the running time of state-of-the-art database systems by orders of magnitude.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476306",
        "category": "Databases"
    },
    {
        "title": "Software Quality Assessment of a Web Application for Biomedical Data Analysis",
        "authors": "['Lena Wiese', 'Ingmar Wiese', 'Kristina Lietz']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Data Science as a multidisciplinary discipline has seen a massive transformation in the direction of operationalisation of analysis workflows. Yet it can be observed that such a workflow consists of potentially many diverse components: like modules in different programming languages, database backends, or web frontends. In order to achieve high efficiency and reproducibility of the analysis, a sufficiently high level of software engineering for the different components as well as an overall software architecture that integrates and automates the different components is needed. For the use case of gene expression analysis, from a software quality point of view we analyze a newly developed web application that allows user-friendly access to the underlying workflow.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472172",
        "category": "Databases"
    },
    {
        "title": "Data-Driven Imitation Learning for a Shopkeeper Robot with Periodically Changing Product Information",
        "authors": "['Malcolm Doering', 'Dražen Brščić', 'Takayuki Kanda']",
        "date": "None",
        "source": "ACM Transactions on Human-Robot Interaction",
        "abstract": "Data-driven imitation learning enables service robots to learn social interaction behaviors, but these systems cannot adapt after training to changes in the environment, such as changing products in a store. To solve this, a novel learning system that uses neural attention and approximate string matching to copy information from a product information database to its output is proposed. A camera shop interaction dataset was simulated for training/testing. The proposed system was found to outperform a baseline and a previous state of the art in an offline, human-judged evaluation.",
        "link": "https://dl.acm.org/doi/10.1145/3451883",
        "category": "Databases"
    },
    {
        "title": "Modularis: modular relational analytics over heterogeneous distributed platforms",
        "authors": "['Dimitrios Koutsoukos', 'Ingo Müller', 'Renato Marroquín', 'Ana Klimovic', 'Gustavo Alonso']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The enormous quantity of data produced every day together with advances in data analytics has led to a proliferation of data management and analysis systems. Typically, these systems are built around highly specialized monolithic operators optimized for the underlying hardware. While effective in the short term, such an approach makes the operators cumbersome to port and adapt, which is increasingly required due to the speed at which algorithms and hardware evolve. To address this limitation, we present Modularis, an execution layer for data analytics based on sub-operators, i.e., composable building blocks resembling traditional database operators but at a finer granularity. To demonstrate the feasibility and advantages of our approach, we use Modularis to build a distributed query processing system supporting relational queries running on an RDMA cluster, a serverless cloud platform, and a smart storage engine. Modularis requires minimal code changes to execute queries across these three diverse hardware platforms, showing that the sub-operator approach reduces the amount and complexity of the code to maintain. In fact, changes in the platform affect only those sub-operators that depend on the underlying hardware (in our use cases, mainly the sub-operators related to network communication). We show the end-to-end performance of Modularis by comparing it with a framework for SQL processing (Presto), a commercial cluster database (SingleStore), as well as Query-as-a-Service systems (Athena, BigQuery). Modularis outperforms all these systems, proving that the design and architectural advantages of a modular design can be achieved without degrading performance. We also compare Modularis with a hand-optimized implementation of a join for RDMA clusters. We show that Modularis has the advantage of being easily extensible to a wider range of join variants and group by queries, all of which are not supported in the hand-tuned join.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484229",
        "category": "Databases"
    },
    {
        "title": "Data-Oriented Differential Testing of Object-Relational Mapping Systems",
        "authors": "['Thodoris Sotiropoulos', 'Stefanos Chaliasos', 'Vaggelis Atlidakis', 'Dimitris Mitropoulos', 'Diomidis Spinellis']",
        "date": "May 2021",
        "source": "ICSE '21: Proceedings of the 43rd International Conference on Software Engineering",
        "abstract": "We introduce, what is to the best of our knowledge, the first approach for systematically testing Object-Relational Mapping (ORM) systems. Our approach leverages differential testing to establish a test oracle for ORM-specific bugs. Specifically, we first generate random relational database schemas, set up the respective databases, and then, we query these databases using the APIs of the ORM systems under test. To tackle the challenge that ORMs lack a common input language, we generate queries written in an abstract query language. These abstract queries are translated into concrete, executable ORM queries, which are ultimately used to differentially test the correctness of target implementations. The effectiveness of our method heavily relies on the data inserted to the underlying databases. Therefore, we employ a solver-based approach for producing targeted database records with respect to the constraints of the generated queries. We implement our approach as a tool, called CYNTHIA, which found 28 bugs in five popular ORM systems. The vast majority of these bugs are confirmed (25 / 28), more than half were fixed (20 / 28), and three were marked as release blockers by the corresponding developers.",
        "link": "https://dl.acm.org/doi/10.1109/ICSE43902.2021.00137",
        "category": "Databases"
    },
    {
        "title": "DBMind: a self-driving platform in openGauss",
        "authors": "['Xuanhe Zhou', 'Lianyuan Jin', 'Ji Sun', 'Xinyang Zhao', 'Xiang Yu', 'Jianhua Feng', 'Shifu Li', 'Tianqing Wang', 'Kun Li', 'Luyang Liu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We demonstrate a self-driving system DBMind, which provides three autonomous capabilities in database, including self-monitoring, self-diagnosis and self-optimization. First, self-monitoring judiciously collects database metrics and detects anomalies (e.g., slow queries and IO contention), which can profile database status while only slightly affecting system performance (<5%). Then, self-diagnosis utilizes an LSTM model to analyze the root causes of the anomalies and automatically detect root causes from a pre-defined failure hierarchy. Next, self-optimization automatically optimizes the database performance using learning-based techniques, including deep reinforcement learning based knob tuning, reinforcement learning based index selection, and encoder-decoder based view selection. We have implemented DBMind in an open source database openGauss and demonstrated real scenarios.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476334",
        "category": "Databases"
    },
    {
        "title": "CHIMERA: Supporting Wearables Development across Multidisciplinary Perspectives",
        "authors": "['Luis Paredes', 'Caroline McMillan', 'Wan Kyn Chan', 'Senthil Chandrasegaran', 'Ramyak Singh', 'Karthik Ramani', 'Danielle Wilde']",
        "date": "None",
        "source": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "abstract": "Wearable technologies draw on a range of disciplines, including fashion, textiles, HCI, and engineering. Due to differences in methodology, wearables researchers can experience gaps or breakdowns in values, goals, and vocabulary when collaborating. This situation makes wearables development challenging, even more so when technologies are in the early stages of development and their technological and cultural potential is not fully understood. We propose a common ground to enhance the accessibility of wearables-related resources. The objective is to raise awareness and create a convergent space for researchers and developers to both access and share information across domains. We present CHIMERA, an online search interface that allows users to explore wearable technologies beyond their discipline. CHIMERA is powered by a Wearables Taxonomy and a database of research, tutorials, aesthetic approaches, concepts, and patents. To validate CHIMERA, we used a design task with multidisciplinary designers, an open-ended usability study with experts, and a usability survey with students of a wearables design class. Our findings suggest that CHIMERA assists users with different mindsets and skillsets to engage with information, expand and share knowledge when developing wearables. It forges common ground across divergent disciplines, encourages creativity, and affords the formation of inclusive, multidisciplinary perspectives in wearables development.",
        "link": "https://dl.acm.org/doi/10.1145/3494974",
        "category": "Databases"
    },
    {
        "title": "IP Geolocation through Reverse DNS",
        "authors": "['Ovidiu Dan', 'Vaibhav Parikh', 'Brian D. Davison']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "IP Geolocation databases are widely used in online services to map end-user IP addresses to their geographical location. However, they use proprietary geolocation methods, and in some cases they have poor accuracy. We propose a systematic approach to use reverse DNS hostnames for geolocating IP addresses, with a focus on end-user IP addresses as opposed to router IPs. Our method is designed to be combined with other geolocation data sources. We cast the task as a machine learning problem where, for a given hostname, we first generate a list of potential location candidates, and then we classify each hostname and candidate pair using a binary classifier to determine which location candidates are plausible. Finally, we rank the remaining candidates by confidence (class probability) and break ties by population count. We evaluate our approach against three state-of-the-art academic baselines and two state-of-the-art commercial IP geolocation databases. We show that our work significantly outperforms the academic baselines and is complementary and competitive with commercial databases. To aid reproducibility, we open source our entire approach and make it available to the academic community.",
        "link": "https://dl.acm.org/doi/10.1145/3457611",
        "category": "Databases"
    },
    {
        "title": "Making RDBMSs efficient on graph workloads through predefined joins",
        "authors": "['Guodong Jin', 'Semih Salihoglu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Joins in native graph database management systems (GDBMSs) are predefined to the system as edges, which are indexed in adjacency list indices and serve as pointers. This contrasts with and can be more performant than value-based joins in RDBMSs. Existing approaches to integrate predefined joins into RDBMSs adopt a strict separation of graph and relational data and processors, where a graph-specific processor uses left-deep and index nested loop joins (INLJ) for a subset of joins. In this paper we study and experimentally evaluate this technique's performance against an alternative technique that is based on using hash joins that use system-level row IDs (RIDs). In this alternative approach, when a join between two tables is predefined to the system, the RIDs of joining tuples are materialized in extended tables and optionally in RID indices. Instead of using the RID index to perform the join directly, we use it primarily in hash joins to generate filters that can be passed to scans using sideways information passing (sip), ensuring sequential scans. We further compare these two approaches against: (i) the default value-based joins of an RDBMS; and (ii) using materialized views that can avoid evaluating predefined joins completely and instead replace them with scans. We integrated our alternative approach to DuckDB and call the resulting system GRainDB. Our evaluation demonstrates that existing INJL-based approach can be very efficient when entity relations contain very selective filters. However, GRainDB's approach is more robust and is either competitive with or outperforms the INLJ-based approach across a wide range of settings. We further demonstrate that GRainDB far improves the performance of DuckDB, which uses default value-based joins, on relational and graph workloads with large many-to-many joins, making it competitive with a state-of-the-art GDBMS, and incurs no major overheads otherwise.",
        "link": "https://dl.acm.org/doi/10.14778/3510397.3510400",
        "category": "Databases"
    },
    {
        "title": "Explanations for Data Repair Through Shapley Values",
        "authors": "['Daniel Deutch', 'Nave Frost', 'Amir Gilad', 'Oren Sheffer']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Data repair, i.e., the identification and fix of errors in the data, is a central component of the Data Science cycle. As such, significant research effort has been devoted to automate the repair process. Yet it still requires significant manual labor by the Data Scientists, tweaking and optimizing repair modules (up to 80% of their time, according to surveys).   To this end, we propose in this paper a novel framework for explaining the results of any data repair module. Explanations involve identifying the table cells and database constraints having the strongest influence on the process. Influence, in turn, is quantified through the game-theoretic notion of Shapley values, commonly used for explaining Machine Learning classifier results. The main technical challenge is that exact computation of Shapley values incurs exponential time. We consequently devise and optimize novel approximation algorithms, and analyze them both theoretically and empirically. Our results show the efficiency of our approach when compared to the alternative of adapting existing Shapley value computation techniques to the data repair settings.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3482341",
        "category": "Databases"
    },
    {
        "title": "Towards cost-optimal query processing in the cloud",
        "authors": "['Viktor Leis', 'Maximilian Kuschewski']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Public cloud providers offer hundreds of heterogeneous hardware instances. For analytical query processing systems, this presents a major challenge: depending on the hardware configuration, performance and cost may differ by orders of magnitude. We propose a simple and intuitive model that takes the workload, hardware, and cost into account to determine the optimal instance configuration. We discuss how such a model-based approach can significantly reduce costs and also guide the evolution of cloud-native database systems to achieve our vision of cost-optimal query processing.",
        "link": "https://dl.acm.org/doi/10.14778/3461535.3461549",
        "category": "Databases"
    },
    {
        "title": "Residual Sensitivity for Differentially Private Multi-Way Joins",
        "authors": "['Wei Dong', 'Ke Yi']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "A general-purpose query engine that supports a large class of SQLs under differential privacy is the holy grail in privacy-preserving query release. The join operator presents a major difficulty towards realizing this goal, since a single tuple may affect a large number of query results, and the problem worsens as more relations are involved in the join. The traditional approach of global sensitivity fails to work as it assumes pessimistically that every pair of tuples from two different relations may join. To address the issue, instance-dependent sensitivity measures have been proposed, but so far none has met the following three desiderata for it to be truly practical: (1) the released answer should have low noise levels (i.e., high utility); (2) it can be computed efficiently; and (3) the method can be easily integrated into an existing relational database. This paper presents the first differentially private mechanism for multi-way joins that satisfies all three desiderata while supporting any number of private relations, moving us one step closer to a full-featured query engine for private relational data.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452813",
        "category": "Databases"
    },
    {
        "title": "Vertex-centric Parallel Computation of SQL Queries",
        "authors": "['Ainur Smagulova', 'Alin Deutsch']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We present a scheme for parallel execution of SQL queries on top of any vertex-centric BSP graph processing engine. The scheme comprises a graph encoding of relational instances and a vertex program specification of our algorithm called TAG-join, which matches the theoretical communication and computation complexity of state-of-the-art join algorithms. When run on top of the vertex-centric TigerGraph database engine on a single multi-core server, TAG-join exploits thread parallelism and is competitive with (and often outperforms) reference RDBMSs on the TPC benchmarks they are traditionally tuned for. In a distributed cluster, TAG-join outperforms the popular Spark SQL engine.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457314",
        "category": "Databases"
    },
    {
        "title": "Game Illusionization: A Workflow for Applying Optical Illusions to Video Games",
        "authors": "['Po-Yao (Cosmos) Wang', 'Cong-He Xu', 'Ping-Yi Wang', 'Hsin-Yu Huang', 'Yu-Wei Chang', 'Jen-Hao Cheng', 'Yu-Hsin Lin', 'Lung-Pan Cheng']",
        "date": "October 2021",
        "source": "UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology",
        "abstract": "Optical illusions have been brought into recent video games to enhance gaming experiences. However, a large corpus of optical illusions remains unused, while few games incorporate illusions seamlessly. To mitigate the gap, we propose a workflow to guide game designers in applying optical illusions to their video games, i.e., in making more illusion games. In particular, our workflow consists of 5 stages: (1) choosing a game object, (2) searching for a matching illusion, (3) selecting an illusion mechanic, (4) integrating the selected illusion into the game, and (5) optionally revealing the illusion. To facilitate our workflow, we provide a tag database with 163 illusions that are labeled by their in-game visual elements and desired effects. We also provide example editing interfaces of 6 illusions for game designers. We walk through our workflow and showcase 6 resulting illusion games. We implemented these 6 games (with and without illusion) and conducted a 12-participant study to gain a preliminary understanding of how illusions enhance gaming experiences. To evaluate our workflow, we invited 6 game designers and 6 experienced players to follow our workflow and design their own illusion games, where 3 experienced game designers completed 2-week in-depth developments. We report their games, qualitative feedback and discuss reflection on our workflow, database and editing interfaces.",
        "link": "https://dl.acm.org/doi/10.1145/3472749.3474824",
        "category": "Databases"
    },
    {
        "title": "Locomotion Vault: the Extra Mile in Analyzing VR Locomotion Techniques",
        "authors": "['Massimiliano Di Luca', 'Hasti Seifi', 'Simon Egan', 'Mar Gonzalez-Franco']",
        "date": "May 2021",
        "source": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "abstract": "Numerous techniques have been proposed for locomotion in virtual reality (VR). Several taxonomies consider a large number of attributes (e.g., hardware, accessibility) to characterize these techniques. However, finding the appropriate locomotion technique (LT) and identifying gaps for future designs in the high-dimensional space of attributes can be quite challenging. To aid analysis and innovation, we devised Locomotion Vault (https://locomotionvault.github.io/), a database and visualization of over 100 LTs from academia and industry. We propose similarity between LTs as a metric to aid navigation and visualization. We show that similarity based on attribute values correlates with expert similarity assessments (a method that does not scale). Our analysis also highlights an inherent trade-off between simulation sickness and accessibility across LTs. As such, Locomotion Vault shows to be a tool that unifies information on LTs and enables their standardization and large-scale comparison to help understand the space of possibilities in VR locomotion.",
        "link": "https://dl.acm.org/doi/10.1145/3411764.3445319",
        "category": "Databases"
    },
    {
        "title": "Provenance-based data skipping",
        "authors": "['Xing Niu', 'Boris Glavic', 'Ziyu Liu', 'Pengyuan Li', 'Dieter Gawlick', 'Vasudha Krishnaswamy', 'Zhen Hua Liu', 'Danica Porobic']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Database systems use static analysis to determine upfront which data is needed for answering a query and use indexes and other physical design techniques to speed-up access to that data. However, for important classes of queries, e.g., HAVING and top-k queries, it is impossible to determine up-front what data is relevant. To overcome this limitation, we develop provenance-based data skipping (PBDS), a novel approach that generates provenance sketches to concisely encode what data is relevant for a query. Once a provenance sketch has been captured it is used to speed up subsequent queries. PBDS can exploit physical design artifacts such as indexes and zone maps.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494130",
        "category": "Databases"
    },
    {
        "title": "A Survey of Similarity Measures for Time stamped Temporal Datasets",
        "authors": "['Aravind Cheruvu', 'Radhakrishna Vangipuram']",
        "date": "April 2021",
        "source": "DATA'21: International Conference on Data Science, E-learning and Information Systems 2021",
        "abstract": "Temporal transactional databases are transactional databases which store data in a temporal aspect. Usage of similarity of measures in temporal data mining tasks have gained significant importance to retrieve information and interesting patterns in data. It is always crucial to understand and decide what similarity measure we should use while performing a data mining task and this is always driven by the actual data and nature of the temporal data sets. The main objective of this research is to perform a detailed survey of the various similarity measures used in the temporal data mining in recent research contributions. This paper also provides insights on how these similarity measures are used in the Temporal association rule mining algorithms based on the works carried out in the literature.",
        "link": "https://dl.acm.org/doi/10.1145/3460620.3460754",
        "category": "Databases"
    },
    {
        "title": "Clustering legal artifacts using text mining",
        "authors": "['Zoi Lachana', 'Michalis Avgerinos Loutsaris', 'Charalampos Alexopoulos', 'Yannis Charalabidis']",
        "date": "October 2021",
        "source": "ICEGOV '21: Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance",
        "abstract": "The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.",
        "link": "https://dl.acm.org/doi/10.1145/3494193.3494202",
        "category": "Databases"
    },
    {
        "title": "Functional Genomics Platform, A Cloud-Based Platform for Studying Microbial Life at Scale",
        "authors": "['Edward E. Seabolt', 'Gowri Nayar', 'Harsha Krishnareddy', 'Akshay Agarwal', 'Kristen L. Beck', 'Ignacio Terrizzano', 'Eser Kandogan', 'Mark Kunitomi', 'Mary Roth', 'Vandana Mukherjee', 'James H. Kaufman']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "The rapid growth in biological sequence data is revolutionizing our understanding of genotypic diversity and challenging conventional approaches to informatics. With the increasing availability of genomic data, traditional bioinformatic tools require substantial computational time and the creation of ever-larger indices each time a researcher seeks to gain insight from the data. To address these challenges, we pre-computed important relationships between biological entities spanning the Central Dogma of Molecular Biology and captured this information in a relational database. The database can be queried across hundreds of millions of entities and returns results in a fraction of the time required by traditional methods. In this paper, we describe <italic>Functional Genomics Platform</italic> (formerly known as OMXWare), a comprehensive database relating genotype to phenotype for bacterial life. Continually updated, the Functional Genomics Platform today contains data derived from 200,000 curated, self-consistently assembled genomes. The database stores functional data for over 68 million genes, 52 million proteins, and 239 million domains with associated biological activity annotations from Gene Ontology, KEGG, MetaCyc, and Reactome. The Functional Genomics Platform maps all of the many-to-many connections between each biological entity including the originating genome, gene, protein, and protein domain. Various microbial studies, from infectious disease to environmental health, can benefit from the rich data and connections. We describe the data selection, the pipeline to create and update the Functional Genomics Platform, and the developer tools (Python SDK and REST APIs)which allow researchers to efficiently study microbial life at scale.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2020.3021231",
        "category": "Databases"
    },
    {
        "title": "OnionPIR: Response Efficient Single-Server PIR",
        "authors": "['Muhammad Haris Mughees', 'Hao Chen', 'Ling Ren']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "This paper presents OnionPIR and stateful OnionPIR, two single-server PIR schemes that significantly improve the response size and computation cost over state-of-the-art schemes. OnionPIR scheme utilizes recent advances in somewhat homomorphic encryption (SHE) and carefully composes two lattice-based SHE schemes and homomorphic operations to control the noise growth and response size. Stateful OnionPIR uses a technique based on the homomorphic evaluation of copy networks. OnionPIR achieves a response overhead of just 4.2x over the insecure baseline, in contrast to the 100x response overhead of state-of-the-art schemes. Our stateful OnionPIR scheme improves upon the recent stateful PIR framework of Patel et al. and drastically reduces its response overhead by avoiding downloading the entire database in the offline stage. Compared to stateless OnionPIR, Stateful OnionPIR reduces the computation cost by 1.8~x for different database sizes.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3485381",
        "category": "Databases"
    },
    {
        "title": "IP Geolocation Using Traceroute Location Propagation and IP Range Location Interpolation",
        "authors": "['Ovidiu Dan', 'Vaibhav Parikh', 'Brian D. Davison']",
        "date": "April 2021",
        "source": "WWW '21: Companion Proceedings of the Web Conference 2021",
        "abstract": "Many online services, including search engines, content delivery networks, ad networks, and fraud detection utilize IP geolocation databases to map IP addresses to their physical locations. However, IP geolocation databases are often inaccurate. We present a novel IP geolocation technique based on combining propagating IP location information through traceroutes with IP interpolation. Using a large ground truth set, we show that physical locations of IP addresses can be propagated along traceroute paths. We also experiment with and expand upon the concept of IP range location interpolation, where we use the location of individual addresses in an IP range to assign a location to the entire range. The results show that our approach significantly outperforms commercial geolocation by up to 31 percentage points. We open source several components to aid in reproducing our results.",
        "link": "https://dl.acm.org/doi/10.1145/3442442.3451888",
        "category": "Databases"
    },
    {
        "title": "Legal Text Processing: Combing two legal ontological approaches through text mining",
        "authors": "['Michalis Avgerinos Loutsaris', 'Zoi Lachana', 'Charalampos Alexopoulos', 'Yannis Charalabidis']",
        "date": "June 2021",
        "source": "DG.O'21: DG.O2021: The 22nd Annual International Conference on Digital Government Research",
        "abstract": "The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.",
        "link": "https://dl.acm.org/doi/10.1145/3463677.3463730",
        "category": "Databases"
    },
    {
        "title": "Consistent and Flexible Selectivity Estimation for High-Dimensional Data",
        "authors": "['Yaoshu Wang', 'Chuan Xiao', 'Jianbin Qin', 'Rui Mao', 'Makoto Onizuka', 'Wei Wang', 'Rui Zhang', 'Yoshiharu Ishikawa']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Selectivity estimation aims at estimating the number of database objects that satisfy a selection criterion. Answering this problem accurately and efficiently is essential to many applications, such as density estimation, outlier detection, query optimization, and data integration. The estimation problem is especially challenging for large-scale high-dimensional data due to the curse of dimensionality, the large variance of selectivity across different queries, and the need to make the estimator consistent (i.e., the selectivity is non-decreasing in the threshold). We propose a new deep learning-based model that learns a query-dependent piecewise linear function as selectivity estimator, which is flexible to fit the selectivity curve of any distance function and query object, while guaranteeing that the output is non-decreasing in the threshold. To improve the accuracy for large datasets, we propose to partition the dataset into multiple disjoint subsets and build a local model on each of them. We perform experiments on real datasets and show that the proposed model consistently outperforms state-of-the-art models in accuracy in an efficient way and is useful for real applications.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452772",
        "category": "Databases"
    },
    {
        "title": "HoneyGen: Generating Honeywords Using Representation Learning",
        "authors": "['Antreas Dionysiou', 'Vassilis Vassiliades', 'Elias Athanasopoulos']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Honeywords are false passwords injected in a database for detecting password leakage. Generating honeywords is a challenging problem due to the various assumptions about the adversary's knowledge as well as users' password-selection behaviour. The success of a Honeywords Generation Technique (HGT) lies on the resulting honeywords; the method fails if an adversary can easily distinguish the real password. In this paper, we propose HoneyGen, a practical and highly robust HGT that produces realistic looking honeywords. We do this by leveraging representation learning techniques to learn useful and explanatory representations from a massive collection of unstructured data, i.e., each operator's password database. We perform both a quantitative and qualitative evaluation of our framework using the state-of-the-art metrics. Our results suggest that HoneyGen generates high-quality honeywords that cause sophisticated attackers to achieve low distinguishing success rates.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453092",
        "category": "Databases"
    },
    {
        "title": "Scalable Mining of High-Utility Sequential Patterns With Three-Tier MapReduce Model",
        "authors": "['Jerry Chun-Wei Lin', 'Youcef Djenouri', 'Gautam Srivastava', 'Yuanfa Li', 'Philip S. Yu']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "High-utility sequential pattern mining (HUSPM) is a hot research topic in recent decades since it combines both sequential and utility properties to reveal more information and knowledge rather than the traditional frequent itemset mining or sequential pattern mining. Several works of HUSPM have been presented but most of them are based on main memory to speed up mining performance. However, this assumption is not realistic and not suitable in large-scale environments since in real industry, the size of the collected data is very huge and it is impossible to fit the data into the main memory of a single machine. In this article, we first develop a parallel and distributed three-stage MapReduce model for mining high-utility sequential patterns based on large-scale databases. Two properties are then developed to hold the correctness and completeness of the discovered patterns in the developed framework. In addition, two data structures called sidset and utility-linked list are utilized in the developed framework to accelerate the computation for mining the required patterns. From the results, we can observe that the designed model has good performance in large-scale datasets in terms of runtime, memory, efficiency of the number of distributed nodes, and scalability compared to the serial HUSP-Span approach.",
        "link": "https://dl.acm.org/doi/10.1145/3487046",
        "category": "Databases"
    },
    {
        "title": "Advanced Domain-Driven Design for Consistency in Distributed Data-Intensive Systems",
        "authors": "['Susanne Braun', 'Annette Bieniusa', 'Frank Elberzhager']",
        "date": "April 2021",
        "source": "PaPoC '21: Proceedings of the 8th Workshop on Principles and Practice of Consistency for Distributed Data",
        "abstract": "More and more data-intensive systems have emerged lately. Big Data, Artificial Intelligence, or cloud-native applications all require high scalability and availability. Data is no longer persisted in one central relational database with serialized and transactional access, but rather distributed and replicated among different nodes running only under eventual consistency. This poses a number of design challenges for software architects, as they cannot rely on a single system to mask the concurrency anomalies of concurrent access to distributed and replicated data. Based on three case studies, we developed a theory regarding how practitioners handle synchronization and consistency design challenges in distributed data-intensive applications. We also identified the \"white spots\" of missing design guidance needed by practitioners to handle the aforementioned challenges appropriately. We are currently evaluating our theory in the context of an action research study. In this study, we are also evaluating the novel design guidelines we are proposing in this regard, which, according to our theory, meet the needs of practitioners. Our design guidelines integrate with Domain-Driven Design, which is widely used in practice. Following the idea of multilevel serializability, we investigate the compatibility of business operations beyond commutativity. We provide concrete practical design guidance to achieve compatibility of non-commutative business operations. We also describe the basic infrastructure guarantees our design guidelines require from replication frameworks.",
        "link": "https://dl.acm.org/doi/10.1145/3447865.3457969",
        "category": "Databases"
    },
    {
        "title": "Secure DNA Motif-Finding Method Based on Sampling Candidate Pruning",
        "authors": "['Kaijian Xia', 'Xiang Wu', 'Yaqing Mao', 'Huanhuan Wang']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "With the continuous exploration of genetic research, gradually exposed privacy issues become the bottleneck that limits its development. DNA motif finding is an important study to understand the regulation of gene expression; however, the existing methods generally ignore the potential sensitive information that may be exposed in the process. In this work, we utilize the \\(\\)-differential privacy model to provide provable privacy guarantees which is independent of attackers’ background knowledge. Our method makes use of sample databases to prune the generated candidate motifs to lower the magnitude of added noise. Furthermore, to improve the utility of mining results, a strategy of threshold modification is designed to reduce the propagation and random sampling errors in the mining process. Extensive experiments on actual DNA databases confirm that our approach can privately find DNA motifs with high utility and efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3382078",
        "category": "Databases"
    },
    {
        "title": "Tractability Beyond ß-Acyclicity for Conjunctive Queries with Negation",
        "authors": "['Matthias Lanzinger']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Numerous fundamental database and reasoning problems are known to be NP-hard in general but tractable on instances where the underlying hypergraph structure is β-acyclic. Despite the importance of many of these problems, there has been little success in generalizing these results beyond acyclicity. In this paper, we take on this challenge and propose nest-set width, a novel generalization of hypergraph β-acyclicity. We demonstrate that nest-set width has desirable properties and algorithmic significance. In particular, evaluation of boolean conjunctive queries with negation is tractable for classes with bounded nest-set width. Furthermore, propositional satisfiability is fixed-parameter tractable when parameterized by nest-set width.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458308",
        "category": "Databases"
    },
    {
        "title": "Index-Accelerated Pattern Matching in Event Stores",
        "authors": "['Michael Körber', 'Nikolaus Glombiewski', 'Bernhard Seeger']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "IoT applications require a new type of database systems termed event stores for ingesting fast arriving event streams and efficiently supporting analytical ad-hoc queries over time. One of the most important operations in this regard is sequential pattern matching also known as Match\\_Recognize, which matches user defined predicates to subsequences of events. While Match\\_Recognize is well known in the field of event processing, it has only recently become part of the SQL standard. Despite of that, Match\\_Recognize has received little attention in the database area so far. We present a novel approach to speed up an important class of Match\\_Recognize queries on event stores by utilizing off-the-shelf secondary indexes on non-temporal attributes (e.g., B$^+$-trees, LSM-trees) and a cost model for selecting the most appropriate indexes. Our approach keeps temporal and sequential information in secondary indexes to prune large parts of the stream from further processing. However, simply using as many secondary indexes as available is not the right choice because the access cost for the index scans can exceed the processing time of the naï ve approach that scans the entire stream and replays it into an event processing system. In order to address this problem, we present a first cost model to estimate the total execution cost of a Match\\_Recognize query for a set of available indexes. Based on this cost model, we devise an efficient index selection strategy that avoids a full enumeration of index configurations. Prototypical implementations of our approach are available in our open-source research prototype, a commercial database system, and Apache Flink. In experiments with synthetic and real-world data sets, all our index-based implementations clearly outperform the naï ve replay strategy that is currently offered in commercial database systems and Flink.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457245",
        "category": "Databases"
    },
    {
        "title": "Gender Classification Using Discrete Cosine Transform and multi classifiers",
        "authors": "['Ezz Omar Abdalali', 'Wafa Ibrahim eltrhoni', 'Salma Ibrahim almajbri', 'Amina Atia Abdo']",
        "date": "October 2021",
        "source": "ICEMIS'21: The 7th International Conference on Engineering &amp; MIS 2021",
        "abstract": "Abstract- Gender detection is one of the fundamental tasks of facial analysis. Most of the presented research focuses on facial images obtained under controlled conditions. Nevertheless, real-world applications necessitate gender categorization of real faces, which is much more difficult due to the large variations in appearance in unlimited scenarios. In this paper, we proposed discrete cosine transform (DCT) to represent the face features, and DCT has the advantage of highly efficient computational algorithms. In designing the proposed gender detection system, linear discrimination analysis (LDA), random forest (RF), and support vector machine (SVM) classifiers are used for the classification process. Experiments were conducted using the challenging ORL, FEI, and JAFFE databases to ascertain the usefulness of the approaches. The proposed approach has a significant improvement in the classification rate compared to other methods used which obtained 100% in terms of accuracy in the (FEI+JAFFE) and (ORL+JAFFE) databases.",
        "link": "https://dl.acm.org/doi/10.1145/3492547.3492633",
        "category": "Databases"
    },
    {
        "title": "Weighted Gaussian Loss based Hamming Hashing",
        "authors": "['Rong-Cheng Tu', 'Xian-Ling Mao', 'Cihang Kong', 'Zihang Shao', 'Ze-Lin Li', 'Wei Wei', 'Heyan Huang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Recently, deep Hamming hashing methods have been proposed for Hamming space retrieval which enables constant-time search by hash table lookups instead of linear scan. When carrying out Hamming space retrieval, for each query datapoint, there is a Hamming ball centered on the query datapoint, and only the datapoints within the Hamming ball are returned as the relevant ones, while those beyond are discarded directly. Thus, to further enhance the retrieval performance, it is a key point for the Hamming hashing methods to decrease the dissimilar datapoints within the Hamming ball. However, nearly all existing Hamming hashing methods cannot effectively penalize the dissimilar pairs within the Hamming ball to push them out. To tackle this problem, in this paper, we propose a novel Weighted Gaussian Loss based Hamming Hashing, called WGLHH, which introduces a weighted Gaussian loss to optimize hashing model. Specifically, the weighted Gaussian loss consists of three parts: a novel Gaussian-distribution based loss, a novel badly-trained-pair attention mechanism and a quantization loss. The Gaussian-distribution based loss is proposed to effectively penalize the dissimilar pairs within the Hamming ball. The badly-trained-pair attention mechanism is proposed to assign a weight for each data pair, which puts more weight on data pairs whose corresponding hash codes cannot preserve original similarity well, and less on those having already handled well. The quantization loss is used to reduce the quantization error. By incorporating the three parts, the proposed weighted Gaussian loss will penalize significantly on the dissimilar pairs within the Hamming ball to generate more compact hashing codes. Extensive experiments on two benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in image retrieval task.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475498",
        "category": "Databases"
    },
    {
        "title": "Preference queries over taxonomic domains",
        "authors": "['Paolo Ciaccia', 'Davide Martinenghi', 'Riccardo Torlone']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "When composing multiple preferences characterizing the most suitable results for a user, several issues may arise. Indeed, preferences can be partially contradictory, suffer from a mismatch with the level of detail of the actual data, and even lack natural properties such as transitivity. In this paper we formally investigate the problem of retrieving the best results complying with multiple preferences expressed in a logic-based language. Data are stored in relational tables with taxonomic domains, which allow the specification of preferences also over values that are more generic than those in the database. In this framework, we introduce two operators that rewrite preferences for enforcing the important properties of transitivity, which guarantees soundness of the result, and specificity, which solves all conflicts among preferences. Although, as we show, these two properties cannot be fully achieved together, we use our operators to identify the only two alternatives that ensure transitivity and minimize the residual conflicts. Building on this finding, we devise a technique, based on an original heuristics, for selecting the best results according to the two possible alternatives. We finally show, with a number of experiments over both synthetic and real-world datasets, the effectiveness and practical feasibility of the overall approach.",
        "link": "https://dl.acm.org/doi/10.14778/3467861.3467874",
        "category": "Databases"
    },
    {
        "title": "SQL-like query language and referential constraints on tree-structured data",
        "authors": "['Foto N Afrati', 'Matthew George Damigos', 'Nikos Stasinopoulos']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "In this paper we investigate within-record referential constraints on tree-structured data. We consider an SQL-like query language such that the one used in Dremel and we call it tree-SQL. We show how to define and process a query in tree-SQL in the presence of referential constraints. We give the semantics of tree-SQL via flattening and show how to produce equivalent semantics using the notion of tree-expansion of a query in the presence of referential constraints.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472184",
        "category": "Databases"
    },
    {
        "title": "Personality and Internet Use: A Meta-Analysis",
        "authors": "['Huang Lu', 'Wu Na', 'Zeng Wenfa']",
        "date": "December 2021",
        "source": "EBIMCS '21: Proceedings of the 2021 4th International Conference on E-Business, Information Management and Computer Science",
        "abstract": "This meta-analysis examined the relationship between varieties of Internet use and personality based on five-factor model. A search of the following databases was conducted for identifying relevant English articles published prior to the end of 2015: PsycINFO, PsycARTICLES, Psychology and Behavioral Science Collection, ISI Web of Science databases, ProQuest Digital Dissertations, and Elsevier Science. A cumulative total sample size ranged to over 4,000, however, the final 40 studies with 295 effects are examined in this article after confirming to be relevant. The result revealed that agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness had differed in participants’ Internet behaviors for Information seeking, Leisure activities and the general, but surprisingly they did not work moderately in social activities online. The effect of publication year and participants’ age were partial supported as the moderators.",
        "link": "https://dl.acm.org/doi/10.1145/3511716.3511759",
        "category": "Databases"
    },
    {
        "title": "A Multi-Threshold Ant Colony System-based Sanitization Model in Shared Medical Environments",
        "authors": "['Jimmy Ming-Tai Wu', 'Gautam Srivastava', 'Jerry Chun-Wei Lin', 'Qian Teng']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "During the past several years, revealing some useful knowledge or protecting individual’s private information in an identifiable health dataset (i.e., within an Electronic Health Record) has become a tradeoff issue. Especially in this era of a global pandemic, security and privacy are often overlooked in lieu of usability. Privacy preserving data mining (PPDM) is definitely going to be have an important role to resolve this problem. Nevertheless, the scenario of mining information in an identifiable health dataset holds high complexity compared to traditional PPDM problems. Leaking individual private information in an identifiable health dataset has becomes a serious legal issue. In this article, the proposed Ant Colony System to Data Mining algorithm takes the multi-threshold constraint to secure and sanitize patents’ records in different lengths, which is applicable in a real medical situation. The experimental results show the proposed algorithm not only has the ability to hide all sensitive information but also to keep useful knowledge for mining usage in the sanitized database.",
        "link": "https://dl.acm.org/doi/10.1145/3408296",
        "category": "Databases"
    },
    {
        "title": "Deep Siamese Metric Learning: A Highly Scalable Approach to Searching Unordered Sets of Trajectories",
        "authors": "['Christoffer Löffler', 'Luca Reeb', 'Daniel Dzibela', 'Robert Marzilger', 'Nicolas Witt', 'Björn M. Eskofier', 'Christopher Mutschler']",
        "date": "None",
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "This work proposes metric learning for fast similarity-based scene retrieval of unstructured ensembles of trajectory data from large databases. We present a novel representation learning approach using Siamese Metric Learning that approximates a distance preserving low-dimensional representation and that learns to estimate reasonable solutions to the assignment problem. To this end, we employ a Temporal Convolutional Network architecture that we extend with a gating mechanism to enable learning from sparse data, leading to solutions to the assignment problem exhibiting varying degrees of sparsity.Our experimental results on professional soccer tracking data provides insights on learned features and embeddings, as well as on generalization, sensitivity, and network architectural considerations. Our low approximation errors for learned representations and the interactive performance with retrieval times several magnitudes smaller shows that we outperform previous state of the art.",
        "link": "https://dl.acm.org/doi/10.1145/3465057",
        "category": "Databases"
    },
    {
        "title": "TSCache: an efficient flash-based caching scheme for time-series data workloads",
        "authors": "['Jian Liu', 'Kefei Wang', 'Feng Chen']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Time-series databases are becoming an indispensable component in today's data centers. In order to manage the rapidly growing time-series data, we need an effective and efficient system solution to handle the huge traffic of time-series data queries. A promising solution is to deploy a high-speed, large-capacity cache system to relieve the burden on the backend time-series databases and accelerate query processing. However, time-series data is drastically different from other traditional data workloads, bringing both challenges and opportunities. In this paper, we present a flash-based cache system design for time-series data, called TSCache. By exploiting the unique properties of time-series data, we have developed a set of optimization schemes, such as a slab-based data management, a two-layered data indexing structure, an adaptive time-aware caching policy, and a low-cost compaction process. We have implemented a prototype based on Twitter's Fatcache. Our experimental results show that TSCache can significantly improve client query performance, effectively increasing the bandwidth by a factor of up to 6.7 and reducing the latency by up to 84.2%.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484225",
        "category": "Databases"
    },
    {
        "title": "Are we ready for learned cardinality estimation?",
        "authors": "['Xiaoying Wang', 'Changbo Qu', 'Weiyuan Wu', 'Jiannan Wang', 'Qingqing Zhou']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Cardinality estimation is a fundamental but long unresolved problem in query optimization. Recently, multiple papers from different research groups consistently report that learned models have the potential to replace existing cardinality estimators. In this paper, we ask a forward-thinking question: Are we ready to deploy these learned cardinality models in production? Our study consists of three main parts. Firstly, we focus on the static environment (i.e., no data updates) and compare five new learned methods with nine traditional methods on four real-world datasets under a unified workload setting. The results show that learned models are indeed more accurate than traditional methods, but they often suffer from high training and inference costs. Secondly, we explore whether these learned models are ready for dynamic environments (i.e., frequent data updates). We find that they cannot catch up with fast data updates and return large errors for different reasons. For less frequent updates, they can perform better but there is no clear winner among themselves. Thirdly, we take a deeper look into learned models and explore when they may go wrong. Our results show that the performance of learned methods can be greatly affected by the changes in correlation, skewness, or domain size. More importantly, their behaviors are much harder to interpret and often unpredictable. Based on these findings, we identify two promising research directions (control the cost of learned models and make learned models trustworthy) and suggest a number of research opportunities. We hope that our study can guide researchers and practitioners to work together to eventually push learned cardinality estimators into real database systems.",
        "link": "https://dl.acm.org/doi/10.14778/3461535.3461552",
        "category": "Databases"
    },
    {
        "title": "MT-teql: evaluating and augmenting neural NLIDB on real-world linguistic and schema variations",
        "authors": "['Pingchuan Ma', 'Shuai Wang']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Natural Language Interface to Database (NLIDB) translates human utterances into SQL queries and enables database interactions for non-expert users. Recently, neural network models have become a major approach to implementing NLIDB. However, neural NLIDB faces challenges due to variations in natural language and database schema design. For instance, one user intent or database conceptual model can be expressed in various forms. However, existing benchmarks, using hold-out datasets, cannot provide thorough understanding of how good neural NLIDBs really are in real-world situations and its robustness against such variations. A key difficulty is to annotate SQL queries for inputs under real-world variations, requiring considerable manual effort and expert knowledge.To systematically assess the robustness of neural NLIDBs without extensive manual effort, we propose MT-Teql, a unified framework to benchmark NLIDBs against real-world language and schema variations. Inspired by recent advances in DBMS metamorphic testing, MT-Teql implements semantics-preserving transformations on utterances and database schemas to generate their variants. NLIDBs can thus be examined for robustness utilizing utterances/schemas and their variants without requiring manual intervention.We benchmarked nine neural NLIDBs using 62,430 inputs and identified 15,433 defects. We analyzed potential root causes of defects and conducted a user study to show how MT-Teql can assist developers to systematically assess NLIDBs. We further show that the transformed (error-triggering) inputs can be used to augment popular NLIDBs and eliminate 46.5%(±5.0%) errors made by them without compromising their accuracy on standard benchmarks. We summarize lessons from this study that can provide insights to select and design NLIDBs that fit particular usage scenarios.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494139",
        "category": "Databases"
    },
    {
        "title": "Load Balanced Semantic Aware Distributed RDF Graph",
        "authors": "['Ami Pandat', 'Nidhi Gupta', 'Minal Bhise']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Modern day application development requires efficient management of huge RDF data. The major approaches for RDF data management are Relational and Graph based techniques. As the relational approach suffers from query joins, we propose a semantic aware graph based partitioning method. The partitioned fragments are further allocated in a load balanced way. For efficient query processing, partial replication is implemented. It reduces Inter node Communication thereby accelerating queries on distributed RDF Graph. This approach has been demonstrated in two phases partitioning and Distribution of Linked Observation Data (LOD). The time complexity for partitioning and distribution of Load Balanced Semantic Aware RDF Graph (LBSD) is O(n) where n is the number of triples which is demonstrated by linear increment in algorithm execution time (AET) for LOD data scaled from 1x to 5x. LBSD has been found to behave well till 4x. LBSD is compared with the state of the art relational and graph-based partitioning techniques. LBSD records 71% QET gain when averaged over all the four query types. For most frequent query types, Linear and Star, on an average 65% QET gain is recorded over original configuration for scaling experiments. The optimal replication level has been found to be 12% of original data.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472167",
        "category": "Databases"
    },
    {
        "title": "MAYUR: Map conflAtion using earlY prUning and Rank join",
        "authors": "['Gorisha Agarwal', 'Laks V.S. Lakshmanan', 'Xiaoming Gao', 'Kevin Ventullo', 'Saurav Mohapatra', 'Saikat Basu']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "OpenStreetMap (OSM) is a collaborative good quality crowd-sourced geospatial database (GDB). The quality of OSM is generally very good, it lacks good coverage in many parts of the world. A natural approach for extending its coverage is to conflate missing spatial features from other GDBs into OSM, but this is laborious and time-consuming. We propose a system MAYUR solving road network conflation between two vector GDBs, representing the GDBs as a graph of road intersections (vertices) and road segments (edges). MAYUR is based on a novel map matching framework that adapts the classic Rank Join in databases, where each edge of the reference GDB is modeled as a relation. Our algorithm finds the best matching between a reference and target GDB, respecting the connectivity of the road network. While classic Rank Join in databases gets quickly inefficient on instances with more than 10 relations, MAYUR's enhanced Rank Join incorporates three optimizations that boost the algorithm's efficiency, making it scale to our problem setting featuring hundreds to thousands of relations. Our manual evaluation of MAYUR conflation results on sidewalks in OSM and Boston Open Data shows an impressive 98.65% precision and 99.55% recall.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3484258",
        "category": "Databases"
    },
    {
        "title": "Two-pronged Strategy: Lightweight Augmented Graph Network Hashing for Scalable Image Retrieval",
        "authors": "['Hui Cui', 'Lei Zhu', 'Jingjing Li', 'Zhiyong Cheng', 'Zheng Zhang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Hashing learns compact binary codes to store and retrieve massive data efficiently. Particularly, unsupervised deep hashing is supported by powerful deep neural networks and has the desirable advantage of label independence. It is a promising technique for scalable image retrieval. However, deep models introduce a large number of parameters, which is hard to optimize due to the lack of explicit semantic labels and brings considerable training cost. As a result, the retrieval accuracy and training efficiency of existing unsupervised deep hashing are still limited. To tackle the problems, in this paper, we propose a simple and efficient Lightweight Augmented Graph Network Hashing (LAGNH) method with a two-pronged strategy. For one thing, we extract the inner structure of the image as the auxiliary semantics to enhance the semantic supervision of the unsupervised hash learning process. For another, we design a lightweight network structure with the assistance of the auxiliary semantics, which greatly reduces the number of network parameters that needs to be optimized and thus greatly accelerates the training process. Specifically, we design a cross-modal attention module based on the auxiliary semantic information to adaptively mitigate the adverse effects in the deep image features. Besides, the hash codes are learned by multi-layer message passing within an adversarial regularized graph convolutional network. Simultaneously, the semantic representation capability of hash codes is further enhanced by reconstructing the similarity graph. Experimental results show that our method achieves significant performance improvement compared with the state-of-the-art unsupervised deep hashing methods in terms of both retrieval accuracy and efficiency. Notably, on MS-COCO dataset, our method achieves more than 10% improvement on retrieval precision and 2.7x speedup on training time compared with the second best result.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475605",
        "category": "Databases"
    },
    {
        "title": "Tractable Orders for Direct Access to Ranked Answers of Conjunctive Queries",
        "authors": "['Nofar Carmeli', 'Nikolaos Tziavelis', 'Wolfgang Gatterbauer', 'Benny Kimelfeld', 'Mirek Riedewald']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "We study the question of when we can provide logarithmic-time direct access to the k-th answer to a Conjunctive Query (CQ) with a specified ordering over the answers, following a preprocessing step that constructs a data structure in time quasilinear in the size of the database. Specifically, we embark on the challenge of identifying the tractable answer orderings that allow for ranked direct access with such complexity guarantees. We begin with lexicographic orderings and give a decidable characterization (under conventional complexity assumptions) of the class of tractable lexicographic orderings for every CQ without self-joins. We then continue to the more general orderings by the sum of attribute weights and show for it that ranked direct access is tractable only in trivial cases. Hence, to better understand the computational challenge at hand, we consider the more modest task of providing access to only a single answer (i.e., finding the answer at a given position) - a task that we refer to as the selection problem. We indeed achieve a quasilinear-time algorithm for a subset of the class of full CQs without self-joins, by adopting a solution of Frederickson and Johnson to the classic problem of selection over sorted matrices. We further prove that none of the other queries in this class admit such an algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458331",
        "category": "Databases"
    },
    {
        "title": "JSON Tiles: Fast Analytics on Semi-Structured Data",
        "authors": "['Dominik Durner', 'Viktor Leis', 'Thomas Neumann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Developers often prefer flexibility over upfront schema design, making semi-structured data formats such as JSON increasingly popular. Large amounts of JSON data are therefore stored and analyzed by relational database systems. In existing systems, however, JSON's lack of a fixed schema results in slow analytics. In this paper, we present JSON tiles, which, without losing the flexibility of JSON, enables relational systems to perform analytics on JSON data at native speed. JSON tiles automatically detects the most important keys and extracts them transparently - often achieving scan performance similar to columnar storage. At the same time, JSON tiles is capable of handling heterogeneous and changing data. Furthermore, we automatically collect statistics that enable the query optimizer to find good execution plans. Our experimental evaluation compares against state-of-the-art systems and research proposals and shows that our approach is both robust and efficient.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452809",
        "category": "Databases"
    },
    {
        "title": "Building Fast and Compact Sketches for Approximately Multi-Set Multi-Membership Querying",
        "authors": "['Rundong Li', 'Pinghui Wang', 'Jiongli Zhu', 'Junzhou Zhao', 'Jia Di', 'Xiaofei Yang', 'Kai Ye']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Given a set S, Membership Querying (MQ) answers whether a query element $q\\in S$. It is a fundamental task in areas like database systems and computer networks. In this paper, we consider a more general problem, Multi-Set Multi-Membership Querying (MS-MMQ). Given n sets $S_0,łdots,S_n-1 $, MS-MMQ answers which sets contain element q. A direct way to address MS-MMQ is to build an MQ structure (e.g., Bloom Filter) for each set. However, the query and space complexities grow linearly with n and become prohibitive for a large n. To address this challenge, we propose a novel Circular Shift and Coalesce (CSC) framework to efficiently achieve approximate MS-MMQ. Instead of building an MQ data structure for each set, the CSC index encodes all n sets into a compact sketch and retrieves only a few bytes in the sketch for a query, which achieves high memory-efficiency and boosts the query speed by several times. CSC is compatible with mainstream data structures for Approximate MQ. We conduct experiments on real-world datasets and results demonstrate that our framework is up to 91.2 times faster and up to 48.9 times more accurate than state-of-the-art methods.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452829",
        "category": "Databases"
    },
    {
        "title": "Personnel Information Management System Design for Universities with Enhanced Affair Efficiency under Intelligent Information Technologies",
        "authors": "['Huanhuan Mao', 'Qiuyan Zhang']",
        "date": "March 2022",
        "source": "ICEDS '22: Proceedings of the 2022 3rd International Conference on Education Development and Studies",
        "abstract": "The quality and efficiency of office affair management in the colleges and universities are the key elements for improving the daily personnel administration works, as well as many other related personnel management. This research introduces one latest designed personnel information management system for universities and its application results after it has been launched for two years. Intelligent information technologies were included in this personnel information management system to conduct the structure design of personnel database, including database management, personnel management, assessment management and statistic query. Such self-designed personnel information management system was then launched in September 2018, and being gradually updated according to the feedbacks from the users. The three years application shows that the efficiency in office affair management has been highly improved, with more than 40% shortened average personalized service time. It satisfied the daily needs of university employee management quite well with reduced data errors due to the self and timely updated database.",
        "link": "https://dl.acm.org/doi/10.1145/3528137.3528153",
        "category": "Databases"
    },
    {
        "title": "Robustness against read committed for transaction templates",
        "authors": "['Brecht Vandevoort', 'Bas Ketsman', 'Christoph Koch', 'Frank Neven']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The isolation level Multiversion Read Committed (RC), offered by many database systems, is known to trade consistency for increased transaction throughput. Sometimes, transaction workloads can be safely executed under RC obtaining the perfect isolation of serializability at the lower cost of RC. To identify such cases, we introduce an expressive model of transaction programs to better reason about the serializability of transactional workloads. We develop tractable algorithms to decide whether any possible schedule of a workload executed under RC is serializable (referred to as the robustness problem). Our approach yields robust subsets that are larger than those identified by previous methods. We provide experimental evidence that workloads that are robust against RC can be evaluated faster under RC compared to stronger isolation levels. We discuss techniques for making workloads robust against RC by promoting selective read operations to updates. Depending on the scenario, the performance improvements can be considerable. Robustness testing and safely executing transactions under the lower isolation level RC can therefore provide a direct way to increase transaction throughput without changing DBMS internals.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476268",
        "category": "Databases"
    },
    {
        "title": "Maximizing Persistent Memory Bandwidth Utilization for OLAP Workloads",
        "authors": "['Björn Daase', 'Lars Jonas Bollmeier', 'Lawrence Benson', 'Tilmann Rabl']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Modern database systems for online analytical processing (OLAP) typically rely on in-memory processing. Keeping all active data in DRAM severely limits the data capacity and makes larger deployments much more expensive than disk-based alternatives. Byte-addressable persistent memory (PMEM) is an emerging storage technology that bridges the gap between slow-but-cheap SSDs and fast-but-expensive DRAM. Thus, research and industry have identified it as a promising alternative to pure in-memory data warehouses. However, recent work shows that PMEM's performance is strongly dependent on access patterns and does not always yield good results when simply treated like DRAM. To characterize PMEM's behavior in OLAP workloads, we systematically evaluate PMEM on a large, multi-socket server commonly used for OLAP workloads. Our evaluation shows that PMEM can be treated like DRAM for most read access but must be used differently when writing. To support our findings, we run the Star Schema Benchmark on PMEM and DRAM. We show that PMEM is suitable for large, read-heavy OLAP workloads with an average query runtime slowdown of 1.66x compared to DRAM. Following our evaluation, we present 7 best practices on how to maximize PMEM's bandwidth utilization in future system designs.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457292",
        "category": "Databases"
    },
    {
        "title": "A Comprehensive Empirical Study of Query Performance Across GPU DBMSes",
        "authors": "['Young-Kyoon Suh', 'Junyoung An', 'Byungchul Tak', 'Gap-Joo Na']",
        "date": "None",
        "source": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "abstract": "In recent years, GPU database management systems (DBMSes) have rapidly become popular largely due to their remarkable acceleration capability obtained through extreme parallelism in query evaluations. However, there has been relatively little study on the characteristics of these GPU DBMSes for a better understanding of their query performance in various contexts. Also, little has been known about what the potential factors could be that affect the query processing jobs within the GPU DBMSes. To fill this gap, we have conducted a study to identify such factors and to propose a structural causal model, including key factors and their relationships, to explicate the variances of the query execution times on the GPU DBMSes. We have also established a set of hypotheses drawn from the model that explained the performance characteristics. To test the model, we have designed and run comprehensive experiments and conducted in-depth statistical analyses on the obtained empirical data. As a result, our model achieves about 77% amount of variance explained on the query time and indicates that reducing kernel time and data transfer time are the key factors to improve the query time. Also, our results show that the studied systems should resolve several concerns such as bounded processing within GPU memory, lack of rich query evaluation operators, limited scalability, and GPU under-utilization.",
        "link": "https://dl.acm.org/doi/10.1145/3508024",
        "category": "Databases"
    },
    {
        "title": "BAASH: lightweight, efficient, and reliable blockchain-as-a-service for HPC systems",
        "authors": "['Abdullah Al Mamun', 'Feng Yan', 'Dongfang Zhao']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "Distributed resiliency becomes paramount to alleviate the growing costs of data movement and I/Os while preserving the data accuracy in HPC systems. This paper proposes to adopt blockchain-like decentralized protocols to achieve such distributed resiliency. The key challenge for such an adoption lies in the mismatch between blockchain's targeting systems (e.g., shared-nothing, loosely-coupled, TCP/IP stack) and HPC's unique design on storage subsystems, resource allocation, and programming models. We present BAASH, Blockchain-As-A-Service for HPC, deployable in a plug-n-play fashion. BAASH bridges the HPC-blockchain gap with two key components: (i) Lightweight consensus protocols for the HPC's shared-storage architecture, (ii) A new fault-tolerant mechanism compensating for the MPI to guarantee the distributed resiliency. We have implemented a prototype system and evaluated it with more than two million transactions on a 500-core HPC cluster. Results show that the prototype of the proposed techniques significantly outperforms vanilla blockchain systems and exhibits strong reliability with MPI.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476155",
        "category": "Databases"
    },
    {
        "title": "On m-Impact Regions and Standing Top-k Influence Problems",
        "authors": "['Bo Tang', 'Kyriakos Mouratidis', 'Mingji Han']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In this paper, we study the m-impact region problem (mIR). In a context where users look for available products with top-k queries, mIR identifies the part of the product space that attracts the most user attention. Specifically, mIR determines the kind of attribute values that lead a (new or existing) product to the top-k result for at least a fraction of the user population. mIR has several applications, ranging from effective marketing to product improvement. Importantly, it also leads to (exact and efficient) solutions for standing top-k impact problems, which were previously solved heuristically only, or whose current solutions face serious scalability limitations. We experiment, among others, on data mined from actual user reviews for real products, and demonstrate the practicality and efficiency of our algorithms, both for mIR and for standing top-k impact problems.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452832",
        "category": "Databases"
    },
    {
        "title": "A DDoS Attack Detection Method Based on LSTM Neural Network in The Internet of Vehicles",
        "authors": "['Yuexin Zhang', 'Yiyang LIU', 'Yiying Zhang', 'Longzhe Han', 'Jia Zhao', 'Yannian Wu']",
        "date": "October 2021",
        "source": "ICITEE '21: Proceedings of the 4th International Conference on Information Technologies and Electrical Engineering",
        "abstract": "With the continuous development and application of 5G wireless communication technology and smart car technology, the Internet of Vehicles has attracted more and more attention. Due to the application requirements of high bandwidth and low latency in the Internet of Vehicles, Mobile Edge Computing (MEC) is introduced into the in-vehicle network. However, when mobile edge computing nodes are deployed near the edge of the Internet of Vehicles to bring users a good experience, more and more attacks on the Internet of Vehicles will follow. Distributed denial of service attacks are violent and direct, but they cause huge damage to the car networking system. Aiming at the problem that the existing DDoS attack detection methods are not suitable for the Internet of Vehicles environment and have hysteresis problems. This paper proposes a detection method based on LSTM neural network in the mobile edge computing environment of the Internet of Vehicles. This method can detect the current network traffic according to the LSTM prediction model, compare the current data with the prediction data of the LSTM detection model, and judge whether the car networking system is under DDOS attack according to the threshold value; in addition, the method can also continuously learn historical information and build local knowledge The database is continuously detected according to the attack signature database of the knowledge base.",
        "link": "https://dl.acm.org/doi/10.1145/3513142.3513204",
        "category": "Databases"
    },
    {
        "title": "System for assessing the efficiency of operating renewable energy sources at an industrial enterprise in the context of the economy digitalization",
        "authors": "['Irina Usacheva']",
        "date": "March 2021",
        "source": "DEFIN-2021: IV International Scientific and Practical Conference",
        "abstract": "The article presents the results of research work on the development of a system for assessing the efficiency of operating renewable energy sources (RES) at an industrial enterprise in the context of economy digitalization, in particular, in the transition to the smart manufacturing concept. The study builds the structure of the database (DB) of the system for assessing the efficiency of RES operation at an industrial enterprise and develops the architecture and user interface of the system. The authors describe advantages and disadvantages. The developed DB allows assessing options for involving RES in the energy balance of an industrial enterprise through clear values and a decision-making process understandable to a wide range of groups and individuals. The database enables the assessment and calculation of the power supply system parameters for a specific area of an industrial enterprise, both in normal and emergency modes, considering different types of loads: active and reactive, low and high voltage, low and high power.",
        "link": "https://dl.acm.org/doi/10.1145/3487757.3490768",
        "category": "Databases"
    },
    {
        "title": "Knowledge MOOCs Publication in Indonesia - A research Positioning through Bibliometric Analysis",
        "authors": "['Fairuz Iqbal Maulana', 'Gamal Zamahsari', 'Ida Ananta Wijaya', 'Dwi Riza Budi Raharja', 'Vandha Widartha']",
        "date": "September 2021",
        "source": "SIET '21: Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology",
        "abstract": "Massive Open Online Courses or we usually name it with MOOCs already have attracted many researchers as a new approach to the application of technology in teaching and learning activities. This is an innovation in the development of e-learning that has grown rapidly in research over the last few years. This research is the brainchild of the publication of Massive Open Online Courses (MOOCs) which demonstrate their widespread use based on data from publications in Indonesia indexed by Scopus. This study visually maps scientific publications in digital techniques indexed by bibliometric analysis at the state level of Indonesia. This study collected data from the Scopus database, used the Scopus website analysis feature, and visualized the bibliometric network using Vosviewer. The method used consists of five stages, namely terming the search keywords, initial search results, narrowing the results of the preparation, initial preparation, and data analysis. 41 academic documents published in Indonesia in the last 6 years from 2014 to 2020 were obtained from searching data based on the Scopus database. The processed data shows the pattern and trend of publication of the number of international publications in MOOCs indexed by Scopus.",
        "link": "https://dl.acm.org/doi/10.1145/3479645.3479651",
        "category": "Databases"
    },
    {
        "title": "Designing a Business View of Enterprise Data: An approach based on a Decentralised Enterprise Knowledge Graph",
        "authors": "['Bastien Vidé', 'Joan Marty', 'Franck Ravat', 'Max Chevalier']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Nowadays, companies manage a large volume of data usually organised in ”silos”. Each ”data silo” contains data related to a specific Business Unit, or a project. This scattering of data does not facilitate decision-making requiring the use and cross-checking of data coming from different silos. So, a challenge remains: the construction of a Business View of all data in a company. In this paper, we introduce the concepts of Enterprise Knowledge Graph (EKG) and Decentralised EKG (DEKG). Our DEKG aims at generating a Business View corresponding to a synthetic view of data sources. We first define and model a DEKG with an original process to generate a Business View before presenting the possible implementation of a DEKG.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472276",
        "category": "Databases"
    },
    {
        "title": "Analysis and Comparison of Binary and Interpolation Search Algorithms in a B-tree",
        "authors": "['Sotirios Salakos', 'Nikolaos Ploskas']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "The consecutive increasing management of large amounts of information in various fields requires the development and construction of efficient database management systems. These systems implement storage, organization, and management of this data in order to perform multiple operations to solve complex and difficult problems. In order to achieve the aforementioned needs, indexing systems have been developed in the context of relational database management systems (RDBMSs). RDBMSs are composed of multiple mechanisms comprising a set of algorithms and data structures. This paper analyzes the performance of the B-tree data structure which laid the foundation as a structural and functional basis for the development of a whole category of tree indexing structures. B-tree indexes have applications in the largest modern RDBMs. Specifically, we study the effect of binary and interpolation search algorithmic techniques on the execution time of the insertion, deletion, and search functions of the B-tree. The computational experiments demonstrate the superiority of the interpolation search technique in the functional level of searching by primary key field.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503837",
        "category": "Databases"
    },
    {
        "title": "Predicting the Risk of Death for Sepsis Based on Within-Class Mixup and Lightgbm",
        "authors": "['Xun Wang', 'YaGang Wang', 'YuLong Hao']",
        "date": "September 2021",
        "source": "AIPR '21: Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition",
        "abstract": "Sepsis is a disease with a high mortality rate in intensive care units. Rapid and accurate identification of the hospitalization risk of sepsis patients is helpful for doctors to intervene in time. To address the problem of complexity, diversity, and imbalance of medical data in hospital, the prediction method, named Mu-Lightgbm, is proposed. Selectively referring to the disease severity scoring system, a public critical care medicine database (MIMIC-III) is used, and demographic information and laboratorial examination data are used as characteristic variables. Firstly, according to the distribution and category of the sample data, the within-class Mixup method is used to augment the sample data to ensure the balance between each class. Secondly, the Lightgbm model for risk prediction is constructed and trained with the processed data. A total of 23741 sepsis patients are collected from the MIMIC-III database with a mortality rate of 19.03%. Five-fold cross-validation shows that the AUC of Mu-Lightgbm model and Lightgbm model are 0.93 and 0.91, respectively. Compared with the existing prediction models, the proposed model performs better in prediction accuracy, which can assist clinicians in more timely treatment.",
        "link": "https://dl.acm.org/doi/10.1145/3488933.3489021",
        "category": "Databases"
    },
    {
        "title": "On Efficiently Equi-Joining Graphs",
        "authors": "['Giacomo Bergami']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Despite the growing popularity of techniques related to graph summarization, a general operator for joining graphs on both the vertices and the edges is still missing. Current languages such as Cypher and SPARQL express binary joins through the non-scalable and inefficient composition of multiple traversal and graph creation operations. In this paper, we propose an efficient equi-join algorithm that is able to perform vertex and path joins over a secondary memory indexed graph, also the resulting graph is serialised in secondary memory. The results show that the implementation of the proposed model outperforms solutions based on graphs, such as Neo4J and Virtuoso, and the relational model, such as PostgreSQL. Moreover, we propose two ways how edges can be combined, namely the conjunctive and disjunctive semantics, Preliminary experiments on the graph conjunctive join are also carried out with incremental updates, thus suggesting that our solution outperforms materialized views over PostgreSQL.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472269",
        "category": "Databases"
    },
    {
        "title": "Noise and Codification Effect on Emotional Speech Classification Systems",
        "authors": "['Fangfang Zhu-Zhou', 'Roberto Gil-Pita', 'Joaquín García-Gómez', 'Manuel Rosa-Zurera']",
        "date": "December 2021",
        "source": "WI-IAT '21: IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology",
        "abstract": "Emotions are reactions that all human beings experience daily, e.g., joy, sadness, fear, anger. They can be manifested through speech: when we speak, words are always accompanied by our emotions. This fact affects the muscular movements of the respiratory system and the larynx, thus modifying the timbre, tone, intensity, and intonation of what we say.  There are many acoustic databases labeled with emotions available in the public domain. However, most of them were created under non-real-world circumstances, i.e., actors recreated emotions, and emotions were labeled under fictitious conditions where noise is absent. Another drawback of the design of emotion recognition systems is the lack of enough patterns in the available databases, thus driving to generalization problems and leading to overfitting. In this paper, a system is developed in order to verify the noise and the codification effect. Results have shown a performance deterioration in both cases, increasing the error probability from 28.54% to 65.29% in the first case and from 28.54% to 40.09% in the second case. Furthermore, an enlargement of the training set is proposed, creating new virtual patterns consisting of the original patterns with the addition of different values of signal-to-noise ratio and its effect in the design of an emotion classification system, confirming an improvement of the test error probability.",
        "link": "https://dl.acm.org/doi/10.1145/3498851.3499022",
        "category": "Databases"
    },
    {
        "title": "MFPMiner: Mining Meaningful Frequent Patterns from Spatio-textual Trajectories",
        "authors": "['Fabio Valdes']",
        "date": "None",
        "source": "ACM Transactions on Spatial Algorithms and Systems",
        "abstract": "In the second decade of this century, technical progress has led to a worldwide proliferation of devices for tracking the movement behavior of a person, a vehicle, or another kind of entity. One of the consequences of this development is a massive and still growing amount of movement and movement-related data recorded by cellphones, automobiles, vessels, aircraft, and further GPS-enabled entities. As a result, the requirements for managing and analyzing movement records also increase, serving commercial, administrative, or private purposes. Since the development of hardware components cannot keep pace with the data growth, exploring methods of analyzing such trajectory datasets has become a very active and influential research field.For many application scenarios, besides the spatial trajectory of an entity, it is desirable to take additional semantic information into consideration. These descriptions also change with time and may represent, e.g., the course of streets passed by a bus, the sequence of region names traversed by an aircraft, or the points of interest in proximity of the positions of a taxi. Such data may be directly recorded by a sensor (such as the altitude of an aircraft) or computed from the spatial trajectory combined with some underlying information (for example, street names). It is often helpful or even necessary to focus on such semantic information for efficient analyses, as changes usually occur less frequently than it is the case for the spatial trajectory, where data points usually arrive in very close temporal distances. However, any kind of querying requires a deep semantic knowledge of the dataset at hand, particularly for retrieving the set of trajectories that match a certain mobility pattern, that is, a sequence of temporal, spatial, and semantic specifications.In this article, we introduce a framework named MFPMiner1 for retrieving all mobility patterns fulfilling a user-specified frequency threshold from a spatio-textual trajectory dataset. The resulting patterns and their relative frequency can be regarded as a knowledge base of the considered data. They may be directly visualized or applied for a pattern matching query yielding the set of matching trajectories. We demonstrate the functionality of our approach in an application scenario and provide an experimental evaluation of its performance on real and synthetic datasets by comparing it to three competitive methods. The framework has been fully implemented in a DBMS environment and is freely available open source software.",
        "link": "https://dl.acm.org/doi/10.1145/3498728",
        "category": "Databases"
    },
    {
        "title": "CoolMoves: User Motion Accentuation in Virtual Reality",
        "authors": "['Karan Ahuja', 'Eyal Ofek', 'Mar Gonzalez-Franco', 'Christian Holz', 'Andrew D. Wilson']",
        "date": "None",
        "source": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "abstract": "Current Virtual Reality (VR) systems are bereft of stylization and embellishment of the user's motion - concepts that have been well explored in animations for games and movies. We present CooIMoves, a system for expressive and accentuated full-body motion synthesis of a user's virtual avatar in real-time, from the limited input cues afforded by current consumer-grade VR systems, specifically headset and hand positions. We make use of existing motion capture databases as a template motion repository to draw from. We match similar spatio-temporal motions present in the database and then interpolate between them using a weighted distance metric. Joint prediction probability is then used to temporally smooth the synthesized motion, using human motion dynamics as a prior. This allows our system to work well even with very sparse motion databases (e.g., with only 3-5 motions per action). We validate our system with four experiments: a technical evaluation of our quantitative pose reconstruction and three additional user studies to evaluate the motion quality, embodiment and agency.",
        "link": "https://dl.acm.org/doi/10.1145/3463499",
        "category": "Databases"
    },
    {
        "title": "SDTA: An Algebra for Statistical Data Transformation",
        "authors": "['Jie Song', 'H. V. Jagadish', 'George Alter']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Statistical data manipulation is a crucial component of many data science analytic pipelines, particularly as part of data ingestion. This task is generally accomplished by writing transformation scripts in languages such as SPSS, Stata, SAS, R, Python (Pandas) and etc. The disparate data models, language representations and transformation operations supported by these tools make it hard for end users to understand and document the transformations performed, and for developers to port transformation code across languages.  Tackling these challenges, we present a formal paradigm for statistical data transformation. It consists of a data model, called Structured Data Transformation Data Model (SDTDM), inspired by the data models of multiple statistical transformations frameworks; an algebra, Structural Data Transformation Algebra (SDTA), with the ability to transform not only data within SDTDM but also metadata at multiple structural levels; and an equivalent descriptive counterpart, called Structured Data Transformation Language (SDTL), recently adopted by the DDI Alliance that maintains international standards for metadata as part of its suite of products. Experiments with real statistical transformations on socio-economic data show that SDTL can successfully represent 86.1% and 91.6% respectively of 4,185 commands in SAS and 9,087 commands in SPSS obtained from a repository.  We illustrate with examples how SDTA/SDTL could assist with the documentation of statistical data transformation, an important aspect often neglected in metadata of datasets. We propose a system called C2Metadata that automatically captures the transformation and provenance information in SDTL as a part of the metadata. Moreover, given the conversion mechanism from a source statistical language to SDTA/SDTL, we show how functional-equivalent transformation programs could be converted to other functionally equivalent programs, in the same or different language, permitting code reuse and result reproducibility, We also illustrate the possibility of using of SDTA to optimize SDTL transformations using rule-based rewrites similar to SQL optimizations.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468811",
        "category": "Databases"
    },
    {
        "title": "Evaluation of Machine Learning Algorithms in Predicting the Next SQL Query from the Future",
        "authors": "['Venkata Vamsikrishna Meduri', 'Kanchan Chowdhury', 'Mohamed Sarwat']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "Prediction of the next SQL query from the user, given her sequence of queries until the current timestep, during an ongoing interaction session of the user with the database, can help in speculative query processing and increased interactivity. While existing machine learning-- (ML) based approaches use recommender systems to suggest relevant queries to a user, there has been no exhaustive study on applying temporal predictors to predict the next user issued query.In this work, we experimentally compare ML algorithms in predicting the immediate next future query in an interaction workload, given the current user query or the sequence of queries in a user session thus far. As a part of this, we propose the adaptation of two powerful temporal predictors: (a) Recurrent Neural Networks (RNNs) and (b) a Reinforcement Learning approach called Q-Learning that uses Markov Decision Processes. We represent each query as a comprehensive set of fragment embeddings that not only captures the SQL operators, attributes, and relations but also the arithmetic comparison operators and constants that occur in the query. Our experiments on two real-world datasets show the effectiveness of temporal predictors against the baseline recommender systems in predicting the structural fragments in a query w.r.t. both quality and time. Besides showing that RNNs can be used to synthesize novel queries, we find that exact Q-Learning outperforms RNNs despite predicting the next query entirely from the historical query logs.",
        "link": "https://dl.acm.org/doi/10.1145/3442338",
        "category": "Databases"
    },
    {
        "title": "MG-Join: A Scalable Join for Massively Parallel Multi-GPU Architectures",
        "authors": "['Johns Paul', 'Shengliang Lu', 'Bingsheng He', 'Chiew Tong Lau']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The recent scale-up of GPU hardware through the integration of multiple GPUs into a single machine and the introduction of higher bandwidth interconnects like NVLink 2.0 has enabled new opportunities of relational query processing on multiple GPUs. However, due to the unique characteristics of GPUs and the interconnects, existing hash join implementations spend up to 66% of their execution time moving the data between the GPUs and achieve lower than 50% utilization of the newer high bandwidth interconnects. This leads to extremely poor scalablity of hash join performance on multiple GPUs, which can be slower than the performance on a single GPU. In this paper, we propose MG-Join, a scalable partitioned hash join implementation on multiple GPUs of a single machine. In order to effectively improve the bandwidth utilization, we develop a novel multi-hop routing for cross-GPU communication that adaptively chooses the efficient route for each data flow to minimize congestion. Our experiments on the DGX-1 machine show that MG-Join helps significantly reduce the communication overhead and achieves up to 97% utilization of the bisection bandwidth of the interconnects, resulting in significantly better scalability. Overall, MG-Join outperforms the state-of-the-art hash join implementations by up to 2.5x. MG-Join further helps improve the overall performance of TPC-H queries by up to 4.5x over multi-GPU version of an open-source commercial GPU database Omnisci.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457254",
        "category": "Databases"
    },
    {
        "title": "Boosting Graph Similarity Search through Pre-Computation",
        "authors": "['Jongik Kim']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Graph similarity search is to retrieve all graphs from a graph database whose graph edit distance (GED) to a query graph is within a given threshold. As GED computation is NP-hard, existing solutions adopt the filtering-and-verification framework, where the main focus is on the filtering phase to reduce the number of GED verifications. However, existing filtering techniques have inherently limited filtering capabilities, and suffer from a large number of GED verifications. To address the problem, in this paper, we propose a fundamentally different approach that utilizes pre-computed GEDs between data graphs in the filtering phase. Based on the approach, we develop a novel search framework Nass, which substantially reduces the verification workload. Because the efficiency of GED computation is essential in GED pre-computation, not to mention the verification of candidate graphs, we also propose an efficient GED computation algorithm as a part of Nass. We conduct extensive experiments on real datasets, and show Nass significantly outperforms the state-of-the art solutions.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452780",
        "category": "Databases"
    },
    {
        "title": "PostCENN: postgreSQL with machine learning models for cardinality estimation",
        "authors": "['Lucas Woltmann', 'Dominik Olwig', 'Claudio Hartmann', 'Dirk Habich', 'Wolfgang Lehner']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In this demo, we present PostCENN, an enhanced PostgreSQL database system with an end-to-end integration of machine learning (ML) models for cardinality estimation. In general, cardinality estimation is a topic with a long history in the database community. While traditional models like histograms are extensively used, recent works mainly focus on developing new approaches using ML models. However, traditional as well as ML models have their own advantages and disadvantages. With PostCENN, we aim to combine both to maximize their potentials for cardinality estimation by introducing ML models as a novel means to increase the accuracy of the cardinality estimation for certain parts of the database schema. To achieve this, we integrate ML models as first class citizen in PostgreSQL with a well-defined end-to-end life cycle. This life cycle consists of creating ML models for different sub-parts of the database schema, triggering the training, using ML models within the query optimizer in a transparent way, and deleting ML models.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476327",
        "category": "Databases"
    },
    {
        "title": "Bayesian pseudo posterior mechanism under asymptotic differential privacy",
        "authors": "['Terrance D. Savitsky', 'Matthew R. Williams', 'Jingchen Hu']",
        "date": "None",
        "source": "The Journal of Machine Learning Research",
        "abstract": "We propose a Bayesian pseudo posterior mechanism to generate record-level synthetic databases equipped with an (ε, π)-- probabilistic differential privacy (pDP) guarantee, where π denotes the probability that any observed database exceeds ε. The pseudo posterior mechanism employs a data record-indexed, risk-based weight vector with weight values ∈ [0, 1] that surgically downweight the likelihood contributions for high-risk records for model estimation and the generation of record-level synthetic data for public release. The pseudo posterior synthesizer constructs a weight for each datum record by using the Lipschitz bound for that record under a log-pseudo likelihood utility function that generalizes the exponential mechanism (EM) used to construct a formally private data generating mechanism. By selecting weights to remove likelihood contributions with non-finite log-likelihood values, we guarantee a finite local privacy guarantee for our pseudo posterior mechanism at every sample size. Our results may be applied to any synthesizing model envisioned by the data disseminator in a computationally tractable way that only involves estimation of a pseudo posterior distribution for parameters, θ, unlike recent approaches that use naturally-bounded utility functions implemented through the EM. We specify conditions that guarantee the asymptotic contraction of π to 0 over the space of databases, such that the form of the guarantee provided by our method is asymptotic. We illustrate our pseudo posterior mechanism on the sensitive family income variable from the Consumer Expenditure Surveys database published by the U.S. Bureau of Labor Statistics. We show that utility is better preserved in the synthetic data for our pseudo posterior mechanism as compared to the EM, both estimated using the same non-private synthesizer, due to our use of targeted downweighting.",
        "link": "https://dl.acm.org/doi/10.5555/3586589.3586644",
        "category": "Databases"
    },
    {
        "title": "Texture Dataset Construction and Texture Image Retrieval based on Deep Learning",
        "authors": "['Zhisheng Zhang', 'Huaijing Qu', 'Hengbin Wang', 'Jia Xu', 'Jiwei Wang', 'Yanan Wei']",
        "date": "December 2021",
        "source": "CSAI '21: Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence",
        "abstract": "In the deep texture image retrieval, to address the problem that the retrieval performance is affected by the lack of sufficiently large texture image dataset used for the effective training of deep neural network, a deep learning based texture dataset construction and texture image retrieval method is proposed in this paper. First, a large-scale texture image dataset containing rich texture information is constructed based on the DTD texture image dataset, and used as the source dataset for pre-training deep neural networks. To effectively characterize the information of the source texture dataset, a revised version of the VGG16 model, called ReV-VGG16, is adaptively designed. Then, the pre-trained ReV-VGG16 model is combined with the target texture image datasets for the transfer learning, and the probability values of the output from the classification layer of the model are used for the computation of the similarity measurement to achieve the retrieval of the target texture image dataset. Finally, the retrieval experiments are conducted on four typical texture image datasets, namely, VisTex, Brodatz, STex and ALOT. The experimental results show that our method outperforms the existing state-of-the-art texture image retrieval approaches in terms of the retrieval performance.",
        "link": "https://dl.acm.org/doi/10.1145/3507548.3507564",
        "category": "Databases"
    },
    {
        "title": "Scalable Learning to Troubleshoot Query Performance Problems",
        "authors": "['Alexandar Mihaylov', 'Vincent Corvinelli', 'Parke Godfrey', 'Piotr Mierzejewski', 'Jaroslaw Szlichta', 'Calisto Zuzarte']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Query optimization has long been fundamental for database systems. There are cracks in the edifice, however, as the complexity of modern query workloads outpace what database systems can manage well. Automatic tools are needed for database vendors, such as IBM with Db2, to help customers troubleshoot their performance problems, as manual troubleshooting is painstaking. To manage complex and large workloads, we develop a distributed system called dGALO that learns recurring problem patterns in query plans over workloads. dGALO employs these problem patterns to build a RDF-based, SPARQL-queried knowledge-base of plan-rewrite remedies. We illustrate a distributed implementation of dGALO on Apache Spark with efficient partitioning strategies for load balancing. The system employs additional pruning strategies via clustering, which yields a fine-grained trade off between runtime and accuracy. dGALO uses its knowledge-base to re-optimize queries, often to dramatic effect, and is a valuable tool for the development team to refine the optimizer with new techniques. We demonstrate by an experimental study over the TPC-DS benchmark the efficiency and effectiveness of our techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3481947",
        "category": "Databases"
    },
    {
        "title": "Graph Convolutional Multi-modal Hashing for Flexible Multimedia Retrieval",
        "authors": "['Xu Lu', 'Lei Zhu', 'Li Liu', 'Liqiang Nie', 'Huaxiang Zhang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Multi-modal hashing makes an important contribution to multimedia retrieval, where a key challenge is to encode heterogeneous modalities into compact hash codes. To solve this dilemma, graph-based multi-modal hashing methods generally define individual affinity matrix of each independent modality and apply linear algorithm for heterogeneous modalities fusion and compact hash learning. Several other methods construct graph Laplacian matrix based on semantic information to help learn discriminative hash code. However, these conventional methods roughly ignore the structural similarity of training set and the complex relations among multi-modal samples, which leads to unsatisfactory complementarity of fused hash codes. More notably, they are faced with two other important problems: huge computing and storage costs caused by graph construction and partial modality feature lost problem when incomplete query sample comes. In this paper, we propose a Flexible Graph Convolutional Multi-modal Hashing (FGCMH) method that adopts GCNs with linear complexity to preserve both the modality-individual and modality-fused structural similarity for discriminative hash learning. Necessarily, accurate multimedia retrieval can be performed on complete and incomplete datasets with our method. Specifically, multiple modality-individual GCNs under semantic guidance are proposed to act on each individual modality independently for intra-modality similarity preserving, then the output representations are fused into a fusion graph with adaptive weighting scheme. Hash GCN and semantic GCN, which share parameters in the first two layers, propagate fusion information and generate hash codes under high-level label space supervision. In the query stage, our method adaptively captures various multi-modal contents in a flexible and robust way, even if partial modality features are lost. Experimental results on three publicly datasets show the flexibility and effectiveness of our proposed method.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475598",
        "category": "Databases"
    },
    {
        "title": "LSQB: a large-scale subgraph query benchmark",
        "authors": "['Amine Mhedhbi', 'Matteo Lissandrini', 'Laurens Kuiper', 'Jack Waudby', 'Gábor Szárnyas']",
        "date": "June 2021",
        "source": "GRADES-NDA '21: Proceedings of the 4th ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)",
        "abstract": "We introduce LSQB, a new large-scale subgraph query benchmark. LSQB tests the performance of database management systems on an important class of subgraph queries overlooked by existing benchmarks. Matching a labelled structural graph pattern, referred to as subgraph matching, is the focus of LSQB. In relational terms, the benchmark tests DBMSs' join performance as a choke-point since subgraph matching is equivalent to multi-way joins between base Vertex and base Edge tables on ID attributes. The benchmark focuses on read-heavy workloads by relying on global queries which have been ignored by prior benchmarks. Global queries, also referred to as unseeded queries, are a type of queries that are only constrained by labels on the query vertices and edges. LSQB contains a total of nine queries and leverages the LDBC social network data generator for scalability. The benchmark gained both academic and industrial interest and is used internally by 5+ different vendors.",
        "link": "https://dl.acm.org/doi/10.1145/3461837.3464516",
        "category": "Databases"
    },
    {
        "title": "Revisiting the design of LSM-tree Based OLTP storage engine with persistent memory",
        "authors": "['Baoyue Yan', 'Xuntao Cheng', 'Bo Jiang', 'Shibin Chen', 'Canfang Shang', 'Jianying Wang', 'Gui Huang', 'Xinjun Yang', 'Wei Cao', 'Feifei Li']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The recent byte-addressable and large-capacity commercialized persistent memory (PM) is promising to drive database as a service (DBaaS) into unchartered territories. This paper investigates how to leverage PMs to revisit the conventional LSM-tree based OLTP storage engines designed for DRAM-SSD hierarchy for DBaaS instances. Specifically we (1) propose a light-weight PM allocator named Hal-loc customized for LSM-tree, (2) build a high-performance Semi-persistent Memtable utilizing the persistent in-memory writes of PM, (3) design a concurrent commit algorithm named Reorder Ring to aschieve log-free transaction processing for OLTP workloads and (4) present a Global Index as the new globally sorted persistent level with non-blocking in-memory compaction. The design of Reorder Ring and Semi-persistent Memtable achieves fast writes without synchronized logging overheads and achieves near instant recovery time. Moreover, the design of Semi-persistent Memtable and Global Index with in-memory compaction enables the byte-addressable persistent levels in PM, which significantly reduces the read and write amplification as well as the background compaction overheads. The overall evaluation shows that the performance of our proposal over PM-SSD hierarchy outperforms the baseline by up to 3.8x in YCSB benchmark and by 2x in TPC-C benchmark.",
        "link": "https://dl.acm.org/doi/10.14778/3467861.3467875",
        "category": "Databases"
    },
    {
        "title": "ArchaeoDAL: A Data Lake for Archaeological Data Management and Analytics",
        "authors": "['Pengfei Liu', 'Sabine Loudcher', 'Jérôme Darmont', 'Camille Noûs']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "With new emerging technologies, such as satellites and drones, archaeologists collect data over large areas. However, it becomes difficult to process such data in time. Archaeological data also have many different formats (images, texts, sensor data) and can be structured, semi-structured and unstructured. Such variety makes data difficult to collect, store, manage, search and analyze effectively. A few approaches have been proposed, but none of them covers the full data lifecycle nor provides an efficient data management system. Hence, we propose the use of a data lake to provide centralized data stores to host heterogeneous data, as well as tools for data quality checking, cleaning, transformation and analysis. In this paper, we propose a generic, flexible and complete data lake architecture. Our metadata management system exploits goldMEDAL, which is the most generic metadata model currently available. Finally, we detail the concrete implementation of this architecture dedicated to an archaeological project.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472266",
        "category": "Databases"
    },
    {
        "title": "Network data acquisition method based on crop pest control knowledge",
        "authors": "['Han Chunyu', 'Fang Jiandong', 'Li Bajin', 'Zhao Yudong']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "In this article, through a multithreaded focused web crawler type crawl, the knowledge related websites to establish database of crop diseases and pests control of crop diseases and pests control to create professional knowledge search engine, and by using natural language processing, knowledge map technology for processing and storage of data, such as, lower demand for the user to enter the search term, optimization of the user's search experience. This study constructed a complete search engine and provided a set of methods for obtaining and gathering data related to crop disease and pest control knowledge, which can also be applied to information collection and processing in other fields.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470368",
        "category": "Databases"
    },
    {
        "title": "Design and Implementation of Integrated Processing Platform Based on Patrol UAVs",
        "authors": "['Rui Tong', 'Qiang Wu', 'Wei He']",
        "date": "October 2021",
        "source": "ICCPR '21: Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition",
        "abstract": "With the development of UAV technology, UAVs are gradually being widely used in various civil fields with their characteristics of no geographical restrictions, high safety and easy operation, etc. The application of UAVs in the field of line patrol has greatly reduced the cost of manpower and time cost. In order to meet the actual needs of patrol UAV, this paper designs a ground integrated processing platform based on patrol UAV, which has the functions of task division, UAV status monitoring, UAV trajectory planning and distribution, flight control, real-time inspection of patrol line, ground inspection, data comparison and database management, etc., to meet the use of patrol UAV for ground processing platform.",
        "link": "https://dl.acm.org/doi/10.1145/3497623.3497640",
        "category": "Databases"
    },
    {
        "title": "Automatic Recognition of Traditional Analog Gauge and Intelligent Management by IOT Method",
        "authors": "['Chuan-Chun Wu', 'Hsien-Leing Tsai', 'Chienhsing Wu', 'Po-Jen Wang']",
        "date": "November 2021",
        "source": "MISNC '21: Proceedings of the 8th Multidisciplinary International Social Networks Conference",
        "abstract": "The purpose of this research is using the technology of the Internet of Things to improve current medical or traditional industries in meter recording system. This research develops an IoT automatic meter reading system that is practical, precise, and highly feasible. The main device includes Arduino, two-axis slide rail, stepping motor and driver, Raspberry PI4, camera and server PC. Two-axis stepping motor is driven by the Arduino through the motor drive board, so that the camera on the slide rail accurately aligns with the center of the meter surface to improve the deviation cause by reader's viewing angle. After the Arduino positioning is completed, it notifies the Raspberry PI4, and then PI4 issues a screen capture command to the camera. We use OpenCV image recognition software to correctly identify and calculate the best value to achieve a goal that is more accurate than human reading. Finally, the original image and the recognized value are sent back and save on the PC server database (My SQL) through the TCP/IP Socket which is running on the Raspberry PI4. And then, Big Data analysis is applied, and its result is presented via Web-Service Page. This saves manpower trip or avoid the risk of entering a dangerous area and realize unmanned gauge recording. And when the system equipment is upgraded, there is no need to replace the original equipment components to achieve seamless system conversion.",
        "link": "https://dl.acm.org/doi/10.1145/3504006.3504011",
        "category": "Databases"
    },
    {
        "title": "Design and Implementation of the E-mall Management System Based on SSM",
        "authors": "['Qiliang Sun']",
        "date": "July 2021",
        "source": "ICEME '21: Proceedings of the 2021 12th International Conference on E-business, Management and Economics",
        "abstract": "Abstract: The electronic mall is an electronic trading platform for large commodities built with e-commerce software. Its main function is to sell products to customers accurately and quickly through this platform. Online Marketing has become an irreplaceable and important means in product sales. This article designs and implements an electronic mall management system. It integrates functions such as product classification, product browsing, shopping cart function, order management, and announcement management. The system provides services for registered members, non-registered members, product managers, system administrators, and big data analysts. By using Spring framework's AOP (Aspect-Oriented Programming) features, Inversion of Control (Inversion of Control) features, Spring MVC model and MyBatis ORM features, using B/S architecture and MySQL database, this paper implements the system. This programming has high reconfigurability. This system provides functions such as product information management, user information management, shopping cart order management, announcement management, system log, and user authority management. At the same time, it is ensured that these functions will not be illegally operated by users without authority, so that the system has higher security. The system has now been deployed and used to meet the needs of registered members, product managers, system administrators, big data analysts and other users. This system provides an integrated solution for Online Marketing, which has high practical and commercial value.",
        "link": "https://dl.acm.org/doi/10.1145/3481127.3481225",
        "category": "Databases"
    },
    {
        "title": "Evaluating query languages and systems for high-energy physics data",
        "authors": "['Dan Graur', 'Ingo Müller', 'Mason Proffitt', 'Ghislain Fourny', 'Gordon T. Watts', 'Gustavo Alonso']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In the domain of high-energy physics (HEP), query languages in general and SQL in particular have found limited acceptance. This is surprising since HEP data analysis matches the SQL model well: the data is fully structured and queried using mostly standard operators. To gain insights on why this is the case, we perform a comprehensive analysis of six diverse, general-purpose data processing platforms using an HEP benchmark. The result of the evaluation is an interesting and rather complex picture of existing solutions: Their query languages vary greatly in how natural and concise HEP query patterns can be expressed. Furthermore, most of them are also between one and two orders of magnitude slower than the domain-specific system used by particle physicists today. These observations suggest that, while database systems and their query languages are in principle viable tools for HEP, significant work remains to make them relevant to HEP researchers.",
        "link": "https://dl.acm.org/doi/10.14778/3489496.3489498",
        "category": "Databases"
    },
    {
        "title": "Design and Implementation of WeChat Public Platform Service for Career Counseling in Colleges and Universities in Development Mode",
        "authors": "['Hong Li']",
        "date": "January 2022",
        "source": "IC4E '22: Proceedings of the 2022 13th International Conference on E-Education, E-Business, E-Management, and E-Learning",
        "abstract": "In order to improve the quality and efficiency of career counseling in colleges and universities, the hot news of career counseling, career counseling resource library and other query functions are embedded into WeChat application based on the development mode of WeChat public platform and using the message receiving and reply interface provided by it, so that students can conveniently use platform services and resources in the social network environment through instruction interaction by configuring service environment preparation, basic language of program development and design, database and front-back linkage design of WeChat public platform. This application can enrich the forms of career education, improve efficiency and strengthen pertinence.",
        "link": "https://dl.acm.org/doi/10.1145/3514262.3514324",
        "category": "Databases"
    },
    {
        "title": "Analysis of Machine Learning Models Predicting Quality of Life for Cancer Patients",
        "authors": "['Miloš Savić', 'Vladimir Kurbalija', 'Mihailo Ilić', 'Mirjana Ivanović', 'Dušan Jakovetić', 'Antonios Valachis', 'Serge Autexier', 'Johannes Rust', 'Thanos Kosmidis']",
        "date": "November 2021",
        "source": "MEDES '21: Proceedings of the 13th International Conference on Management of Digital EcoSystems",
        "abstract": "Quality of life (QoL) is one of the major issues for cancer patients. With the advent of medical databases containing large amounts of relevant QoL information it becomes possible to train predictive QoL models by machine learning (ML) techniques. However, the training of predictive QoL models poses several challenges mostly due to data privacy concerns and missing values in patient data. In this paper, we analyze several classification and regression ML models predicting QoL indicators for breast and prostate cancer patients. Two different approaches are employed for imputing missing values. The examined ML models are trained on datasets formed from two databases containing a large number of anonymized medical records of cancer patients from Sweden. Two learning scenarios are considered: centralized and federated learning. In the centralized learning scenario all patient data coming from different data sources is collected at a central location prior to model training. On the other hand, federated learning enables collective training of machine learning models without data sharing. The results of our experimental evaluation show that the predictive power of federated models is comparable to that of centrally trained models for short-term QoL predictions, whereas for long-term periods centralized models provide more accurate QoL predictions.",
        "link": "https://dl.acm.org/doi/10.1145/3444757.3485103",
        "category": "Databases"
    },
    {
        "title": "Emerging applications of machine learning in modern data management",
        "authors": "['Amin Kamali', 'Calisto Zuzarte', 'Verena Kantere']",
        "date": "November 2021",
        "source": "CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering",
        "abstract": "In recent years, the applications of machine learning (ML) have proliferated in most aspects of traditional computer science. Data management discipline is no exception in this regard. Rule-based modules are being replaced by ML-based counterparts that effectively 'mine the rules' from experience. Approaches that rely on crude statistics are rapidly being outdated by the ones that 'learn' the functional dependencies, correlations, and skewness from the underlying data. These learning-based methods have an upper hand on many different fronts. On one hand, they promise to reduce the cost of development and maintenance of the highly complex classical modules. On the other hand, they avoid the 'one solution fits all' approach by effectively tailoring the behavior to fit the requirements of individual system instances. This workshop brought together leaders of cutting-edge research projects in the area and audience from academia and industry, to discuss some examples of using machine learning for modernizing different aspects of data management. The discussed examples covered four different areas: Query Optimization, Data Partitioning, Database Knobs Tuning, and Data Caching.",
        "link": "https://dl.acm.org/doi/10.5555/3507788.3507841",
        "category": "Databases"
    },
    {
        "title": "Automatic Multiscale-based Peak Detection on Short Time Energy and Spectral Centroid Feature Extraction for Conversational Speech Segmentation",
        "authors": "['Barlian Henryranu Prasetio', 'Edita Rosana Widasari', 'Hiroki Tamura']",
        "date": "September 2021",
        "source": "SIET '21: Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology",
        "abstract": "In this paper, we present a conversational speech segmentation system. We assume that the speech/non-speech has different energy in time and frequency domain. Therefore, the short time energy and spectral centroid are proposed as the feature extraction technique and the automatic multiscale algorithm as the signal peak detection. In the pre-processing phase, the Savitzky-Golay filter is performed to reduce the noise before feature extraction process. The short time feature serves to capture a short-stroke character and the spectral centroid feature is used to response the spectral characteristic. For observation, experimental and validation process, we use the six conversations of the SUSAS Database. The experimental result shows that the average accuracy of the proposed system is 98.26%.",
        "link": "https://dl.acm.org/doi/10.1145/3479645.3479675",
        "category": "Databases"
    },
    {
        "title": "Informing a Statewide Investment: The NYS Voter Registration Data Pattern Detection Prototype Project",
        "authors": "['Meghan Cook', 'Jeffrey Baez']",
        "date": "June 2021",
        "source": "DG.O'21: DG.O2021: The 22nd Annual International Conference on Digital Government Research",
        "abstract": "This management case study presents the results of a prototype project conducted with the New York State Board of Elections (NYSBOE) to investigate and make recommendations on pattern detection analytical models for the purposes of informing their future investment of a statewide detection and visualization system for voter registration data. NYSBOE, a bipartisan organization with a mission to protect the integrity of elections, recognized that a critical element of protecting elections includes a systematic and intelligence driven approach to monitoring voter registration data as part of an overall cybersecurity program. Using over thirteen years of data from the NYS voter registration database (NYSVoter), the prototypes yielded valuable insights on the analytical models and visualizations most appropriate for the purpose of pattern detection in voter registration data so that state election leaders can better inform their investment. This management paper presents a short background on voter registration data, elections, and the importance of pattern detection as a part of a cyber security program, prototype project background, insights generating in identifying most appropriate analytical models and visualizations for voter registration data, and a short conclusion.",
        "link": "https://dl.acm.org/doi/10.1145/3463677.3463693",
        "category": "Databases"
    },
    {
        "title": "Security and Privacy of Patient Information in Medical Systems Based on Blockchain Technology",
        "authors": "['Hongjiao Wu', 'Ashutosh Dhar Dwivedi', 'Gautam Srivastava']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "The essence of “blockchain” is a shared database in which information stored is un-falsifiable, traceable, open, and transparent. Therefore, to improve the security of private information in medical systems, this article uses blockchain technology to design a method to protect private information in medical systems and effectively realize anti-theft control of private information. First, the Patient-oriented Privacy Preserving Access Control model is introduced into the access control process of private information in medical systems. Next, a private information storage platform is built by using blockchain technology, and information transmission is realized using standard cryptographic algorithms. In this process, file authorization contracts are also used to guarantee the security of private information and further prevent theft of medical private information. Our simulation results show that the storage response time of this method is kept below 1,000 ms, and the maximum information throughput rate reaches 550 kbit/s, which indicates that this method has strong performance in information storage and transmission efficiency. Moreover, the reliability and bandwidth utilization of data transmission across domains is higher, so the method has higher information security control performance and superior overall performance.",
        "link": "https://dl.acm.org/doi/10.1145/3408321",
        "category": "Databases"
    },
    {
        "title": "Efficient Skyline Computation of Multiple Range Skyline Queries",
        "authors": "['Zarina Dzolkhifli', 'Hamidah Ibrahim', 'Fatimah Sidi', 'Lilly Suriani Affendey', 'Siti Nurulain Mohd Rum', 'Ali A. Alwan']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "Skyline query which returns a set of skyline objects by filtering those objects that are dominated by others from a potentially large multidimensional data set has attracted significant research attention especially in the database community. Many variants of skyline queries have been introduced which include among others range skyline queries which retrieve skyline objects within a specified range also known as condition (constraint). It enables users to specify preferences within an ideal range value over a dimension(s) instead of a single sought value. Although range skyline queries have been studied extensively, most of the works focus on the optimisation problem of skyline computation for a given range skyline query. Nonetheless, deriving skyline objects for multiple range skyline queries separately is unwise since these queries might specify similar constraints/sub constraints and hence the same set of objects has to be scanned and compared multiple times before the skyline objects can be determined for each individual query. In this paper, we propose the Reduct-Sky framework that attempts to avoid unnecessary skyline computations of multiple range skyline queries. Intuitively, this is achieved by analysing the conditions of multiple range skyline queries to mainly extract the conditions/sub conditions that are the same to ensure that skyline computation over the involved set of objects is performed only once instead of multiple times. An initial performance evaluation of the proposed framework demonstrates its efficiency with regard to number of pairwise comparisons.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487718",
        "category": "Databases"
    },
    {
        "title": "Retrieving Large-scale Product Knowledge by Collaborative Computing of CPUs and GPUs",
        "authors": "['Hao Song', 'Chuangxin Fang', 'Zhengguo Huang', 'Yuming Lin']",
        "date": "July 2021",
        "source": "ICICSE 2021: 2021 10th International Conference on Internet Computing for Science and Engineering",
        "abstract": "The knowledge graph is the essential infrastructure of plenty of intelligent Web applications. There are various types of knowledge graphs designed and deployed to make the applications smarter during the past decade. However, the increasing amount of product data brings new challenges to the query and retrieval of products. In this work, we present a product knowledge representation method that converts text into IDs to reduce the data transmission time. In order to decrease the retrieval time, we transform the join operation in the relational database into a matrix operation. We propose a pipeline query optimization strategy under GPUs to speed up the execution of the query. To evaluate the performance, we compare our method with the state-of-the-art RDF engine RDF-3X and gStore on the large-scale product datasets. The experimental results show that our approach can significantly improve the efficiency of the SPARQL query.",
        "link": "https://dl.acm.org/doi/10.1145/3485314.3485334",
        "category": "Databases"
    },
    {
        "title": "Retrieving Large-scale Product Knowledge by Collaborative Computing of CPUs and GPUs",
        "authors": "['Hao Song', 'Chuangxin Fang', 'Zhengguo Huang', 'Yuming Lin']",
        "date": "July 2021",
        "source": "ICICSE 2021: 2021 10th International Conference on Internet Computing for Science and Engineering",
        "abstract": "The knowledge graph is the essential infrastructure of plenty of intelligent Web applications. There are various types of knowledge graphs designed and deployed to make the applications smarter during the past decade. However, the increasing amount of product data brings new challenges to the query and retrieval of products. In this work, we present a product knowledge representation method that converts text into IDs to reduce the data transmission time. In order to decrease the retrieval time, we transform the join operation in the relational database into a matrix operation. We propose a pipeline query optimization strategy under GPUs to speed up the execution of the query. To evaluate the performance, we compare our method with the state-of-the-art RDF engine RDF-3X and gStore on the large-scale product datasets. The experimental results show that our approach can significantly improve the efficiency of the SPARQL query.",
        "link": "https://dl.acm.org/doi/10.1145/3485314.3485334",
        "category": "Databases"
    },
    {
        "title": "Security and Privacy of Patient Information in Medical Systems Based on Blockchain Technology",
        "authors": "['Hongjiao Wu', 'Ashutosh Dhar Dwivedi', 'Gautam Srivastava']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "The essence of “blockchain” is a shared database in which information stored is un-falsifiable, traceable, open, and transparent. Therefore, to improve the security of private information in medical systems, this article uses blockchain technology to design a method to protect private information in medical systems and effectively realize anti-theft control of private information. First, the Patient-oriented Privacy Preserving Access Control model is introduced into the access control process of private information in medical systems. Next, a private information storage platform is built by using blockchain technology, and information transmission is realized using standard cryptographic algorithms. In this process, file authorization contracts are also used to guarantee the security of private information and further prevent theft of medical private information. Our simulation results show that the storage response time of this method is kept below 1,000 ms, and the maximum information throughput rate reaches 550 kbit/s, which indicates that this method has strong performance in information storage and transmission efficiency. Moreover, the reliability and bandwidth utilization of data transmission across domains is higher, so the method has higher information security control performance and superior overall performance.",
        "link": "https://dl.acm.org/doi/10.1145/3408321",
        "category": "Databases"
    },
    {
        "title": "Example-guided synthesis of relational queries",
        "authors": "['Aalok Thakkar', 'Aaditya Naik', 'Nathaniel Sands', 'Rajeev Alur', 'Mayur Naik', 'Mukund Raghothaman']",
        "date": "June 2021",
        "source": "PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
        "abstract": "Program synthesis tasks are commonly specified via input-output examples. Existing enumerative techniques for such tasks are primarily guided by program syntax and only make indirect use of the examples. We identify a class of synthesis algorithms for programming-by-examples, which we call Example-Guided Synthesis (EGS), that exploits latent structure in the provided examples while generating candidate programs. We present an instance of EGS for the synthesis of relational queries and evaluate it on 86 tasks from three application domains: knowledge discovery, program analysis, and database querying. Our evaluation shows that EGS outperforms state-of-the-art synthesizers based on enumerative search, constraint solving, and hybrid techniques in terms of synthesis time, quality of synthesized programs, and ability to prove unrealizability.",
        "link": "https://dl.acm.org/doi/10.1145/3453483.3454098",
        "category": "Databases"
    },
    {
        "title": "Privacy heroes need data disguises",
        "authors": "['Lillian Tsai', 'Malte Schwarzkopf', 'Eddie Kohler']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "Providing privacy in complex, data-rich applications is hard. Deleting accounts, anonymizing an account's contributions, and other privacy-related actions may require the traversal and transformation of interwoven state in a relational database. Finding the affected data is already nontrivial, but privacy actions must additionally balance competing requirements, such as preserving data trails for legal reasons or allowing users to change their mind. We believe a systematic shared framework for specifying and implementing privacy transformations could simplify and empower applications. Our prototype, data disguising, supports fine-grained, nuanced, and useful policies that would be cumbersome to implement manually, including reversible transformations that can compose.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465284",
        "category": "Databases"
    },
    {
        "title": "Chiller: Contention-centric Transaction Execution and Data Partitioning for Modern Networks (Technical Perspective)",
        "authors": "['Alan D. Fekete']",
        "date": "March 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Many computing researchers and practitioners may be surprised to find a \"research highlight\" which innovates on the way to process database transactions. Work in the early 1970s, by Turing winner Jim Gray and others, established a standard set of techniques for transaction management. These remain the basis of most commercial and open-source platforms [1], and they are still taught in university database classes. So why is important research still needed in this topic? The technology environment keeps evolving, and new performance characteristics mean that new algorithms and system designs become appropriate. This perspective will summarise the early work, and point to how the field has continued to progress.",
        "link": "https://dl.acm.org/doi/10.1145/3471485.3471489",
        "category": "Databases"
    },
    {
        "title": "When is approximate counting for conjunctive queries tractable?",
        "authors": "['Marcelo Arenas', 'Luis Alberto Croquevielle', 'Rajesh Jayaram', 'Cristian Riveros']",
        "date": "June 2021",
        "source": "STOC 2021: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing",
        "abstract": "Conjunctive queries are one of the most common class of queries used in database systems, and the best studied in the literature. A seminal result of Grohe, Schwentick, and Segoufin (STOC 2001) demonstrates that for every class G of graphs, the evaluation of all conjunctive queries whose underlying graph is in G is tractable if, and only if, G has bounded treewidth. In this work, we extend this characterization to the counting problem for conjunctive queries. Specifically, for every class C of conjunctive queries with bounded treewidth, we introduce the first fully polynomial-time randomized approximation scheme (FPRAS) for counting answers to a query in C, and the first polynomial-time algorithm for sampling answers uniformly from a query in C. As a corollary, it follows that for every class G of graphs, the counting problem for conjunctive queries whose underlying graph is in G admits an FPRAS if, and only if, G has bounded treewidth (unless BPP is different from P). In fact, our FPRAS is more general, and also applies to conjunctive queries with bounded hypertree width, as well as unions of such queries.   The key ingredient in our proof is the resolution of a fundamental counting problem from automata theory. Specifically, we demonstrate the first FPRAS and polynomial time sampler for the set of trees of size n accepted by a tree automaton, which improves the prior quasi-polynomial time randomized approximation scheme (QPRAS) and sampling algorithm of Gore, Jerrum, Kannan, Sweedyk, and Mahaney ’97. We demonstrate how this algorithm can be used to obtain an FPRAS for many open problems, such as counting solutions to constraint satisfaction problems (CSP) with bounded hypertree width, counting the number of error threads in programs with nested call subroutines, and counting valid assignments to structured DNNF circuits.",
        "link": "https://dl.acm.org/doi/10.1145/3406325.3451014",
        "category": "Databases"
    },
    {
        "title": "Viktor Leis Speaks Out on Concurrency and Parallelism on Multicore CPUs",
        "authors": "['Marianne Winslett', 'Vanessa Braganholo']",
        "date": "June 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Welcome to ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today I have here with me Viktor Leis who won the 2018 ACM SIGMOD Jim Gray Dissertation Award for his thesis entitled Query Processing and Optimization in Modern Database Systems. Viktor is now at the University of University of Erlangen-Nuremberg and his Ph.D. is from the Technical University of Munich, where he worked with Thomas Neumann and Alfons Kemper. So, Viktor, welcome.",
        "link": "https://dl.acm.org/doi/10.1145/3484622.3484633",
        "category": "Databases"
    },
    {
        "title": "Bringing Cloud-Native Storage to SAP IQ",
        "authors": "['Mohammed Abouzour', 'Güneş Aluç', 'Ivan T. Bowman', 'Xi Deng', 'Nandan Marathe', 'Sagar Ranadive', 'Muhammed Sharique', 'John C. Smirnios']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In this paper, we describe our journey of transforming SAP IQ into a relational database management system (RDBMS) that utilizes cheap, elastically scalable object stores on the cloud. SAP IQ is a three-decade old, disk-based, columnar RDBMS that is optimized for complex online analytical processing (OLAP) workloads. Traditionally, SAP IQ has been designed to operate on shared storage devices with strong consistency guarantees (e.g., high-caliber storage area network devices). Therefore, deploying SAP IQ on the cloud, as is, would have meant utilizing storage solutions such as NetApp or AWS EFS that provide a POSIX compliant file interface and strong consistency guarantees, but at a much higher monetary cost. These costs can accumulate easily to diminish the economies of scale that one would expect on the cloud, which can be undesirable. Instead, we have enhanced the design of SAP IQ to operate on cloud object stores such as AWS S3 and Azure Blob Storage. Object stores rely on a weaker consistency model, and potentially have higher latency; however, because of these design trade-offs, they are able to offer (i) better pricing, (ii) enhanced durability, (iii) improved elasticity, and (iv) higher throughput. By enhancing SAP IQ to operate under these design trade-offs, we have unlocked many of the opportunities offered by object stores. More specifically, we have extended SAP IQ's buffer manager and transaction manager, and have introduced a new caching layer that utilizes instance storage on AWS EC2. Experiments using the TPC-H benchmark demonstrate that we can gain an order of magnitude reduction in data-at rest storage costs while improving query and load performance.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457563",
        "category": "Databases"
    },
    {
        "title": "Caching Support for Range Query Processing on Bitmap Indices",
        "authors": "['Sarah McClain', 'Manya Mutschler-Aldine', 'Colin Monaghan', 'David Chiu', 'Jason Sawin', 'Patrick Jarvis']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Bitmaps are commonly used for indexing read-mostly data sets. The range of an attribute is split into bins, where its values are placed: bij = 1 denotes the value of the ith tuple is in the jth bin, and bij = 0 otherwise. A number of query types can be decomposed into the systematic application of boolean operators over sets of bins. However, when bitmaps are high-dimensional, the overall query-processing performance can deteriorate due to the increased number of bins that participate per query.  We propose a caching framework that organizes, manages, and integrates cached partial results to accelerate query processing on high-dimensional bitmaps. We begin by showing that, to resolve general complex disjunctive and conjunctive queries, the selection of an optimal set of partial bitmap results is NP-complete. A restriction on this problem to only consider consecutive bin sequences (characteristic of common range and point queries) allows us to solve it efficiently. The evaluation our caching system over several workloads carried out on the TPC-H benchmark and a real network-intrusion data set is presented.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468800",
        "category": "Databases"
    },
    {
        "title": "On the Enumeration Complexity of Unions of Conjunctive Queries",
        "authors": "['Nofar Carmeli', 'Markus Kröll']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "We study the enumeration complexity of Unions of Conjunctive Queries (UCQs). We aim to identify the UCQs that are tractable in the sense that the answer tuples can be enumerated with a linear preprocessing phase and a constant delay between every successive tuples. It has been established that, in the absence of self-joins and under conventional complexity assumptions, the CQs that admit such an evaluation are precisely the free-connex ones. A union of tractable CQs is always tractable. We generalize the notion of free-connexity from CQs to UCQs, thus showing that some unions containing intractable CQs are, in fact, tractable. Interestingly, some unions consisting of only intractable CQs are tractable too. We show how to use the techniques presented in this article also in settings where the database contains cardinality dependencies (including functional dependencies and key constraints) or when the UCQs contain disequalities. The question of finding a full characterization of the tractability of UCQs remains open. Nevertheless, we prove that, for several classes of queries, free-connexity fully captures the tractable UCQs.",
        "link": "https://dl.acm.org/doi/10.1145/3450263",
        "category": "Databases"
    },
    {
        "title": "Enumerating Fair Packages for Group Recommendations",
        "authors": "['Ryoma Sato']",
        "date": "February 2022",
        "source": "WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
        "abstract": "Package-to-group recommender systems recommend a set of unified items to a group of people. Different from conventional settings, it is not easy to measure the utility of group recommendations because it involves more than one user. In particular, fairness is crucial in group recommendations. Even if some members in a group are substantially satisfied with a recommendation, it is undesirable if other members are ignored to increase the total utility. Many methods for evaluating and applying the fairness of group recommendations have been proposed in the literature. However, all these methods maximize the score and output only one package. This is in contrast to conventional recommender systems, which output several (e.g., top-K) candidates. This can be problematic because a group can be dissatisfied with the recommended package owing to some unobserved reasons, even if the score is high. To address this issue, we propose a method to enumerate fair packages efficiently. Our method furthermore supports filtering queries, such as top-K and intersection, to select favorite packages when the list is long. We confirm that our algorithm scales to large datasets and can balance several aspects of the utility of the packages.",
        "link": "https://dl.acm.org/doi/10.1145/3488560.3498432",
        "category": "Databases"
    },
    {
        "title": "Dr.Aid: Supporting Data-governance Rule Compliance for Decentralized Collaboration in an Automated Way",
        "authors": "['Rui Zhao', 'Malcolm Atkinson', 'Petros Papapanagiotou', 'Federica Magnoni', 'Jacques Fleuriot']",
        "date": "None",
        "source": "Proceedings of the ACM on Human-Computer Interaction",
        "abstract": "Collaboration across institutional boundaries is widespread and increasing today. It depends on federations sharing data that often have governance rules or external regulations restricting their use. However, the handling of data governance rules (aka. data-use policies) remains manual, time-consuming and error-prone, limiting the rate at which collaborations can form and respond to challenges and opportunities, inhibiting citizen science and reducing data providers' trust in compliance. Using an automated system to facilitate compliance handling reduces substantially the time needed for such non-mission work, thereby accelerating collaboration and improving productivity. We present a framework, Dr.Aid, that helps individuals, organisations and federations comply with data rules, using automation to track which rules are applicable as data is passed between processes and as derived data is generated. It encodes data-governance rules using a formal language and performs reasoning on multi-input-multi-output data-flow graphs in decentralised contexts. We test its power and utility by working with users performing cyclone tracking and earthquake modelling to support mitigation and emergency response. We query standard provenance traces to detach Dr.Aid from details of the tools and systems they are using, as these inevitably vary across members of a federation and through time. We evaluate the model in three aspects by encoding real-life data-use policies from diverse fields, showing its capability for real-world usage and its advantages compared with traditional frameworks. We argue that this approach will lead to more agile, more productive and more trustworthy collaborations and show that the approach can be adopted incrementally. This, in-turn, will allow more appropriate data policies to emerge opening up new forms of collaboration.",
        "link": "https://dl.acm.org/doi/10.1145/3479604",
        "category": "Databases"
    },
    {
        "title": "Building verified neural networks with specifications for systems",
        "authors": "['Cheng Tan', 'Yibo Zhu', 'Chuanxiong Guo']",
        "date": "August 2021",
        "source": "APSys '21: Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems",
        "abstract": "Neural networks (NNs) are beneficial to many services, and we believe systems—such as OSes, databases, networked systems—are not an exception. But applying NNs in these critical systems is challenging: people have to risk getting unexpected outcomes from NNs because NN behaviors are not well-defined. To tame these undefined behaviors, we introduce a framework ouroboros, which builds verified NNs that follow user-defined specifications. These specifications comprise input and output constraints which characterize the behaviors of a NN. We do a case study on database learned indexes to demonstrate that training verified NN models is possible. Though many challenges remain, ouroboros enables us, for the first time, to apply NNs in critical systems with _confidence_.",
        "link": "https://dl.acm.org/doi/10.1145/3476886.3477508",
        "category": "Databases"
    },
    {
        "title": "Inverted Secondary Index Cluster Technique for Fast Query of Mass Quasi-Real-Time Data in Power Systems",
        "authors": "['Zhijian Qu', 'Zixiao Wang', 'Guanglong Wu']",
        "date": "May 2021",
        "source": "ICAIIS 2021: 2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "abstract": "For the problem that the growing scale of quasi-real-time data in power grids leads to the increasingly prominent obstacles of accessing it by using traditional relational databases, a secondary index technique is proposed by combining cluster framework with the inverted index structure. The processes of queries based on non-primary keys by the designed technique, first reverse locate the primary row key of the corresponding record with the help of inverted index cluster, and then complete the search in the NoSQL database cluster based on values of the primary row key. The designed technique integrates the advantages of inverted index and NoSQL database, and with the support of multi-host cluster, it can quickly complete the mass of quasi-real-time data query processing. To set the millions of engineering data as an example, the results show that the designed technique can reduce the time required to complete non-primary key query processing to hundred milliseconds.",
        "link": "https://dl.acm.org/doi/10.1145/3469213.3470275",
        "category": "Databases"
    },
    {
        "title": "Marrying Top-k with Skyline Queries: Relaxing the Preference Input while Producing Output of Controllable Size",
        "authors": "['Kyriakos Mouratidis', 'Keming Li', 'Bo Tang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The two most common paradigms to identify records of preference in a multi-objective setting rely either on dominance (e.g., the skyline operator) or on a utility function defined over the records' attributes (typically, using a top-k query). Despite their proliferation, each of them has its own palpable drawbacks. Motivated by these drawbacks, we identify three hard requirements for practical decision support, namely, personalization, controllable output size, and flexibility in preference specification. With these requirements as a guide, we combine elements from both paradigms and propose two new operators, ORD and ORU. We perform a qualitative study to demonstrate how they work, and evaluate their performance against adaptations of previous work that mimic their output.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457299",
        "category": "Databases"
    },
    {
        "title": "Data Mining Autosomal Archaeogenetic Data to Determine Minoan Origins",
        "authors": "['Peter Z. Revesz']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "This paper presents a method for data mining archaeogenetic autosomal data. The method is applied to the widely debated topic of the origin of the Bronze Age Minoan culture that existed on the island of Crete from 5000 to 3500 years ago. The data is compared with some Neolithic and early Bronze Age samples from the nearby Cycladic islands, mainland Greece and other Neolithic sites. The method shows that a large component of the Minoan autosomal genomes has sources from the Neolithic areas of northern Greece and the rest of the Balkans and a minor component comes directly from Neolithic Anatolia and the Caucasus.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472178",
        "category": "Databases"
    },
    {
        "title": "The end of Moore's law and the rise of the data processor",
        "authors": "['Niv Dayan', 'Moshe Twitto', 'Yuval Rochman', 'Uri Beitler', 'Itai Ben Zion', 'Edward Bortnikov', 'Shmuel Dashevsky', 'Ofer Frishman', 'Evgeni Ginzburg', 'Igal Maly', 'Avraham (Poza) Meir', 'Mark Mokryn', 'Iddo Naiss', 'Noam Rabinovich']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "With the end of Moore's Law, database architects are turning to hardware accelerators to offload computationally intensive tasks from the CPU. In this paper, we show that accelerators can facilitate far more than just computation: they enable algorithms and data structures that lavishly expand computation in order to optimize for disparate cost metrics. We introduce the Pliops Extreme Data Processor (XDP), a novel storage engine implemented from the ground up using customized hardware. At its core, XDP consists of an accelerated hash table to index the data in storage using less memory and fewer storage accesses for queries than the best alternative. XDP also employs an accelerated compressor, a capacitor, and a lock-free RAID sub-system to minimize storage space and recovery time while minimizing performance penalties. As a result, XDP overcomes cost contentions that have so far been inescapable.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476373",
        "category": "Databases"
    },
    {
        "title": "Automatic Optimization of Matrix Implementations for Distributed Machine Learning and Linear Algebra",
        "authors": "['Shangyu Luo', 'Dimitrije Jankov', 'Binhang Yuan', 'Chris Jermaine']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Machine learning (ML) computations are often expressed using vectors, matrices, or higher-dimensional tensors. Such data structures can have many different implementations, especially in a distributed environment: a matrix could be stored as row or column vectors, tiles of different sizes, or relationally, as a set of (rowIndex, colIndex, value) triples. Many other storage formats are possible. The choice of format can have a profound impact on the performance of a ML computation. In this paper, we propose a framework for automatic optimization of the physical implementation of a complex ML or linear algebra (LA) computation in a distributed environment, develop algorithms for solving this problem, and show, through a prototype on top of a distributed relational database system, that our ideas can radically speed up common ML and LA computations.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457317",
        "category": "Databases"
    },
    {
        "title": "Exploratory analysis of methods for automated classification of clinical diagnoses in Veterinary Medicine",
        "authors": "['Oscar Tamburis', 'Elio Masciari', 'Gerardo Fatone']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "The present work describes the analysis conducted on the diagnoses made during the general physical examinations in the decade 2010–2020, starting from the DB of the EMR previously implemented in the University Veterinary Teaching Hospital at Federico II University of Naples. A decision tree algorithm was implemented to work out a predictive model for an effective recognition of neoplastic diseases and zoonoses for cats and dogs from Campania Region. The results achievable by data mining techniques for what concerns computer aided disease diagnosis and exploration of risk factors and their relations to diseases, show the increasing importance of Veterinary Informatics within the wider field of Biomedical and Health Informatics, and in particular its capacity to point out the existing connections between humans, animals, and surrounding environment, according to the One (Digital) Health perspective specifics.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472165",
        "category": "Databases"
    },
    {
        "title": "Effect of reverse logistics in the Latin American context",
        "authors": "['Carlos Cueva Clemente', 'Yampier Ramirez Macuri', 'Lucia Maribel Bautista Zúñiga']",
        "date": "January 2022",
        "source": "ICCMB '22: Proceedings of the 2022 5th International Conference on Computers in Management and Business",
        "abstract": "The objective of this research work is to analyze and present the effects of implementing reverse logistics in Latin American companies. This research was developed through a qualitative and quantitative review system of articles from studies found in different databases such as ALICIA, SCIELO, GOOGLE ACADEMICO, RENATI, DOAJ, REDALYC and SCOPUS, under exclusion and inclusion criteria such as language, year of publication, keywords objectives, etc., which were aligned with the specific objective of the investigation. From the results it was obtained that 18% mention that reverse logistics improved the company's image, while 15% reduced its production costs, 14% obtained higher profit margins,",
        "link": "https://dl.acm.org/doi/10.1145/3512676.3512699",
        "category": "Databases"
    },
    {
        "title": "SkinnerDB: Regret-bounded Query Evaluation via Reinforcement Learning",
        "authors": "['Immanuel Trummer', 'Junxiong Wang', 'Ziyun Wei', 'Deepak Maram', 'Samuel Moseley', 'Saehan Jo', 'Joseph Antonakakis', 'Ankush Rayabhari']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "SkinnerDB uses reinforcement learning for reliable join ordering, exploiting an adaptive processing engine with specialized join algorithms and data structures. It maintains no data statistics and uses no cost or cardinality models. Also, it uses no training workloads nor does it try to link the current query to seemingly similar queries in the past. Instead, it uses reinforcement learning to learn optimal join orders from scratch during the execution of the current query. To that purpose, it divides the execution of a query into many small time slices. Different join orders are tried in different time slices. SkinnerDB merges result tuples generated according to different join orders until a complete query result is obtained. By measuring execution progress per time slice, it identifies promising join orders as execution proceeds.Along with SkinnerDB, we introduce a new quality criterion for query execution strategies. We upper-bound expected execution cost regret, i.e., the expected amount of execution cost wasted due to sub-optimal join order choices. SkinnerDB features multiple execution strategies that are optimized for that criterion. Some of them can be executed on top of existing database systems. For maximal performance, we introduce a customized execution engine, facilitating fast join order switching via specialized multi-way join algorithms and tuple representations.We experimentally compare SkinnerDB’s performance against various baselines, including MonetDB, Postgres, and adaptive processing methods. We consider various benchmarks, including the join order benchmark, TPC-H, and JCC-H, as well as benchmark variants with user-defined functions. Overall, the overheads of reliable join ordering are negligible compared to the performance impact of the occasional, catastrophic join order choice.",
        "link": "https://dl.acm.org/doi/10.1145/3464389",
        "category": "Databases"
    },
    {
        "title": "Low-latency compilation of SQL queries to machine code",
        "authors": "['Henning Funke', 'Jens Teubner']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Query compilation has proven to be one of the most efficient query processing techniques. Despite its fast processing speed, the additional compilation times of the technique limit its applicability. This is because the approach is most beneficial only when the improvements in processing time clearly exceed the additional compilation time.Recently the feasibility of query compilers with very low compilation times has been shown. This may prove query compilation as a merely universal approach. In this article and in the corresponding live demo, we show the capabilities of the ReSQL database system, which uses the intermediate representation Flounder IR to achieve very low compilation times. ReSQL reduces the compilation times from SQL to machine code compared to existing LLVM-based techniques by up to 101.1x for real-world analytic queries.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476321",
        "category": "Databases"
    },
    {
        "title": "A Zone-Based Data Lake Architecture for IoT, Small and Big Data",
        "authors": "['Yan Zhao', 'Imen Megdiche', 'Franck Ravat', 'Vincent-nam Dang']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented a web application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472185",
        "category": "Databases"
    },
    {
        "title": "A Unified Deep Model of Learning from both Data and Queries for Cardinality Estimation",
        "authors": "['Peizhi Wu', 'Gao Cong']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Cardinality estimation is a fundamental problem in database systems. To capture the rich joint data distributions of a relational table, most of the existing work either uses data as unsupervised information or uses query workload as supervised information. Very little work has been done to use both types of information, and cannot fully make use of both types of information to learn the joint data distribution. In this work, we aim to close the gap between data-driven and query-driven methods by proposing a new unified deep autoregressive model, UAE, that learns the joint data distribution from both the data and query workload. First, to enable using the supervised query information in the deep autoregressive model, we develop differentiable progressive sampling using the Gumbel-Softmax trick. Second, UAE is able to utilize both types of information to learn the joint data distribution in a single model. Comprehensive experimental results demonstrate that UAE achieves single-digit multiplicative error at tail, better accuracies over state-of-the-art methods, and is both space and time efficient.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452830",
        "category": "Databases"
    },
    {
        "title": "Updatable learned index with precise positions",
        "authors": "['Jiacheng Wu', 'Yong Zhang', 'Shimin Chen', 'Jin Wang', 'Yu Chen', 'Chunxiao Xing']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Index plays an essential role in modern database engines to accelerate the query processing. The new paradigm of \"learned index\" has significantly changed the way of designing index structures in DBMS. The key insight is that indexes could be regarded as learned models that predict the position of a lookup key in the dataset. While such studies show promising results in both lookup time and index size, they cannot efficiently support update operations. Although recent studies have proposed some preliminary approaches to support update, they are at the cost of scarifying the lookup performance as they suffer from the overheads brought by imprecise predictions in the leaf nodes.In this paper, we propose LIPP, a brand new framework of learned index to address such issues. Similar with state-of-the-art learned index structures, LIPP is able to support all kinds of index operations, namely lookup query, range query, insert, delete, update and bulkload. Meanwhile, we overcome the limitations of previous studies by properly extending the tree structure when dealing with update operations so as to eliminate the deviation of location predicted by the models in the leaf nodes. Moreover, we further propose a dynamic adjustment strategy to ensure that the height of the tree index is tightly bounded and provide comprehensive theoretical analysis to illustrate it. We conduct an extensive set of experiments on several real-life and synthetic datasets. The results demonstrate that our method consistently outperforms state-of-the-art solutions, achieving by up to 4X for a broader class of workloads with different index operations.",
        "link": "https://dl.acm.org/doi/10.14778/3457390.3457393",
        "category": "Databases"
    },
    {
        "title": "New Algorithms for Monotone Classification",
        "authors": "['Yufei Tao', 'Yu Wang']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "In \\em monotone classification, the input is a set P of n points in d-dimensional space, where each point carries a label 0 or 1. A point p \\em dominates another point q if the coordinate of p is at least that of q on every dimension. A \\em monotone classifier is a function h mapping each d-dimensional point to $\\0, 1\\ $, subject to the condition that $h(p) \\ge h(q)$ holds whenever p dominates q. The classifier h \\em mis-classifies a point $p \\in P$ if $h(p)$ is different from the label of p. The \\em error of h is the number of points in P mis-classified by h. The objective is to find a monotone classifier with a small error. The problem is fundamental to numerous database applications in entity matching, record linkage, and duplicate detection. This paper studies two variants of the problem. In the first \\em active version, all the labels are hidden in the beginning; an algorithm must pay a unit cost to \\em probe (i.e., reveal) the label of a point in P. We prove that $Ømega(n)$ probes are necessary to find an optimal classifier even in one-dimensional space ($d=1$). On the other hand, given an arbitrary $\\eps > 0$, we show how to obtain (with high probability) a monotone classifier whose error is worse than the optimum by at most a $1 + \\eps$ factor, while probing $\\tO(w/\\eps^2)$ labels, where w is the dominance width of P and $\\tO(.)$ hides a polylogarithmic factor. For constant $\\eps$, the probing cost matches an existing lower bound up to an $\\tO(1)$ factor. In the second \\em passive version, the point labels in P are explicitly given; the goal is to minimize CPU computation in finding an optimal classifier. We show that the problem can be settled in time polynomial to both d and n.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458324",
        "category": "Databases"
    },
    {
        "title": "An RDMA-enabled In-memory Computing Platform for R-tree on Clusters",
        "authors": "['Mengbai Xiao', 'Hao Wang', 'Liang Geng', 'Rubao Lee', 'Xiaodong Zhang']",
        "date": "None",
        "source": "ACM Transactions on Spatial Algorithms and Systems",
        "abstract": "R-tree is a foundational data structure used in spatial databases and scientific databases. With the advancement of networks and computer architectures, in-memory data processing for R-tree in distributed systems has become a common platform. We have observed new performance challenges to process R-tree as the amount of multidimensional datasets become increasingly high. Specifically, an R-tree server can be heavily overloaded while the network and client CPU are lightly loaded, and vice versa.In this article, we present the design and implementation of Catfish, an RDMA-enabled R-tree for low latency and high throughput by adaptively utilizing the available network bandwidth and computing resources to balance the workloads between clients and servers. We design and implement two basic mechanisms of using RDMA for a client-server R-tree data processing system. First, in the fast messaging design, we use RDMA writes to send R-tree requests to the server and let server threads process R-tree requests to achieve low query latency. Second, in the RDMA offloading design, we use RDMA reads to offload tree traversal from the server to the client, which rescues the server as it is overloaded. We further develop an adaptive scheme to effectively switch an R-tree search between fast messaging and RDMA offloading, maximizing the overall performance. Our experiments show that the adaptive solution of Catfish on InfiniBand significantly outperforms R-tree that uses only fast messaging or only RDMA offloading in both latency and throughput. Catfish can also deliver up to one order of magnitude performance over the traditional schemes using TCP/IP on 1 and 40 Gbps Ethernet. We make a strong case to use RDMA to effectively balance workloads in distributed systems for low latency and high throughput.",
        "link": "https://dl.acm.org/doi/10.1145/3503513",
        "category": "Databases"
    },
    {
        "title": "Xenic: SmartNIC-Accelerated Distributed Transactions",
        "authors": "['Henry N. Schuh', 'Weihao Liang', 'Ming Liu', 'Jacob Nelson', 'Arvind Krishnamurthy']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "High-performance distributed transactions require efficient remote operations on database memory and protocol metadata. The high communication cost of this workload calls for hardware acceleration. Recent research has applied RDMA to this end, leveraging the network controller to manipulate host memory without consuming CPU cycles on the target server. However, the basic read/write RDMA primitives demand trade-offs in data structure and protocol design, limiting their benefits. SmartNICs are a flexible alternative for fast distributed transactions, adding programmable compute cores and on-board memory to the network interface. Applying measured performance characteristics, we design Xenic, a SmartNIC-optimized transaction processing system. Xenic applies an asynchronous, aggregated execution model to maximize network and core efficiency. Xenic's co-designed data store achieves low-overhead remote object accesses. Additionally, Xenic uses flexible, point-to-point communication patterns between SmartNICs to minimize transaction commit latency. We compare Xenic against prior RDMA- and RPC-based transaction systems with the TPC-C, Retwis, and Smallbank benchmarks. Our results for the three benchmarks show 2.42x, 2.07x, and 2.21x throughput improvement, 59%, 42%, and 22% latency reduction, while saving 2.3, 8.1, and 10.1 threads per server.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483555",
        "category": "Databases"
    },
    {
        "title": "Evaluating the Data Inconsistency of Open-Source Vulnerability Repositories",
        "authors": "['Yuning Jiang', 'Manfred Jeusfeld', 'Jianguo Ding']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "Modern security practices promote quantitative methods to provide prioritisation insights and support predictive analysis, which is supported by open-source cybersecurity databases such as the Common Vulnerabilities and Exposures (CVE), the National Vulnerability Database (NVD), CERT, and vendor websites. These public repositories provide a way to standardise and share up-to-date vulnerability information, with the purpose to enhance cybersecurity awareness. However, data quality issues of these vulnerability repositories may lead to incorrect prioritisation and misemployment of resources. In this paper, we aim to empirically analyse the data quality impact of vulnerability repositories for actual information technology (IT) and operating technology (OT) systems, especially on data inconsistency. Our case study shows that data inconsistency may misdirect investment of cybersecurity resources. Instead, correlated vulnerability repositories and trustworthiness data verification bring substantial benefits for vulnerability management.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3470093",
        "category": "Databases"
    },
    {
        "title": "Bladerunner: Stream Processing at Scale for a Live View of Backend Data Mutations at the Edge",
        "authors": "['Jeff Barber', 'Ximing Yu', 'Laney Kuenzel Zamore', 'Jerry Lin', 'Vahid Jazayeri', 'Shie Erlich', 'Tony Savor', 'Michael Stumm']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Consider a social media platform with hundreds of millions of online users at any time, utilizing a social graph that has many billions of nodes and edges. The problem this paper addresses is how to provide each user a continuously fresh, up-to-date view of the parts of the social graph they are currently interested in, so as to provide a positive interactive user experience. The problem is challenging because the social graph mutates at a high rate, users change their focus of interest frequently, and some mutations are of interest to many online users. We describe Bladerunner, a system we use at Facebook to deliver relevant social graph updates to user devices efficiently and quickly. The heart of Bladerunner is a set of back-end stream processors that obtain streams of social graph updates and process them on a per application and per-user basis before pushing selected updates to user devices. Separate stream processors are used for each application to enable application-specific customization, complex filtering, aggregation and other message delivery operations on a per-user basis. This strategy minimizes device processing overhead and last-mile bandwidth usage, which are critical given that users are mostly on mobile devices.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483572",
        "category": "Databases"
    },
    {
        "title": "Wikinegata: a knowledge base with interesting negative statements",
        "authors": "['Hiba Arnaout', 'Simon Razniewski', 'Gerhard Weikum', 'Jeff Z. Pan']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Databases about general-world knowledge, so-called knowledge bases (KBs), are important in applications such as search and question answering. Traditionally, although KBs use open world assumption, popular KBs only store positive information, but withhold from taking any stance towards statements not contained in them. In this demo, we show that storing and presenting noteworthy negative statements would be important to overcome current limitations in various use cases. In particular, we introduce the Wiki negata portal, a platform to explore negative statements for Wikidata entities, by implementing a peer-based ranking method for inferring interesting negations in KBs. The demo is available at http://d5demos.mpi-inf.mpg.de/negation.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476350",
        "category": "Databases"
    },
    {
        "title": "VLDB 2021: Designing a Hybrid Conference",
        "authors": "['Philippe Bonnet', 'Xin Luna Dong', 'Felix Naumann', 'Pinar Tözün']",
        "date": "December 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "The 47th International Conference on Very Large Databases (VLDB'21) was held on August 16-20, 2021 as a hybrid conference. It attracted 180 in-person attendees in Copenhagen and 840 remote attendees. In this paper, we describe our key decisions as general chairs and program committee chairs and share the lessons we learned.",
        "link": "https://dl.acm.org/doi/10.1145/3516431.3516447",
        "category": "Databases"
    },
    {
        "title": "Let Trajectories Speak Out the Traffic Bottlenecks",
        "authors": "['Hui Luo', 'Zhifeng Bao', 'Gao Cong', 'J. Shane Culpepper', 'Nguyen Lu Dang Khoa']",
        "date": "None",
        "source": "ACM Transactions on Intelligent Systems and Technology",
        "abstract": "Traffic bottlenecks are a set of road segments that have an unacceptable level of traffic caused by a poor balance between road capacity and traffic volume. A huge volume of trajectory data which captures realtime traffic conditions in road networks provides promising new opportunities to identify the traffic bottlenecks. In this paper, we define this problem as trajectory-driven traffic bottleneck identification: Given a road network R, a trajectory database T, find a representative set of seed edges of size K of traffic bottlenecks that influence the highest number of road segments not in the seed set. We show that this problem is NP-hard and propose a framework to find the traffic bottlenecks as follows. First, a traffic spread model is defined which represents changes in traffic volume for each road segment over time. Then, the traffic diffusion probability between two connected segments and the residual ratio of traffic volume for each segment can be computed using historical trajectory data. We then propose two different algorithmic approaches to solve the problem. The first one is a best-first algorithm BF, with an approximation ratio of 1-1/e. To further accelerate the identification process in larger datasets, we also propose a sampling-based greedy algorithm SG. Finally, comprehensive experiments using three different datasets compare and contrast various solutions, and provide insights into important efficiency and effectiveness trade-offs among the respective methods.",
        "link": "https://dl.acm.org/doi/10.1145/3465058",
        "category": "Databases"
    },
    {
        "title": "Steering Query Optimizers: A Practical Take on Big Data Workloads",
        "authors": "['Parimarjan Negi', 'Matteo Interlandi', 'Ryan Marcus', 'Mohammad Alizadeh', 'Tim Kraska', 'Marc Friedman', 'Alekh Jindal']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In recent years, there has been tremendous interest in research that applies machine learning to database systems. Being one of the most complex components of a DBMS, query optimizers could benefit from adaptive policies that are learned systematically from the data and the query workload. Recent research has brought up novel ideas towards a learned query optimizer, however these ideas have not been evaluated on a commercial query processor or on large scale, real-world workloads. In this paper, we take the approach used by Marcus et al. in Bao and adapt it to SCOPE, a big data system used internally at Microsoft. Along the way, we solve multiple new challenges: we define how optimizer rules affect final query plans by introducing the concept of a rule signature, we devise a pipeline computing interesting rule configurations for recurring jobs, and we define a new learning problem allowing us to apply such interesting rule configurations to previously unseen jobs. We evaluate the efficacy of the approach on production workloads that include 150K daily jobs. Our results show that alternative rule configurations can generate plans with lower costs, and this can translate to runtime latency savings of 7-30% on average and up to 90% for a non trivial subset of the workload.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457568",
        "category": "Databases"
    },
    {
        "title": "Instant Graph Query Recovery on Persistent Memory",
        "authors": "['Alexander Baumstark', 'Philipp Götze', 'Muhammad Attahir Jibril', 'Kai-Uwe Sattler']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "Persistent memory (PMem) - also known as non-volatile memory (NVM) - offers new opportunities not only for the design of data structures and system architectures but also for failure recovery in databases. However, instant recovery can mean not only to bring the system up as fast as possible but also to continue long-running queries which have been interrupted by a system failure. In this work, we discuss how PMem can be utilized to implement query recovery for analytical graph queries. Furthermore, we investigate the trade-off between the overhead of managing the query state in PMem at query runtime as well as the recovery and restart costs.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466011",
        "category": "Databases"
    },
    {
        "title": "Analysis of Eyebrow Motion for Micro-Expression Recognition",
        "authors": "['Sultan Almalki', 'Usman Saeed', 'Mohammed Alkatheiri']",
        "date": "January 2022",
        "source": "ICIGP '22: Proceedings of the 2022 5th International Conference on Image and Graphics Processing",
        "abstract": "Facial expression recognition is a field of emotion recognition that has received considerable attention over the last decades. However, most of the research has been focused on the recognition of macro-expressions that are generally posed and easy to imitate. On the other hand, micro-expressions are much subtler and localized therefore by definition harder to replicate, thus making them valuable for medical diagnosis and criminal investigations. Normally, micro-expressions are detected and recognized by studying the entire face. We believe that the eyebrows contain the most relevant information pertaining to micro-expression recognition. Therefore, in this paper, we propose to recognize facial micro-expressions based solely on the motion of the eyebrow. The proposed method starts with the preprocessing and cropping of image sequences based on the eye area. Next, feature extraction is carried out using Histogram of Oriented Optical Flow (HOOF), and Local Binary Patterns on Three Orthogonal Planes (LBP-TOP). Validation of our methodology is carried out on the SMIC-HS and CASME II databases. We achieved promising recognition accuracy of 54% with magnification and 64% without magnification on the SMIC-HS database. In the case of CASME II database we achieved an accuracy of 57% with magnification, and 59% without magnification.",
        "link": "https://dl.acm.org/doi/10.1145/3512388.3512405",
        "category": "Databases"
    },
    {
        "title": "Investigating Preferred Food Description Practices in Digital Food Journaling",
        "authors": "['Lucas M. Silva', 'Daniel A. Epstein']",
        "date": "June 2021",
        "source": "DIS '21: Proceedings of the 2021 ACM Designing Interactive Systems Conference",
        "abstract": "Journaling of consumed foods through digital devices is a popular self-tracking strategy for weight loss and eating mindfulness. Research has explored modalities, like photos and open-ended text and voice descriptions, to make journaling less burdensome and more descriptive than traditional barcode and database searches. However, less is known about how people prefer to journal foods when less constrained by limitations of databases, natural language processing, and image recognition. We deployed a food journal prototype supporting varied devices and input modalities, which 15 participants used to journal 1008 food logs over two weeks. Participants had diverse strategies for indicating what and how much they ate, varying from ambiguous foods to specifying varieties and using different measurements for clarifying amount. Some strategies were interpretable by natural language food identification and image classification services, while others point to open research questions. We finally discuss opportunities for accounting for variance in food journaling.",
        "link": "https://dl.acm.org/doi/10.1145/3461778.3462145",
        "category": "Databases"
    },
    {
        "title": "A Serverless Framework for Distributed Bulk Metadata Extraction",
        "authors": "['Tyler J. Skluzacek', 'Ryan Wong', 'Zhuozhao Li', 'Ryan Chard', 'Kyle Chard', 'Ian Foster']",
        "date": "June 2021",
        "source": "HPDC '21: Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing",
        "abstract": "We introduce Xtract, an automated and scalable system for bulk metadata extraction from large, distributed research data repositories. Xtract orchestrates the application of metadata extractors to groups of files, determining which extractors to apply to each file and, for each extractor and file, where to execute. A hybrid computing model, built on the funcX federated FaaS platform, enables Xtract to balance tradeoffs between extraction time and data transfer costs by dispatching each extraction task to the most appropriate location. Experiments on a range of clouds and supercomputers show that Xtract can efficiently process multi-million-file repositories by orchestrating the concurrent execution of container-based extractors on thousands of nodes. We highlight the flexibility of Xtract by applying it to a large, semi-curated scientific data repository and to an uncurated scientific Google Drive repository. We show that by remotely orchestrating metadata extraction across decentralized storage and compute nodes, Xtract can process large repositories in 50% of the time it takes just to transfer the same data to a machine within the same computing facility. We also show that when transferring data is necessary (e.g., no local compute is available), Xtract can scale to process files as fast as they are received, even over a multi-GB/s network.",
        "link": "https://dl.acm.org/doi/10.1145/3431379.3460636",
        "category": "Databases"
    },
    {
        "title": "Proposal of the Aesthetic Experience-Oriented Evaluation Framework for Field-recording Sound Retrieval System: Experiments using Acoustic Feature Signatures Based on Multiscale Fractal Dimension",
        "authors": "['Motohiro Sunouchi', 'Masaharu Yoshioka']",
        "date": "March 2021",
        "source": "IVSP '21: Proceedings of the 2021 3rd International Conference on Image, Video and Signal Processing",
        "abstract": "Sound designers and musicians often need to retrieve sound materials based on their similarity to aesthetic hearing experiences from sound databases such as Freesound. This study proposes an aesthetic experience-oriented evaluation framework for a field-recording sound retrieval system, using the sound clips extracted from Freesound. Furthermore, we discuss the features of the framework by analyzing the performance of the similarity search system for field-recording sound material using acoustic feature signatures that are based on the multiscale fractal dimension.",
        "link": "https://dl.acm.org/doi/10.1145/3459212.3459223",
        "category": "Databases"
    },
    {
        "title": "Beyond NVD: Cybersecurity meets the Semantic Web.",
        "authors": "['Raúl Aranovich', 'Muting Wu', 'Dian Yu', 'Katya Katsy', 'Benyamin Ahmadnia', 'Matthew Bishop', 'Vladimir Filkov', 'Kenji Sagae']",
        "date": "October 2021",
        "source": "NSPW '21: Proceedings of the 2021 New Security Paradigms Workshop",
        "abstract": "Cybersecurity experts rely on the knowledge stored in databases like the NVD to do their work, but these are not the only sources of information about threats and vulnerabilities. Much of that information flows through social media channels. In this paper we argue that security experts and general users alike can benefit from the technologies of the Semantic Web, merging heterogeneous sources of knowledge in an ontological representation. We present a system that has an ontology of vulnerabilities at its core, but that is enhanced with NLP tools to identify cybersecurity-related information in social media and to launch queries over heterogeneous data sources. The transformative power of Semantic Web technologies for cybersecurity, which has been proven in the biomedical field, is evaluated and discussed.",
        "link": "https://dl.acm.org/doi/10.1145/3498891.3501259",
        "category": "Databases"
    },
    {
        "title": "Diffusing the Liveness Cues for Face Anti-spoofing",
        "authors": "['Sheng Li', 'Xun Zhu', 'Guorui Feng', 'Xinpeng Zhang', 'Zhenxing Qian']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Face anti-spoofing is an important step for secure face recognition. One of the main challenges is how to learn and build a general classifier that is able to resist various presentation attacks. Recently, the patch-based face anti-spoofing schemes are shown to be able to improve the robustness of the classifier. These schemes extract subtle liveness cues from small local patches independently, which do not fully exploit the correlations among the patches. In this paper, we propose a Patch-based Compact Graph Network (PCGN) to diffuse the subtle liveness cues from all the patches. Firstly, the image is encoded into a compact graph by connecting each node with its backward neighbors. We then propose an asymmetrical updating strategy to update the compact graph. Such a strategy aggregates the node based on whether it is a sender or receiver, which leads to better message-passing. The updated graph is eventually decoded for making the final decision. We conduct the experiments on four public databases with four intra-database protocols and eight cross-database protocols, the results of which demonstrate the effectiveness of our PCGN for face anti-spoofing.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475305",
        "category": "Databases"
    },
    {
        "title": "Colonization of the Internet",
        "authors": "['Bipin C. Desai']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "The internet was introduced to connect computers and allow communication between these computers. It evolved to provide applications such as email, talk and file sharing with the associated system to search. The files were made available, freely, by users. However, the internet was out of the reach of most people since it required equipment and know-how as well as connection to a computer on the internet. One method of connection used an acoustic coupler and an analog phone. With the introduction of the personal computer and higher speed modems, accessing the internet became easier. The introduction of user-friendly graphical interfaces, as well as the convenience and portablility of laptops and smartphones made the internet much more widely accessible for a broad swath of users. A small number of newly established companies, supported by a large amount of venture capital and a lack of regulation have since established a stranglehold on the internet with billions of people using these applications. Their monopolistic practices and exploitation of the open nature of the internet has created a need in the ordinary person to replace the traditional way of communication with what they provide: in exchange for giving up personal information these persons have become dependent on the service provided. Due to the regulatory desert around privacy and ownership of personal electornic data, a handful of massive corporations have expropriated and exploited aggregated and disaggregated personal information. This amounts, we argue, to the colonization of the internet.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472179",
        "category": "Databases"
    },
    {
        "title": "ASdb: a system for classifying owners of autonomous systems",
        "authors": "['Maya Ziv', 'Liz Izhikevich', 'Kimberly Ruth', 'Katherine Izhikevich', 'Zakir Durumeric']",
        "date": "November 2021",
        "source": "IMC '21: Proceedings of the 21st ACM Internet Measurement Conference",
        "abstract": "While Autonomous Systems (ASes) are crucial for routing Internet traffic, organizations that own them are little understood. Regional Internet Registries (RIRs) inconsistently collect, release, and update basic AS organization information (e.g., website), and prior work provides only coarse-grained classification. Bootstrapping from RIR WHOIS data, we build ASdb, a system that uses data from established business intelligence databases and machine learning to accurately categorize ASes at scale. ASdb achieves 96% coverage of ASes, and 93% and 75% accuracy on 17 industry categories and 95 sub-categories, respectively. ASdb creates a more rich, accurate, comprehensive, and maintainable dataset cataloging AS-owning organizations. This system, and resulting dataset, will allow researchers to better understand who owns the Internet, and perform new forms of meaningful analysis and interpretation at scale.",
        "link": "https://dl.acm.org/doi/10.1145/3487552.3487853",
        "category": "Databases"
    },
    {
        "title": "Meta-Analysis of Blended Learning in High School Physics from 2014-2020",
        "authors": "['Jorge Victor Martinez Sales', 'Maricar Sison Prudente', 'Denis Dyvee Errabo']",
        "date": "January 2022",
        "source": "IC4E '22: Proceedings of the 2022 13th International Conference on E-Education, E-Business, E-Management, and E-Learning",
        "abstract": "Blended learning (BL) is a technologically driven approach that relies on multiple modalities in teaching content. The researchers investigated BL by integrating information communication technology (ICT) in high school physics from 2014 to 2020. Twenty-four (24) individual studies were presented where twenty-seven (27) reports were extracted. The random-effects size indicated a significant large ES preferring BL as a promising approach in teaching Physics among high school learners. Favorability towards LMS use in teaching was observed, while a significant ES is seen in both Junior and Senior learners showing preference in BL. Studies with low ES showed a teaching approach that relied on less teacher supervision. Furthermore, publication bias suggests that studies available in the searched online databases primarily published BL panned results.",
        "link": "https://dl.acm.org/doi/10.1145/3514262.3514283",
        "category": "Databases"
    },
    {
        "title": "Technical Perspective: Scaling Dynamic Hash Tables on Real Persistent Memory",
        "authors": "['Kenneth A. Ross']",
        "date": "March 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Byte-addressable persistent memory was considered in the data management community as long ago as 1986. Thatte saw the advantages for programmability in unifying the abstractions of byte-addressable RAM with persistence [2]. Thatte's context was object-oriented databases containing a variety data structures that would be awkward to transform into the block-oriented abstractions provided by typical secondary storage. Thatte's proposed physical instantiation of persistent memory was a disk-backed device, although it is unclear whether such a device was ever built. Thatte recognized the importance of recovery to the overall scheme.",
        "link": "https://dl.acm.org/doi/10.1145/3471485.3471505",
        "category": "Databases"
    },
    {
        "title": "Interactive demonstration of SQLCheck",
        "authors": "['Arthita Ghosh', 'Deven Bansod', 'Arpit Narechania', 'Prashanth Dintyala', 'Su Timurturkan', 'Joy Arulraj']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We will demonstrate a prototype of sqlcheck, a holistic toolchain for automatically finding and fixing anti-patterns in database applications. The advent of modern database-as-a-service platforms has made it easy for developers to quickly create scalable applications. However, it is still challenging for developers to design performant, maintainable, and accurate applications. This is because developers may unknowingly introduce anti-patterns in the application's SQL statements. These anti-patterns are design decisions that are intended to solve a problem, but often lead to other problems by violating fundamental design principles.sqlcheck leverages techniques for automatically: (1) detecting anti-patterns with high accuracy, (2) ranking them based on their impact on performance, maintainability, and accuracy of applications, and (3) suggesting alternative queries and changes to the database design to fix these anti-patterns. We will demonstrate that sqlcheck enables developers to create more performant, maintainable, and accurate applications. We will show the prevalence of these anti-patterns in a large collection of queries and databases collected from open-source repositories.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476343",
        "category": "Databases"
    },
    {
        "title": "CIKM 2020 conference report",
        "authors": "[\"Mathieu D'Aquin\", 'Stefan Dietze']",
        "date": "Spring 2021",
        "source": "ACM SIGWEB Newsletter",
        "abstract": "The 29th ACM International Conference on Information and Knowledge Management (CIKM) was held online from the 19th to the 23rd of October 2020. CIKM is an annual computer science conference, focused on research at the intersection of information retrieval, machine learning, databases as well as semantic and knowledge-based technologies. Since it was first held in the United States in 1992, 28 conferences have been hosted in 9 countries around the world.",
        "link": "https://dl.acm.org/doi/10.1145/3460304.3460305",
        "category": "Databases"
    },
    {
        "title": "Design and Implementation of the Integrated Platform of Housing and Land Investigation Based on Geopackage",
        "authors": "['MEI QING DU', 'PENG SONG', 'DONGLIN LI', 'WEIPENG DING', 'XINFENG LI']",
        "date": "December 2021",
        "source": "EBEE '21: Proceedings of the 2021 3rd International Conference on E-Business and E-commerce Engineering",
        "abstract": "Geopackage is a geographic database implemented by SQLite, which supports a variety of geospatial data storage. It is lightweight, deployment-free and easy to maintain. After comparing the advantages among several mainstream databases and Geopackage database in the application of geographic information industry, we choose Geopackage database in the construction of the integrated platform of housing and land investigation. It realizes the integrated operation of spatial data across platforms and software. The platform has realized the whole process of integrated operation of rural housing information collection, data mapping of housing information, data processing in the office, database creation and output of achievement data, one-stop graphics and data integration processing operation; and the platform changed the working mode of traditional; and developed highlight functions such as multimedia annotation, direct connection of rangefinder, verification, workload statistics, free graffiti, file shooting correction, OCR ID card identification etc. By means of information technology, it solved many pain points of the original works, and increased the accuracy of data, then greatly improved the operation efficiency. Finally, it realized the standardized and intelligent of data management, which provides a reference for the application of Geopackage in the massive data database of smart city in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3510249.3510305",
        "category": "Databases"
    },
    {
        "title": "An In-Depth Benchmarking of Text-to-SQL Systems",
        "authors": "['Orest Gkini', 'Theofilos Belmpas', 'Georgia Koutrika', 'Yannis Ioannidis']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Text-to-SQL systems allow users to explore relational databases by posing free-form queries, alleviating the need for using structured languages, such as SQL. Although numerous systems have been developed so far, existing system evaluations lack in rigour. In this work, we build a text-to-SQL benchmark that covers different classes of queries, and we evaluate the effectiveness of several systems in the field. To evaluate system efficiency, we measure execution time and resource consumption for the different query classes. Our comprehensive evaluation aims at filling in a big gap in understanding the capabilities and boundaries of existing systems and it reveals several open challenges.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452836",
        "category": "Databases"
    },
    {
        "title": "Systematic Review of Virtual Reality Solutions Employing Artificial Intelligence Methods",
        "authors": "['Taina Ribeiro de Oliveira', 'Matheus Moura da Silva', 'Rafael Antonio Nepomuceno Spinasse', 'Gabriel Giesen Ludke', 'Mateus Ruy Soares Gaudio', 'Guilherme Iglesias Rocha Gomes', 'Luan Guio Cotini', 'Daniel Vargens', 'MARCELO QUEIROZ SCHIMIDT', 'Rodrigo Varejao Andreao', 'Mario Mestria']",
        "date": "October 2021",
        "source": "SVR'21: Symposium on Virtual and Augmented Reality",
        "abstract": "This paper first presents a systematic literature review of artificial intelligence (AI) methods used in virtual reality (VR) solutions. Based on the systematic literature review, a methodology for locating existing studies, selecting and evaluating contributions, performing analyses, and synthesizing data was proposed. We used search engines, such as Google Scholar and databases such as Elsevier's Scopus, ACM Digital Library, and IEEE Xplore Digital Library. A set of inclusion and exclusion criteria was used to select documents. The results showed that the AI scientific technique most applied in VR applications is machine learning. The findings revealed several fields adopting real-world applications that employ AI in VR: human–robot interaction, emotion interaction and behavior recognition, education, agriculture, transport, manufacturing, and health.",
        "link": "https://dl.acm.org/doi/10.1145/3488162.3488209",
        "category": "Databases"
    },
    {
        "title": "Systematic Review of Virtual Reality Solutions Employing Artificial Intelligence Methods",
        "authors": "['Taina Ribeiro de Oliveira', 'Matheus Moura da Silva', 'Rafael Antonio Nepomuceno Spinasse', 'Gabriel Giesen Ludke', 'Mateus Ruy Soares Gaudio', 'Guilherme Iglesias Rocha Gomes', 'Luan Guio Cotini', 'Daniel Vargens', 'MARCELO QUEIROZ SCHIMIDT', 'Rodrigo Varejao Andreao', 'Mario Mestria']",
        "date": "October 2021",
        "source": "SVR'21: Symposium on Virtual and Augmented Reality",
        "abstract": "This paper first presents a systematic literature review of artificial intelligence (AI) methods used in virtual reality (VR) solutions. Based on the systematic literature review, a methodology for locating existing studies, selecting and evaluating contributions, performing analyses, and synthesizing data was proposed. We used search engines, such as Google Scholar and databases such as Elsevier's Scopus, ACM Digital Library, and IEEE Xplore Digital Library. A set of inclusion and exclusion criteria was used to select documents. The results showed that the AI scientific technique most applied in VR applications is machine learning. The findings revealed several fields adopting real-world applications that employ AI in VR: human–robot interaction, emotion interaction and behavior recognition, education, agriculture, transport, manufacturing, and health.",
        "link": "https://dl.acm.org/doi/10.1145/3488162.3488209",
        "category": "Databases"
    },
    {
        "title": "MAMBO - Indexing Dead Space to Accelerate Spatial Queries✱",
        "authors": "['Giannis Evagorou', 'Thomas Heinis']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "With the increasing size and prevalence of spatial data across applications, efficiently indexing it becomes key. Minimum bounding boxes (MBBs) — i.e., axis-aligned rectangles that minimally enclose an object — used as approximations for complex geometric objects have become crucial for spatial indexes. MBBs succinctly summarize complex spatial objects and thus allow for an efficient filtering stage thanks to faster intersection tests. However, they introduce dead-space, i.e., space that is indexed but contains no spatial objects. Querying dead space gives no result but reads data from disk thus slowing down query execution unnecessarily.  In this paper, we propose MaMBo (Meshed MBb), a grid-based data structure to index dead space in addition to an index of the spatial objects. We augment intersection operations of established indexes to consult our data structure while executing queries, thereby avoiding retrieval of unnecessary data from disk, i.e., data which only contains dead space. As our experiments show, we can significantly reduce I/O — the major overhead for disk-resident datasets — by over 50% when using MaMBo with an R-Tree.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468804",
        "category": "Databases"
    },
    {
        "title": "An Unsupervised and Robust Line and Word Segmentation Method for Handwritten and Degraded Printed Document",
        "authors": "['Jayati Mukherjee', 'Swapan K. Parui', 'Utpal Roy']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "Segmentation of text lines and words in an unconstrained handwritten or a machine-printed degraded document is a challenging document analysis problem due to the heterogeneity in the document structure. Often there is un-even skew between the lines and also broken words in a document. In this article, the contribution lies in segmentation of a document page image into lines and words. We have proposed an unsupervised, robust, and simple statistical method to segment a document image that is either handwritten or machine-printed (degraded or otherwise). In our proposed method, the segmentation is treated as a two-class classification problem. The classification is done by considering the distribution of gap size (between lines and between words) in a binary page image. Our method is very simple and easy to implement. Other than the binarization of the input image, no pre-processing is necessary. There is no need of high computational resources. The proposed method is unsupervised in the sense that no annotated document page images are necessary. Thus, the issue of a training database does not arise. In fact, given a document page image, the parameters that are needed for segmentation of text lines and words are learned in an unsupervised manner. We have applied our proposed method on several popular publicly available handwritten and machine-printed datasets (ISIDDI, IAM-Hist, IAM, PBOK) of different Indian and other languages containing different fonts. Several experimental results are presented to show the effectiveness and robustness of our method. We have experimented on ICDAR-2013 handwriting segmentation contest dataset and our method outperforms the winning method. In addition to this, we have suggested a quantitative measure to compute the level of degradation of a document page image.",
        "link": "https://dl.acm.org/doi/10.1145/3474118",
        "category": "Databases"
    },
    {
        "title": "Exquisitor at the Lifelog Search Challenge 2021: Relationships Between Semantic Classifiers",
        "authors": "['Omar Shahbaz Khan', 'Aaron Duane', 'Björn Þór Jónsson', 'Jan Zahálka', 'Stevan Rudinac', 'Marcel Worring']",
        "date": "August 2021",
        "source": "LSC '21: Proceedings of the 4th Annual on Lifelog Search Challenge",
        "abstract": "Exquisitor is a scalable media exploration system based on interactive learning. To satisfy a user's information need, the system asks the user for feedback on media items and uses that feedback to interactively construct a classifier, that is in turn used to identify the next potentially relevant set of media items. To facilitate effective exploration of a collection, the system offers filters to narrow the scope of exploration, search functionality for finding good examples for the classifier, and support for timeline browsing of videos or image sequences. For this year's Lifelog Search Challenge, we have enhanced Exquisitor to better support tasks with a temporal component, by adding features that allow the user to build multiple classifiers and merge the classifier results, using both traditional set operators and advanced temporal operators.",
        "link": "https://dl.acm.org/doi/10.1145/3463948.3469255",
        "category": "Databases"
    },
    {
        "title": "Robust Image Representation via Low Rank Locality Preserving Projection",
        "authors": "['Shuai Yin', 'Yanfeng Sun', 'Junbin Gao', 'Yongli Hu', 'Boyue Wang', 'Baocai Yin']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Locality preserving projection (LPP) is a dimensionality reduction algorithm preserving the neighhorhood graph structure of data. However, the conventional LPP is sensitive to outliers existing in data. This article proposes a novel low-rank LPP model called LR-LPP. In this new model, original data are decomposed into the clean intrinsic component and noise component. Then the projective matrix is learned based on the clean intrinsic component which is encoded in low-rank features. The noise component is constrained by the ℓ1-norm which is more robust to outliers. Finally, LR-LPP model is extended to LR-FLPP in which low-dimensional feature is measured by F-norm. LR-FLPP will reduce aggregated error and weaken the effect of outliers, which will make the proposed LR-FLPP even more robust for outliers. The experimental results on public image databases demonstrate the effectiveness of the proposed LR-LPP and LR-FLPP.",
        "link": "https://dl.acm.org/doi/10.1145/3434768",
        "category": "Databases"
    },
    {
        "title": "No-Reference Video Quality Assessment with Heterogeneous Knowledge Ensemble",
        "authors": "['Jinjian Wu', 'Yongxu Liu', 'Leida Li', 'Weisheng Dong', 'Guangming Shi']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Blind assessment of video quality is still challenging even in this deep learning era. The limited number of samples in existing databases is insufficient to learn a good feature extractor for video quality assessment (VQA), while manually labeling a larger database with subjective perception is very labor-intensive and time-consuming. To relieve such difficulty, we first collect 3589 high-quality video clips as the reference and build a large VQA dataset. The dataset contains more than 300K samples degraded by various distortion types due to compression and transmission error, and provides weak labels for each distorted sample with several full-reference VQA algorithms. To learn effective representation from the weakly labeled data, we alleviate the bias of single weak label (i.e., single knowledge) via learning from multiple heterogeneous knowledge. To this end, we propose a novel no-reference VQA (NR-VQA) method with HEterogeneous Knowledge Ensemble (HEKE). Comparing to learning from single knowledge, HEKE can theoretically reach a lower infimum, and learn richer representation due to the heterogeneity. Extensive experimental results show that the proposed HEKE outperforms existing NR-VQA methods, and achieves the state-of-the-art performance. The source code will be available at https://github.com/Sissuire/BVQA-HEKE.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475550",
        "category": "Databases"
    },
    {
        "title": "Research on the Application of Computer Big Data Retrieval Technology and Cloud Computing in Internet Course",
        "authors": "['Peisheng Li']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "The current Internet teaching resource retrieval system has problems such as large information retrieval errors and low work efficiency. In order to obtain ideal Internet teaching resource information retrieval results, the article designs an Internet teaching resource information retrieval system based on big data analysis technology. The article collects a large amount of data from digital Internet teaching resource information retrieval, and introduces big data analysis technology to establish an Internet teaching resource information retrieval model. Finally, specific examples of Internet teaching resource information retrieval are used to analyze the superiority of the proposed model. Experimental results show that this technology is suitable for organizing, managing and querying massive amounts of Internet teaching resource data, and the query performance is better than traditional relational databases.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3501155",
        "category": "Databases"
    },
    {
        "title": "The Aurora operating system: revisiting the single level store",
        "authors": "['Emil Tsalapatis', 'Ryan Hancock', 'Tavian Barnes', 'Ali José Mashtizadeh']",
        "date": "June 2021",
        "source": "HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems",
        "abstract": "Applications on modern operating systems manage their ephemeral state in memory, and persistent state on disk. Ensuring consistency between them is a source of significant developer effort, yet still a source of significant bugs in mature applications. We present the Aurora single level store (SLS), an OS that simplifies persistence by automatically persisting all traditionally ephemeral application state. With recent storage hardware like NVMe SSDs and NVDIMMs, Aurora is able to continuously checkpoint entire applications with millisecond granularity. Aurora is the first full POSIX single level store to handle complex applications ranging from databases to web browsers. Moreover, by providing new ways to interact with and manipulate application state, it enables applications to provide features that would otherwise be prohibitively difficult to implement. We argue that this provides strong evidence that manipulation and persistence of application state naturally belong in an operating system.",
        "link": "https://dl.acm.org/doi/10.1145/3458336.3465285",
        "category": "Databases"
    },
    {
        "title": "A Temporal RDF Model for Multi-grained Time Information Modeling",
        "authors": "['Haixia Li', 'Li Yan']",
        "date": "July 2021",
        "source": "DSIT 2021: 2021 4th International Conference on Data Science and Information Technology",
        "abstract": "With the rapid increase of temporal data, how to represent and manage temporal data has become a research issue worth digging in. To better represent temporal data, there have been many works on adding the dimension to RDF or other data representations such as relational databases. However, few works pay attention to the problem of updating time information in the form of triple elements in RDF. Note that this not only makes it easy to express that the relationship between entities is effective over a period of time, but also makes it easy to express that the entities themselves are effective in the time. A model of temporal data representation based on RDF is proposed in this paper which not only considering the validity of triples, but also considering the temporal validity of the entities themselves within the triples.",
        "link": "https://dl.acm.org/doi/10.1145/3478905.3478908",
        "category": "Databases"
    },
    {
        "title": "Speech Emotion Recognition Using MFCC and Wide Residual Network",
        "authors": "['Manas Gupta', 'Satish Chandra']",
        "date": "August 2021",
        "source": "IC3-2021: Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing",
        "abstract": "Emotion recognition from speech has been a topic of research from many years due to its importance in human-computer interaction. While a lot of work has been done upon recognizing emotions through facial expressions, recognition of emotions through speech is still a challenging task in Machine Learning due to the obscure knowledge about the effectiveness of different speech features. In this work, Mel-frequency cepstral coefficients (MFCCs) has been used as a feature extractor for speech files. Further, classification of speech signals has been done using Convolution Neural Network (CNN) in the form of Wide Residual Network (WRN) followed by a Dense Neural Network (DNN). To train and test this approach we used Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and Toronto Emotional Speech Set (TESS) databases together. Results show that the proposed approach is gives an accuracy of 90.09% in recognizing emotions from speech into 8 categories.",
        "link": "https://dl.acm.org/doi/10.1145/3474124.3474171",
        "category": "Databases"
    },
    {
        "title": "Neural PathSim for Inductive Similarity Search in Heterogeneous Information Networks",
        "authors": "['Wenyi Xiao', 'Huan Zhao', 'Vincent W. Zheng', 'Yangqiu Song']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "PathSim is a widely used meta-path-based similarity in heterogeneous information networks. Numerous applications rely on the computation of PathSim, including similarity search and clustering. Computing PathSim scores on large graphs is computationally challenging due to its high time and storage complexity. In this paper, we propose to transform the problem of approximating the ground truth PathSim scores into a learning problem. We design an encoder-decoder based framework, NeuPath, where the algorithmic structure of PathSim is considered. Specifically, the encoder module identifies Top T optimized path instances, which can approximate the ground truth PathSim, and maps each path instance to an embedding vector. The decoder transforms each embedding vector into a scalar respectively, which identifies the similarity score. We perform extensive experiments on two real-world datasets in different domains, ACM and IMDB. Our results demonstrate that NeuPath performs better than state-of-the-art baselines in the PathSim approximation task and similarity search task.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3482454",
        "category": "Databases"
    },
    {
        "title": "The laplace mechanism has optimal utility for differential privacy over continuous queries",
        "authors": "['Natasha Fernandes', 'Annabelle McIver', 'Carroll Morgan']",
        "date": "June 2021",
        "source": "LICS '21: Proceedings of the 36th Annual ACM/IEEE Symposium on Logic in Computer Science",
        "abstract": "Differential Privacy protects individuals' data when statistical queries are published from aggregated databases: applying \"obfuscating\" mechanisms to the query results makes the released information less specific but, unavoidably, also decreases its utility. Yet it has been shown that for discrete data (e.g. counting queries), a mandated degree of privacy and a reasonable interpretation of loss of utility, the Geometric obfuscating mechanism is optimal: it loses as little utility as possible [Ghosh et al.[1]]. For continuous query results however (e.g. real numbers) the optimality result does not hold. Our contribution here is to show that optimality is regained by using the Laplace mechanism for the obfuscation. The technical apparatus involved includes the earlier discrete result [Ghosh op. cit.], recent work on abstract channels and their geometric representation as hyper-distributions [Alvim et al.[2]], and the dual interpretations of distance between distributions provided by the Kantorovich-Rubinstein Theorem.",
        "link": "https://dl.acm.org/doi/10.1109/LICS52264.2021.9470718",
        "category": "Databases"
    },
    {
        "title": "Biomedical Image Segmentation Based on Classification Supervision",
        "authors": "['Kaixuan Guo', 'Jun Wu', 'Wan Wan', 'Longfei Li', 'Tao Wang', 'Xingliang Zhu', 'Lei Qu']",
        "date": "May 2021",
        "source": "ICBBT '21: Proceedings of the 2021 13th International Conference on Bioinformatics and Biomedical Technology",
        "abstract": "Convolutional neural networks (CNN) has been widely used in the biomedical image segmentation (BIS) for their remarkable feature representation capability. However, there are often segmentation errors and missing segmentation problems in biomedical image segmentation based on deep learning. In this paper, we propose a full convolutional neural network, which is assisted by classification supervision based on segmentation network. The algorithm first obtains a segmentation result through a basic segmentation network. Then a Classification Supervision Module (CSM) is designed to enable the network to judge whether each slicer contains lesions from the perspective of classification. In this way we allow the network to take advantage of more global information. Experimental results on several available databases demonstrate the effectiveness and advancement of the proposed method.",
        "link": "https://dl.acm.org/doi/10.1145/3473258.3473262",
        "category": "Databases"
    },
    {
        "title": "Real Market Basket Analysis using Apriori and Frequent Pattern Tree Algorithm",
        "authors": "['Michael Albert Christian', 'Nathanael Nathanael', 'Annisa Mauliani', 'Ariani Indrawati', 'Lindung Parningotan Manik', 'Zaenal Akbar']",
        "date": "October 2021",
        "source": "IC3INA '21: Proceedings of the 2021 International Conference on Computer, Control, Informatics and Its Applications",
        "abstract": "Recently, data mining has been implemented in various fields, including business and telecommunications. Data mining is a technique for extracting and detecting patterns in massive data sets that combines machine learning, statistics, and database systems. One of the most important use-cases in data mining is finding the high-frequency patterns between the set of itemset called association rules. Association rule mining is a well-researched technique for finding some relations between variables in large databases. This paper aims to measure the performance of the Apriori and Frequent Pattern Tree algorithms by comparing them using several points of comparison. Then we compared the outputs, whether they produce the same or different rules, to find out whether the way the two algorithms work is similar or not. After that, we looked for the itemsets that best match the reality in the market by giving them to a user who had transaction data from his spare parts shop.",
        "link": "https://dl.acm.org/doi/10.1145/3489088.3489133",
        "category": "Databases"
    },
    {
        "title": "On Modeling Influence Maximization in Social Activity Networks under General Settings",
        "authors": "['Rui Wang', 'Yongkun Li', 'Shuai Lin', 'Hong Xie', 'Yinlong Xu', 'John C. S. Lui']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Finding the set of most influential users in online social networks (OSNs) to trigger the largest influence cascade is meaningful, e.g., companies may leverage the “word-of-mouth” effect to trigger a large cascade of purchases by offering free samples/discounts to those most influential users. This task is usually modeled as an influence maximization problem, and it has been widely studied in the past decade. However, considering that users in OSNs may participate in various online activities, e.g., joining discussion groups and commenting on same pages or products, influence diffusion through online activities becomes even more significant. In this article, we study the impact of online activities by formulating social-activity networks which contain both users and online activities, and thus induce two types of weighted edges, i.e., edges between users and edges between users and activities. To address the computation challenge, we define an influence centrality via random walks, and use the Monte Carlo framework to efficiently estimate the centrality. Furthermore, we develop a greedy-based algorithm with novel optimizations to find the most influential users for node recommendation. Experiments on real-world datasets show that our approach is very computationally efficient under different influence models, and also achieves larger influence spread by considering online activities.",
        "link": "https://dl.acm.org/doi/10.1145/3451218",
        "category": "Databases"
    },
    {
        "title": "Fauce: fast and accurate deep ensembles with uncertainty for cardinality estimation",
        "authors": "['Jie Liu', 'Wenqian Dong', 'Qingqing Zhou', 'Dong Li']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Cardinality estimation is a fundamental and critical problem in databases. Recently, many estimators based on deep learning have been proposed to solve this problem and they have achieved promising results. However, these estimators struggle to provide accurate results for complex queries, due to not capturing real inter-column and inter-table correlations. Furthermore, none of these estimators contain the uncertainty information about their estimations. In this paper, we present a join cardinality estimator called Fauce. Fauce learns the correlations across all columns and all tables in the database. It also contains the uncertainty information of each estimation. Among all studied learned estimators, our results are promising: (1) Fauce is a light-weight estimator, it has 10× faster inference speed than the state of the art estimator; (2) Fauce is robust to the complex queries, it provides 1.3×--6.7× smaller estimation errors for complex queries compared with the state of the art estimator; (3) To the best of our knowledge, Fauce is the first estimator that incorporates uncertainty information for cardinality estimation into a deep learning model.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476254",
        "category": "Databases"
    },
    {
        "title": "Activity-aware privacy protection for smart water meters",
        "authors": "['Rachel Cardell-Oliver', 'Harrison Carter-Turner']",
        "date": "November 2021",
        "source": "BuildSys '21: Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
        "abstract": "Identifying water end-uses from household meter readings yields valuable insights of commercial and environmental value for both customers and water providers. But smart water meter data may expose sensitive information about the activities of metered households. This paper considers the case where a water provider wishes to publish a database of household water meter traces to be used by water analysts for research and planning purposes. We consider the risks to privacy should an adversary gain access to this database, with the threat of uniquely identifying a household of interest and exposing their water use activities. Previous work shows that the effectiveness of privacy protection techniques is strongly domain and application dependent, but few privacy studies have considered smart water metering. This paper introduces a framework for activity-aware privacy protection for databases of household smart water meters and evaluates its effectiveness using real-world and synthetic datasets. We found that privacy-protection is strongly dependent on the type of activity. Aggregating readings protects households from disclosure of small scale and common activities. For individualistic, infrequent activities such as garden watering or clothes washing, we found that data-aware sampling of households and seasons offers the best protection.",
        "link": "https://dl.acm.org/doi/10.1145/3486611.3486650",
        "category": "Databases"
    },
    {
        "title": "Locally Adaptive Structure and Texture Similarity for Image Quality Assessment",
        "authors": "['Keyan Ding', 'Yi Liu', 'Xueyi Zou', 'Shiqi Wang', 'Kede Ma']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "The latest advances in full-reference image quality assessment (IQA) involve unifying structure and texture similarity based on deep representations. The resulting Deep Image Structure and Texture Similarity (DISTS) metric, however, makes rather global quality measurements, ignoring the fact that natural photographic images are locally structured and textured across space and scale. In this paper, we describe a locally adaptive structure and texture similarity index for full-reference IQA, which we term A-DISTS. Specifically, we rely on a single statistical feature, namely the dispersion index, to localize texture regions at different scales. The estimated probability (of one patch being texture) is in turn used to adaptively pool local structure and texture measurements. The resulting A-DISTS is adapted to local image content, and is free of expensive human perceptual scores for supervised training. We demonstrate the advantages of A-DISTS in terms of correlation with human data on ten IQA databases and optimization of single image super-resolution methods.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475419",
        "category": "Databases"
    },
    {
        "title": "University Social Responsibility in the context of Social Appropriation of Knowledge: A Systematic Literature Mapping Approach",
        "authors": "['Elsa Nadia Ontiveros-Ortíz', 'María Soledad Ramírez-Montoya']",
        "date": "October 2021",
        "source": "TEEM'21: Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM&apos;21)",
        "abstract": "University Social Responsibility (USR) should be seen beyond a yearly report and be understood as a process of Social Appropriation of Knowledge (SAK) though different stakeholders around higher education. The object of this literature mapping is to analyze the recent developments regarding USR regarding SAK in the higher education context where different stakeholders are involved. Following a PRISMA flow mapping process, six general-designed questions of 286 articles are revised for this paper as to discover how SAK happens within universities. All articles were retrieved from Scopus and Web of Sciences databases. The findings include a USR field divided between models´ approach and experience such as service-learning. Furthermore, sustainability and the environment take a special role within USR in the context of SAK that must be analyzed by itself as there is a wide interest in the topic.",
        "link": "https://dl.acm.org/doi/10.1145/3486011.3486549",
        "category": "Databases"
    },
    {
        "title": "GNNfam: utilizing sparsity in protein family predictions using graph neural networks",
        "authors": "['Anuj Godase', 'Md. Khaledur Rahman', 'Ariful Azad']",
        "date": "August 2021",
        "source": "BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "abstract": "We present GNNfam, a pipeline for predicting protein families from protein sequences. GNNfam aligns proteins using pairwise sequence aligner LAST, creates a sparse graph based on the alignment scores, and employs graph neural networks (GNNs) to predict protein families. Unlike alignment-free deep learning methods such as DeepFam, GNNfam can control the sparsity of the protein similarity graph to prune uninformative edges. We develop three pruning strategies to improve the prediction accuracy, convergence, and running time of the downstream graph neural networks. We also demonstrate that semi-supervised GNNs outperform traditional graph clustering-based methods by a large margin. When trained with three labeled sequence datasets from the SCOPe and COG databases, GNNfam achieves more than 90% test accuracy when predicting protein families and performs significantly better than clustering, embedding and other deep learning methods. GNNfam is available at https://github.com/HipGraph/GNNfam.",
        "link": "https://dl.acm.org/doi/10.1145/3459930.3469538",
        "category": "Databases"
    },
    {
        "title": "Dissecting self-describing data formats to enable advanced querying of file metadata",
        "authors": "['Kira Duwe', 'Michael Kuhn']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "In times of continuously growing data sizes, performing insightful analysis is increasingly difficult. I/O libraries such as NetCDF and ADIOS2 offer options to manage additional metadata to make the data retrieval more efficient. However, queries on this metadata are difficult as it is currently stored inside the corresponding self-describing data formats. By replacing the file system underneath with the storage framework JULEA, we can use dedicated backends for key-value and object stores, as well as databases. Splitting the BP file content into file metadata and file data enables novel and highly efficient data management techniques without creating redundancy. We have kept our approach transparent to the application layer by implementing a custom ADIOS2 engine. Moreover, our data analysis interface allows speeding up metadata queries by a factor of up to 60,000 in comparison to the ADIOS2 API and data formats.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463778",
        "category": "Databases"
    },
    {
        "title": "Mixing Modalities of 3D Sketching and Speech for Interactive Model Retrieval in Virtual Reality",
        "authors": "['Daniele Giunchi', 'Alejandro Sztrajman', 'Stuart James', 'Anthony Steed']",
        "date": "June 2021",
        "source": "IMX '21: Proceedings of the 2021 ACM International Conference on Interactive Media Experiences",
        "abstract": "Sketch and speech are intuitive interaction methods that convey complementary information and have been independently used for 3D model retrieval in virtual environments. While sketch has been shown to be an effective retrieval method, not all collections are easily navigable using this modality alone. We design a new challenging database for sketch comprised of 3D chairs where each of the components (arms, legs, seat, back) are independently colored. To overcome this, we implement a multimodal interface for querying 3D model databases within a virtual environment. We base the sketch on the state-of-the-art for 3D Sketch Retrieval, and use a Wizard-of-Oz style experiment to process the voice input. In this way, we avoid the complexities of natural language processing which frequently requires fine-tuning to be robust. We conduct two user studies and show that hybrid search strategies emerge from the combination of interactions, fostering the advantages provided by both modalities.",
        "link": "https://dl.acm.org/doi/10.1145/3452918.3458806",
        "category": "Databases"
    },
    {
        "title": "Limited Associativity Makes Concurrent Software Caches a Breeze",
        "authors": "['Dolev Adas', 'Gil Einziger', 'Roy Friedman']",
        "date": "January 2022",
        "source": "ICDCN '22: Proceedings of the 23rd International Conference on Distributed Computing and Networking",
        "abstract": "Software caches optimize the performance of diverse storage systems, databases and other software systems. Existing works on software caches automatically resort to fully associative cache designs. Our work shows that limited associativity caches are a promising direction for concurrent software caches. Specifically, we demonstrate that limited associativity enables simple yet efficient realizations of multiple cache management schemes that can be trivially parallelized. We show that the obtained hit ratio is usually similar to fully associative caches of the same management policy, but the throughput is improved by up to x5 compared to production-grade caching libraries, especially in multi-threaded executions.",
        "link": "https://dl.acm.org/doi/10.1145/3491003.3491013",
        "category": "Databases"
    },
    {
        "title": "Long Short-term Convolutional Transformer for No-Reference Video Quality Assessment",
        "authors": "['Junyong You']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "No-reference video quality assessment has not been widely benefited from deep learning, mainly due to the complexity, diversity and particularity of modelling spatial and temporal characteristics in quality assessment scenario. Image quality assessment (IQA) performed on video frames plays a key role in NR-VQA. A perceptual hierarchical network (PHIQNet) with an integrated attention module is first proposed that can appropriately simulate the visual mechanisms of contrast sensitivity and selective attention in IQA. Subsequently, perceptual quality features of video frames derived from PHIQNet are fed into a long short-term convolutional Transformer (LSCT) architecture to predict the perceived video quality. LSCT consists of CNN formulating quality features in video frames within short-term units that are then fed into Transformer to capture the long-range dependence and attention allocation over temporal units. Such architecture is in line with the intrinsic properties of VQA. Experimental results on publicly available video quality databases have demonstrated that the LSCT architecture based on PHIQNet significantly outperforms state-of-the-art video quality models.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475368",
        "category": "Databases"
    },
    {
        "title": "Face Template Protection through Residual Learning Based Error-Correcting Codes",
        "authors": "['Junwei Zhou', 'Delong Shang', 'Huile Lang', 'Guodong Ye', 'Zhe Xia']",
        "date": "August 2021",
        "source": "ICCCV '21: Proceedings of the 4th International Conference on Control and Computer Vision",
        "abstract": "The leakage of the face template leads to severe security problems since the facial image is unique and irreplaceable to each individual. Many researchers have been devoted to protecting the face template. Nevertheless, to achieve high security for the face template, partial matching accuracy is usually sacrificed. The main challenge of this problem is the low inter-user variations and high intra-user variations of facial images. In this work, we propose a method integrating residual learning and error-correcting codes for face template protection. In particular, the proposed method consists of two major components: (a) a deep residual network component mapping facial images to polar codewords assigned to users, and (b) a polar decoder reducing noise brought by high intra-user variations in the predicted codewords. The proposed method is evaluated on extended Yale B, CMU-PIE, and FEI databases. It provides high security of face template and achieves a high (100%) genuine accept rate at a low false accept rate (0%) simultaneously, which outperforms most state-of-the-arts.",
        "link": "https://dl.acm.org/doi/10.1145/3484274.3484292",
        "category": "Databases"
    },
    {
        "title": "Literature teaching methods: A systematic mapping",
        "authors": "['Rosa Núñez-Pacheco', 'Evelyn-Paola Guillén-Chávez', 'Aymé Barreda-Parra', 'Mª Cruz Sánchez-Gómez']",
        "date": "October 2021",
        "source": "TEEM'21: Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM&apos;21)",
        "abstract": "This research is a study from a qualitative perspective. Its purpose is to carry out a systematic mapping of methods and methodological approaches related to the teaching of literature. The search was carried out in Scopus and Web of Science databases in the period 2016-2021. The results of this study show that the most published countries on this topic are Turkey and Malaysia. In addition, the methodological approach used is qualitative predominantly. Besides, there is a strong link between the teaching of literature with the use of technology and the teaching of English as a foreign language. Consequently, the methods used are appropriate to these purposes. Finally, it highlights the aesthetic, civic, and moral values that emerge from the teaching of literature.",
        "link": "https://dl.acm.org/doi/10.1145/3486011.3486423",
        "category": "Databases"
    },
    {
        "title": "VSS: A Storage System for Video Analytics",
        "authors": "['Brandon Haynes', 'Maureen Daum', 'Dong He', 'Amrita Mazumdar', 'Magdalena Balazinska', 'Alvin Cheung', 'Luis Ceze']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We present a new video storage system (VSS) designed to decouple high-level video operations from the low-level details required to store and efficiently retrieve video data. VSS is designed to be the storage subsystem of a video data management system (VDBMS) and is responsible for: (1) transparently and automatically arranging the data on disk in an efficient, granular format; (2) caching frequently-retrieved regions in the most useful formats; and (3) eliminating redundancies found in videos captured from multiple cameras with overlapping fields of view. Our results suggest that VSS can improve VDBMS read performance by up to 54%, reduce storage costs by up to 45%, and enable developers to focus on application logic rather than video storage and retrieval.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3459242",
        "category": "Databases"
    },
    {
        "title": "On generalizations of some distance based classifiers for HDLSS data",
        "authors": "['Sarbojit Roy', 'Soham Sarkar', 'Subhajit Dutta', 'Anil K. Ghosh']",
        "date": "None",
        "source": "The Journal of Machine Learning Research",
        "abstract": "In high dimension, low sample size (HDLSS) settings, classifiers based on Euclidean distances like the nearest neighbor classifier and the average distance classifier perform quite poorly if differences between locations of the underlying populations get masked by scale differences. To rectify this problem, several modifications of these classifiers have been proposed in the literature. However, existing methods are confined to location and scale differences only, and they often fail to discriminate among populations differing outside of the first two moments. In this article, we propose some simple transformations of these classifiers resulting in improved performance even when the underlying populations have the same location and scale. We further propose a generalization of these classifiers based on the idea of grouping of variables. High-dimensional behavior of the proposed classifiers is studied theoretically. Numerical experiments with a variety of simulated examples as well as an extensive analysis of benchmark data sets from three different databases exhibit advantages of the proposed methods.",
        "link": "https://dl.acm.org/doi/10.5555/3586589.3586603",
        "category": "Databases"
    },
    {
        "title": "Deep-based Self-refined Face-top Coordination",
        "authors": "['Honglin Li', 'Xiaoyang Mao', 'Mengdi Xu', 'Xiaogang Jin']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Face-top coordination, which exists in most clothes-fitting scenarios, is challenging due to varieties of attributes, implicit correlations, and tradeoffs between general preferences and individual preferences. We present a Deep-Based Self-Refined (DBSR) system to simulate face-top coordination based on intuition evaluation. To this end, we first establish a well-coordinated face-top (WCFT) dataset from fashion databases and communities. Then, we use a jointly trained CNN Deep Canonical Correlation Analysis (DCCA) method to bridge the semantic face-top gap based on the WCFT dataset to deal with general preferences. Subsequently, an irrelevance-based Optimum-path Forest (OPF) method is developed to adapt the results to individual preferences iteratively. Experimental results and user study demonstrate the effectiveness of our method.",
        "link": "https://dl.acm.org/doi/10.1145/3446970",
        "category": "Databases"
    },
    {
        "title": "Digital Tools to Promote Healthy Eating for Working-Age Individuals: A Scoping Review",
        "authors": "['Sibo Pan', 'Xipei Ren', 'Steven Vos', 'Aarnout Brombacher']",
        "date": "October 2021",
        "source": "Chinese CHI 2021: The Ninth International Symposium of Chinese CHI",
        "abstract": "In this scoping review, we aimed to understand current developments of digital tools for promoting healthy eating behaviors in a work context among working-age individuals and identify research gaps for future design opportunities. The papers published over the last decade (2010-2021) were searched in three databases: the Association for Computing Machinery (ACM) digital library, the interdisciplinary library Scopus, and the PubMed database. Initially, 2098 papers were identified, of which 16 papers were included in the final analysis. These 16 papers were published in 15 various conference proceedings or journals between 2010 and 2021, and mainly focused on tracking eating moment and promoting healthy food intake. Our findings showed that four types of digital tools for healthy eating promotion were commonly used, including mobile applications, wearables, service, and multicomponent (i.e., a combination between mobile apps and wearables). Moreover, we found that current digital tools made small using a range of existing working infrastructures. Future design research could focus on personalized, interactive, and playful digital tools in human-computer interaction field with behavior change techniques and user-centered approaches to promote healthy eating behaviors in daily work routines.",
        "link": "https://dl.acm.org/doi/10.1145/3490355.3490356",
        "category": "Databases"
    },
    {
        "title": "CBIR Using Features Derived by Deep Learning",
        "authors": "['Subhadip Maji', 'Smarajit Bose']",
        "date": "None",
        "source": "ACM/IMS Transactions on Data Science",
        "abstract": "In a Content-based Image Retrieval (CBIR) System, the task is to retrieve similar images from a large database given a query image. The usual procedure is to extract some useful features from the query image and retrieve images that have a similar set of features. For this purpose, a suitable similarity measure is chosen, and images with high similarity scores are retrieved. Naturally, the choice of these features play a very important role in the success of this system, and high-level features are required to reduce the “semantic gap.” In this article, we propose to use features derived from pre-trained network models from a deep-learning convolution network trained for a large image classification problem. This approach appears to produce vastly superior results for a variety of databases, and it outperforms many contemporary CBIR systems. We analyse the retrieval time of the method and also propose a pre-clustering of the database based on the above-mentioned features, which yields comparable results in a much shorter time in most of the cases.",
        "link": "https://dl.acm.org/doi/10.1145/3470568",
        "category": "Databases"
    },
    {
        "title": "Automation of Daily Monitoring Operations of N2N Connect Berhad using Zabbix Technology",
        "authors": "['Jayvee Mae Meman', 'Jocelyn Flores Villaverde', 'Noel B. Linsangan']",
        "date": "April 2021",
        "source": "ICIEI '21: Proceedings of the 6th International Conference on Information and Education Innovations",
        "abstract": "Information and communication technology (ICT) plays an important role in a company's business and in other modern enterprises. ICT helps in organizing and automating processes, structuring load of employees within the workforce of the company, and simplifies communication with external processes. Operations is the backbone of N2N and automating the daily monitoring operations is essential and will greatly help the company is easing the workload of their Operations Engineers. This paper aimed to deploy and implement automation of daily monitoring operations of N2N Connect Berhad which monitors not only services, the uptime of their servers, databases, etc., but also sends out alert notification to the operations team when an issue or problem occurs. The results showed that the setup and deployment of Zabbix was a success and works accordingly in production environment. The results further displayed that there is no significant difference in monitoring services, servers, and applications of N2N using Zabbix Technology. Furthermore, the setup satisfied all requirements of the company before it was deployed and implemented in production.",
        "link": "https://dl.acm.org/doi/10.1145/3470716.3470739",
        "category": "Databases"
    },
    {
        "title": "Experience Paper: sgx-dl: dynamic loading and hot-patching for secure applications",
        "authors": "['Nico Weichbrodt', 'Joshua Heinemann', 'Lennart Almstedt', 'Pierre-Louis Aublin', 'Rüdiger Kapitza']",
        "date": "December 2021",
        "source": "Middleware '21: Proceedings of the 22nd International Middleware Conference",
        "abstract": "Trusted execution as offered by Intel's Software Guard Extensions (SGX) is considered as an enabler to protect the integrity and confidentiality of stateful workloads such as key-value stores and databases in untrusted environments. These systems are typically long running and require extension mechanisms built on top of dynamic loading as well as hot-patching to avoid downtimes and apply security updates faster. However, such essential mechanisms are currently neglected or even missing in combination with trusted execution. We present sgx-dl, a lean framework that enables dynamic loading of enclave code at the function level and hot-patching of dynamically loaded code. Additionally, sgx-dl is the first framework to utilize the new SGX version 2 features and also provides a versioning mechanism for dynamically loaded code. Our evaluation shows that sgx-dl introduces a performance overhead of less than 5% and shrinks application downtime by an order of magnitude in the case of a database system.",
        "link": "https://dl.acm.org/doi/10.1145/3464298.3476134",
        "category": "Databases"
    },
    {
        "title": "Frequency-hiding order-preserving encryption with small client storage",
        "authors": "['Dongjie Li', 'Siyi Lv', 'Yanyu Huang', 'Yijing Liu', 'Tong Li', 'Zheli Liu', 'Liang Guo']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The range query on encrypted databases is usually implemented using the order-preserving encryption (OPE) technique which preserves the order of plaintexts. Since the frequency leakage of plaintexts makes OPE vulnerable to frequency-analyzing attacks, some frequency-hiding order-preserving encryption (FH-OPE) schemes are proposed. However, existing FH-OPE schemes require either the large client storage of size O(n) or O(log n) rounds of interactions for each query, where n is the total number of plaintexts. To this end, we propose a FH-OPE scheme that achieves the small client storage without additional client-server interactions. In detail, our scheme achieves O(N) client storage and 1 interaction per query, where N is the number of distinct plaintexts and N ≤ n. Especially, our scheme has a remarkable performance when N ≪ n. Moreover, we design a new coding tree for producing the order-preserving encoding which indicates the order of each ciphertext in the database. The coding strategy of our coding tree ensures that encodings update in the low frequency when inserting new ciphertexts. Experimental results show that the single round interaction and low-frequency encoding updates make our scheme more efficient than previous FH-OPE schemes.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484228",
        "category": "Databases"
    },
    {
        "title": "Feat-SKSJ: Fast and Exact Algorithm for Top-k Spatial-Keyword Similarity Join",
        "authors": "['Daichi Amagata', 'Shohei Tsuruoka', 'Yusuke Arai', 'Takahiro Hara']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "Due to the proliferation of GPS-enabled mobile devices and IoT environments, location-based services are generating a large number of objects that contain both spatial and keyword information, and spatial-keyword databases are receiving much attention. This paper addresses the problem of top-k spatial-keyword similarity join, which outputs k object pairs with the highest similarity. This query is a primitive operator for important applications, including duplicate detection, recommendation, and clustering. The main bottleneck of the top-k spatial-keyword similarity join is to compute the similarity of a given object pair. To avoid this computation as much as possible, a state-of-the-art algorithm utilizes a filter that can skip the exact similarity computation of a given pair. However, this algorithm suffers from a loose threshold at the first stage, a high filtering cost, and the impossibility of filtering many pairs in a batch. We propose Feat-SKSJ, which removes these drawbacks and quickly outputs the exact result. Extensive experiments on real datasets show that Feat-SKSJ is significantly faster than the state-of-the-art algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3483629",
        "category": "Databases"
    },
    {
        "title": "An introduction to MPI for Python",
        "authors": "['Xuguang Chen']",
        "date": "None",
        "source": "Journal of Computing Sciences in Colleges",
        "abstract": "Because parallel computing has many potential applications, such as databases and data mining, real time simulation of systems, and advanced graphics, it is becoming more and more important. There are two widely known parallel programming models today: message-passing model and shared memory model. In the message-passing model, each task has its private memories, and different tasks can communicate each other via message exchanges. Message Passing Interface (MPI) is a specification designed by a group of researchers from academia and industry and primarily addressing the message-passing model. It specifies the syntax and semantics of a core of library routines useful for message-passing programs and can be implemented in different programming languages, for example, C, Fortran, Java, and Python.",
        "link": "https://dl.acm.org/doi/10.5555/3512469.3512484",
        "category": "Databases"
    },
    {
        "title": "JDMAN: Joint Discriminative and Mutual Adaptation Networks for Cross-Domain Facial Expression Recognition",
        "authors": "['Yingjian Li', 'Yingnan Gao', 'Bingzhi Chen', 'Zheng Zhang', 'Lei Zhu', 'Guangming Lu']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Cross-domain Facial Expression Recognition (FER) is challenging due to the difficulty of concurrently handling the domain shift and semantic gap during domain adaptation. Existing methods mainly focus on reducing the domain discrepancy for transferable features but fail to decrease the semantic one, which may result in negative transfer. To this end, we propose Joint Discriminative and Mutual Adaptation Networks (JDMAN), which collaboratively bridge the domain shift and semantic gap by domain- and category-level co-adaptation based on mutual information and discriminative metric learning techniques. Specifically, we design a mutual information minimization module for domain-level adaptation, which narrows the domain shift by simultaneously distilling the domain-invariant components and eliminating the untransferable ones lying in different domains. Moreover, we propose a semantic metric learning module for category-level adaptation, which can close the semantic discrepancy during discriminative intra-domain representation learning and transferable inter-domain knowledge discovery. These two modules are jointly leveraged in our JDMAN to safely transfer the source knowledge to target data in an end-to-end manner. Extensive experimental results on six databases show that our method achieves state-of-the-art performance. The code of our JDMAN is available at https://github.com/YingjianLi/JDMAN.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475484",
        "category": "Databases"
    },
    {
        "title": "Neural Networks for Semantic Gaze Analysis in XR Settings",
        "authors": "['Lena Stubbemann', 'Dominik Dürrschnabel', 'Robert Refflinghaus']",
        "date": "May 2021",
        "source": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
        "abstract": "Virtual-reality (VR) and augmented-reality (AR) technology is increasingly combined with eye-tracking. This combination broadens both fields and opens up new areas of application, in which visual perception and related cognitive processes can be studied in interactive but still well controlled settings. However, performing a semantic gaze analysis of eye-tracking data from interactive three-dimensional scenes is a resource-intense task, which so far has been an obstacle to economic use. In this paper we present a novel approach which minimizes time and information necessary to annotate volumes of interest (VOIs) by using techniques from object recognition. To do so, we train convolutional neural networks (CNNs) on synthetic data sets derived from virtual models using image augmentation techniques. We evaluate our method in real and virtual environments, showing that the method can compete with state-of-the-art approaches, while not relying on additional markers or preexisting databases but instead offering cross-platform use.",
        "link": "https://dl.acm.org/doi/10.1145/3448017.3457380",
        "category": "Databases"
    },
    {
        "title": "A Promising Form of Suicide Intervention Model: Distant Supervision in Social Media The application of artificial intelligence in suicide intervention",
        "authors": "['Yuwei Shan']",
        "date": "October 2021",
        "source": "ISAIMS '21: Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences",
        "abstract": "Suicide is a growing public health concern. The method currently assessing suicide risk is very subjective, which may limit the efficacy and accuracy of forecasting efforts. It has been suggested that artificial intelligence (AI) has potential application for suicide intervention. Therefore, predicting suicide is stepping to artificial intelligence platforms, which can identify patterns within 'big data' to generate risk algorithms. These algorithms can predict the outbreaks of suicide and identify high-risk individuals. At the beginning, scholars combined AI with clinical databases, such as electronic medical record system, to identify suicide risks. But it only targets on patients with medical record information, the form of intervention has many limitations. Now, more and more people tend to express emotions on social media. It has demonstrated great potential to monitor social media for suicide intervention via artificial intelligence.",
        "link": "https://dl.acm.org/doi/10.1145/3500931.3500941",
        "category": "Databases"
    },
    {
        "title": "A farming-for-mining-framework to gain knowledge in supply chains",
        "authors": "['Joachim Hunker', 'Alexander Wuttke', 'Anne Antonia Scheidler', 'Markus Rabe']",
        "date": "December 2021",
        "source": "WSC '21: Proceedings of the Winter Simulation Conference",
        "abstract": "Gaining knowledge from a given data basis is a complex challenge. One of the frequently used methods in the context of a supply chain (SC) is knowledge discovery in databases (KDD). For a purposeful and successful knowledge discovery, valid and preprocessed input data are necessary. Besides preprocessing collected observational data, simulation can be used to generate a data basis as an input for the knowledge discovery process. The process of using a simulation model as a data generator is called data farming. This paper investigates the link between data farming and data mining. We developed a Farming-for-Mining-Framework, where we highlight requirements of knowledge discovery techniques and derive how the simulation model for data generation can be configured accordingly, e.g., to meet the required data accuracy. We suggest that this is a promising approach and is worth further research attention.",
        "link": "https://dl.acm.org/doi/10.5555/3522802.3522903",
        "category": "Databases"
    },
    {
        "title": "MS-GraphSIM: Inferring Point Cloud Quality via Multiscale Graph Similarity",
        "authors": "['Yujie Zhang', 'Qi Yang', 'Yiling Xu']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "To address the point cloud quality assessment (PCQA) problem, GraphSIM was proposed via jointly considering geometrical and color features, which shows compelling performance in multiple distortion detection. However, GraphSIM does not take into account the mutiscale characteristics of human perception. In this paper, we propose a multiscale PCQA model, called Multiscale Graph Similarity (MS-GraphSIM), that can better predict human subjective perception. First, exploring the multiscale processing method used in image processing, we introduce a multiscale representation of point clouds based on graph signal processing. Second, we extend GraphSIM into multiscale version based on the proposed multiscale representation. Specifically, MS-GraphSIM constructs a multiscale representation for each local patch extracted from the reference point cloud or the distorted point cloud, and then fuses GraphSIM at different scales to obtain an overall quality score. Experiment results demonstrate that the proposed MS-GraphSIM outperforms the state-of-the-art PCQA metrics over two fairly large and independent databases. Ablation studies further prove the proposed MS-GraphSIM is robust to different model hyperparameter settings. The code is available at https://github.com/zyj1318053/MS_GraphSIM.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475294",
        "category": "Databases"
    },
    {
        "title": "Noisy Speech Based Temporal Decomposition to Improve Fundamental Frequency Estimation",
        "authors": "['Anderson Queiroz', 'Rosângela Coelho']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "This article introduces a novel method to separate voiced frames of noisy speech signals into low-frequency or high-frequency. This separation improves the accuracy of fundamental frequency (F0) estimators. In this proposal, the target signal is analyzed by means of the ensemble empirical mode decomposition. Next, the pitch information is extracted from the first decomposition modes. This feature indicates the frequency region where the speech F0 should be located, thus separating the frames into low-frequency or high-frequency. The frames separation is then applied to correct pitch candidates extracted from a F0 detection method, improving the estimation accuracy. The proposed method and a baseline separation approach are evaluated considering four different F0 estimation algorithms. Experiments are conducted with the CSTR and TIMIT databases, and six noises with various signal-to-noise ratios. The Gross Error (GE) and Mean Absolute Error (MAE) metrics are adopted to evaluate the solutions in terms of F0 estimation errors. Results show that the proposed method outperforms the baseline, in terms of low/high frequency separation accuracy. Moreover, the novel solution is able to better improve F0 detection accuracy under different noisy conditions.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3190670",
        "category": "Databases"
    },
    {
        "title": "MetaCache-GPU: Ultra-Fast Metagenomic Classification",
        "authors": "['Robin Kobus', 'André Müller', 'Daniel Jünger', 'Christian Hundt', 'Bertil Schmidt']",
        "date": "August 2021",
        "source": "ICPP '21: Proceedings of the 50th International Conference on Parallel Processing",
        "abstract": "The cost of DNA sequencing has dropped exponentially over the past decade, making genomic data accessible to a growing number of scientists. In bioinformatics, localization of short DNA sequences (reads) within large genomic sequences is commonly facilitated by constructing index data structures which allow for efficient querying of substrings. Recent metagenomic classification pipelines annotate reads with taxonomic labels by analyzing their k-mer histograms with respect to a reference genome database. CPU-based index construction is often performed in a preprocessing phase due to the relatively high cost of building irregular data structures such as hash maps. However, the rapidly growing amount of available reference genomes establishes the need for index construction and querying at interactive speeds. In this paper, we introduce MetaCache-GPU – an ultra-fast metagenomic short read classifier specifically tailored to fit the characteristics of CUDA-enabled accelerators. Our approach employs a novel hash table variant featuring efficient minhash fingerprinting of reads for locality-sensitive hashing and their rapid insertion using warp-aggregated operations. Our performance evaluation shows that MetaCache-GPU is able to build large reference databases in a matter of seconds, enabling instantaneous operability, while popular CPU-based tools such as Kraken2 require over an hour for index construction on the same data. In the context of an ever-growing number of reference genomes, MetaCache-GPU is the first metagenomic classifier that makes analysis pipelines with on-demand composition of large-scale reference genome sets practical. The source code is publicly available at https://github.com/muellan/metacache.",
        "link": "https://dl.acm.org/doi/10.1145/3472456.3472460",
        "category": "Databases"
    },
    {
        "title": "Abstraction in data integration",
        "authors": "['Gianluca Cima', 'Marco Console', 'Maurizio Lenzerini', 'Antonella Poggi']",
        "date": "June 2021",
        "source": "LICS '21: Proceedings of the 36th Annual ACM/IEEE Symposium on Logic in Computer Science",
        "abstract": "Data integration provides a unified and abstract view over a set of existing data sources. The typical architecture of a data integration system comprises the global schema, which is the structure for the unified view, the source schema, and the mapping, which is a formal account of how data at the sources relate to the global view. Most of the research work on data integration in the last decades deals with the problem of processing a query expressed on the global schema by computing a suitable query over the sources, and then evaluating the latter in order to derive the answers to the original query. Here, we address a novel issue in data integration: starting from a query expressed over the sources, the goal is to find an abstraction of such query, i.e., a query over the global schema that captures the original query, modulo the mapping. The goal of the paper is to provide an overview of the notion of abstraction in data integration, by presenting a formal framework, illustrating the results that have appeared in the recent literature, and discussing interesting directions for future research.",
        "link": "https://dl.acm.org/doi/10.1109/LICS52264.2021.9470716",
        "category": "Databases"
    },
    {
        "title": "Scalable and Usable Relational Learning With Automatic Language Bias",
        "authors": "['Jose Picado', 'Arash Termehchy', 'Alan Fern', 'Sudhanshu Pathak', 'Praveen Ilango', 'John Davis']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "A large body of machine learning and AI is focused on learning models composed of (probabilistic) logical rules, i.e., relational models, over relational databases and knowledge bases. To learn effective relational models over the huge space of possible ones efficiently, users of the current learning systems must restrict the structure of the candidate models using language bias. ML experts have to spend a long time inspecting the data and performing many rounds of trial and error to develop an effective language bias. We propose AutoBias, a system that leverages information in the underlying data to generate the language bias. As its induced language bias may not restrict the set of candidate models as tightly as the manually-written ones, learning may not scale to large datasets. Thus, we design novel and efficient methods to sample and learn effective relational models over large data. Our extensive empirical study shows that AutoBias delivers the same accuracy as using manually-written language bias by imposing only a slight overhead on the learning time.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457275",
        "category": "Databases"
    },
    {
        "title": "Wearable Interactions for Users with Motor Impairments: Systematic Review, Inventory, and Research Implications",
        "authors": "['Alexandru-Ionut Siean', 'Radu-Daniel Vatavu']",
        "date": "October 2021",
        "source": "ASSETS '21: Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility",
        "abstract": "We conduct a systematic literature review on wearable interactions for users with motor impairments and report results from a meta-analysis of 57 scientific articles identified in the ACM DL and IEEE Xplore databases. Our findings show limited research conducted on accessible wearable interactions (e.g., just four papers addressing smartwatch input), a disproportionate interest for hand gestures compared to other input modalities for wearable devices, and low numbers of participants with motor impairments involved in user studies about wearable interactions (a median of 6.0 and average of 8.2 participants per study). We compile an inventory of 92 finger, hand, head, shoulder, eye gaze, and foot gesture commands for smartwatches, smartglasses, headsets, earsets, fitness trackers, data gloves, and armband wearable devices extracted from the scientific literature that we surveyed. Based on our findings, we propose four directions for future research on accessible wearable interactions for users with motor impairments.",
        "link": "https://dl.acm.org/doi/10.1145/3441852.3471212",
        "category": "Databases"
    },
    {
        "title": "Putting Things into Context: Rich Explanations for Query Answers using Join Graphs",
        "authors": "['Chenjie Li', 'Zhengjie Miao', 'Qitian Zeng', 'Boris Glavic', 'Sudeepa Roy']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In many data analysis applications there is a need to explain why a surprising or interesting result was produced by a query. Previous approaches to explaining results have directly or indirectly relied on data provenance, i.e., input tuples contributing to the result(s) of interest. However, some information that is relevant for explaining an answer may not be contained in the provenance. We propose a new approach for explaining query results by augmenting provenance with information from other related tables in the database. Using a suite of optimization techniques, we demonstrate experimentally using real datasets and through a user study that our approach produces meaningful results and is efficient.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3459246",
        "category": "Databases"
    },
    {
        "title": "Priority-Based Skyline Query Processing for Incomplete Data",
        "authors": "['Chuang-Ming Liu', 'Denis Pak', 'Ari Ernesto Ortiz Castellanos']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Over the years, several skyline query techniques have been introduced to handle incompleteness of data, the most recent of which has proposed to sort the points of a dataset into several distinct lists based on each dimension. The points would be accessed based on these lists in round robin fashion, and the points that haven’t been dominated by the end would compose the final skyline. The work is based on the assumption that relatively dominant points, if sorted, would be processed first, and even if the point wouldn’t be a skyline point, it would prune huge amount of data. However, that approach doesn’t take into consideration that the dominance of a point depends not only on the highest value of a given dimension, but also on the number of complete dimensions a point has. Hence, we propose a Priority-First Sort-Based Incomplete Data Skyline (PFSIDS) that utilizes a different indexing technique that allows optimization of access based on both number of complete dimensions a point has as well as sorting of the data.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472272",
        "category": "Databases"
    },
    {
        "title": "CGPTuner: a contextual gaussian process bandit approach for the automatic tuning of IT configurations under varying workload conditions",
        "authors": "['Stefano Cereda', 'Stefano Valladares', 'Paolo Cremonesi', 'Stefano Doni']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Properly selecting the configuration of a database management system (DBMS) is essential to increase performance and reduce costs. However, the task is astonishingly tricky due to a large number of tunable configuration parameters and their inter-dependencies. Also, the optimal configuration depends upon the workload to which the DBMS is exposed. To extract the full potential of a DBMS, we must also consider the entire IT stack on which the DBMS is running, comprising layers like the Java virtual machine, the operating system and the physical machine. Each layer offers a multitude of parameters that we should take into account. The available parameters vary as new software versions are released, making it impractical to rely on historical knowledge bases. We present a novel tuning approach for the DBMS configuration auto-tuning that quickly finds a well-performing configuration of an IT stack and adapts it to workload variations, without having to rely on a knowledge base. We evaluate the proposed approach using the Cassandra and MongoDB DBMSs, showing that it adjusts the suggested configuration to the observed workload and is portable across different IT applications. We try to minimise the memory consumption without increasing the response time, showing that the proposed approach reduces the response time and increases the memory requirements only under heavy-load conditions, reducing it again when the load decreases.",
        "link": "https://dl.acm.org/doi/10.14778/3457390.3457404",
        "category": "Databases"
    },
    {
        "title": "FexRNA: Exploratory Data Analysis and Feature Selection of Non-Coding RNA",
        "authors": "['Noorul Amin', 'Annette McGrath', 'Yi-Ping Phoebe Chen']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Non-coding RNA (ncRNA) is involved in many biological processes and diseases in all species. Many ncRNA datasets exist that provide ncRNA data in FASTA format which is well suited for biomedical purposes. However, for ncRNA analysis and classification, statistical learning methods require hidden numerical features from the data. Furthermore, in the literature, a wealth of sequence intrinsic features has been proposed for ncRNA identification. The extraction of hidden features, their analysis, and usage of a suitable set of features is crucial for the performance of any statistical learning method. To alleviate the posed challenges, we generated 96 feature datasets from ncRNA widely used features. The feature datasets are based on RNACentral and consist of species, ncRNA types, and expert databases that are available on the FexRNA platform. Additionally, the feature datasets are explored and analysed to provide statistical information, univariate, and bivariate analysis. We sought to determine which of these 17 features would be most appropriate to use in developing ncRNA classification approaches. For feature selection (FS), a two-phase hierarchical FS framework based on correlation and majority voting is proposed and evaluated on 5 species. The FexRNA platform provides information about ncRNA feature analysis and selection.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2021.3057128",
        "category": "Databases"
    },
    {
        "title": "FexRNA: Exploratory Data Analysis and Feature Selection of Non-Coding RNA",
        "authors": "['Noorul Amin', 'Annette McGrath', 'Yi-Ping Phoebe Chen']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Non-coding RNA (ncRNA) is involved in many biological processes and diseases in all species. Many ncRNA datasets exist that provide ncRNA data in FASTA format which is well suited for biomedical purposes. However, for ncRNA analysis and classification, statistical learning methods require hidden numerical features from the data. Furthermore, in the literature, a wealth of sequence intrinsic features has been proposed for ncRNA identification. The extraction of hidden features, their analysis, and usage of a suitable set of features is crucial for the performance of any statistical learning method. To alleviate the posed challenges, we generated 96 feature datasets from ncRNA widely used features. The feature datasets are based on RNACentral and consist of species, ncRNA types, and expert databases that are available on the FexRNA platform. Additionally, the feature datasets are explored and analysed to provide statistical information, univariate, and bivariate analysis. We sought to determine which of these 17 features would be most appropriate to use in developing ncRNA classification approaches. For feature selection (FS), a two-phase hierarchical FS framework based on correlation and majority voting is proposed and evaluated on 5 species. The FexRNA platform provides information about ncRNA feature analysis and selection.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2021.3057128",
        "category": "Databases"
    },
    {
        "title": "Materialized View Driven Architecture over Lattice of Cuboids in Data Warehouse",
        "authors": "['Partha Ghosh', 'Takaaki Goto', 'Jyotsna Kumar Mandal', 'Soumya Sen']",
        "date": "June 2021",
        "source": "ACIT '21: Proceedings of the the 8th International Virtual Conference on Applied Computing &amp; Information Technology",
        "abstract": "Data warehouse is used across the organizations for analytical processing. It is organized in the form of lattice of cuboids for carrying out the different types of analysis. The structure of lattice of cuboids grows exponentially with the number of dimensions. Every cuboid contains huge amount of data. The data size is multiplied in the data warehouse as a new entry takes place in any dimension. In modern day business analysis, processing the data in real-time is one of the intrinsic requirements. However, with the gigantic structure of lattice of cuboids along with the highly loaded data in every cuboid it is difficult to achieve the real-time processing. View materialization is practiced over the decades for faster query processing in the database applications. This research proposed a fuzzy based materialized data-cube driven warehouse architecture for fast decision making. Experimental output shows the proposed methodology achieves faster data analysis and better hit-miss ratio in the materialized data-cubes.",
        "link": "https://dl.acm.org/doi/10.1145/3468081.3471064",
        "category": "Databases"
    },
    {
        "title": "Compiler-assisted object inlining with value fields",
        "authors": "['Rodrigo Bruno', 'Vojin Jovanovic', 'Christian Wimmer', 'Gustavo Alonso']",
        "date": "June 2021",
        "source": "PLDI 2021: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
        "abstract": "Object Oriented Programming has flourished in many areas ranging from web-oriented microservices, data processing, to databases. However, while representing domain entities as objects is appealing to developers, it leads to data fragmentation, resulting in high memory footprint and poor locality.   To improve memory footprint and memory locality, embedding the payload of an object into another (object inlining) has been proposed, however, with severe limitations. We argue that object inlining is mostly useful to optimize objects in the application data-path and that such objects have value semantics, unlocking great potential for inlining objects.   We propose value fields, an abstraction which allows fields to be marked as having value semantics. We take advantage of the closed-world assumption provided by GraalVM Native Image to implement Object inlining. Results show that using value fields requires minimal to no effort from developers and leads to improvements in throughput of up to 3x, memory footprint of up to 40%, and GC pause times of up to 35%.",
        "link": "https://dl.acm.org/doi/10.1145/3453483.3454034",
        "category": "Databases"
    },
    {
        "title": "LDAH2V: Exploring Meta-Paths Across Multiple Networks for lncRNA-Disease Association Prediction",
        "authors": "['Lei Deng', 'Wenkai Li', 'Jingpu Zhang']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Accumulating evidence has demonstrated dysfunctions of long non-coding RNAs (lncRNAs) are involved in various complex human diseases. However, even today, the relationships between lncRNAs and diseases remain unknown in most cases. Developing effective computational approaches to identify potential lncRNA-disease associations has become a hot topic. Existing network-based approaches are usually focused on the intrinsic features of lncRNAs and diseases but ignore the heterogeneous information of biological networks. Considering the limitations in previous methods, we propose LDAH2V, an efficient computational framework for predicting potential lncRNA-disease associations. LDAH2V uses the HIN2Vec to calculate the meta-path and feature vector for each lncRNA-disease pair in the heterogeneous information network (HIN), which consists of lncRNA similarity network, disease similarity network, miRNA similarity network, and the associations between them. Then, a Gradient Boosting Tree (GBT) classifier to predict lncRNA-disease associations is built with the feature vectors. The results show that LDAH2V performs significantly better than the four existing state-of-the-art methods and gains an AUC of 0.97 in the 10-fold cross-validation test. Furthermore, case studies of colon cancer and ovarian cancer-related lncRNAs have been confirmed in related databases and medical literature.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2019.2946257",
        "category": "Databases"
    },
    {
        "title": "Yes, \"Attention Is All You Need\", for Exemplar based Colorization",
        "authors": "['Wang Yin', 'Peng Lu', 'Zhaoran Zhao', 'Xujun Peng']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Conventional exemplar based image colorization tends to transfer colors from reference image only to grayscale image based on the semantic correspondence between them. But their practical capabilities are limited when semantic correspondence can hardly be found. To overcome this issue, additional information, such as colors from the database is normally introduced. However, it's a great challenge to consider color information from reference image and database simultaneously because there lacks a unified framework to model different color information and the multi-modal ambiguity in database cannot be removed easily. Also, it is difficult to fuse different color information effectively. Thus, a general attention based colorization framework is proposed in this work, where the color histogram of reference image is adopted as a prior to eliminate the ambiguity in database. Moreover, a sparse loss is designed to guarantee the success of information fusion. Both qualitative and quantitative experimental results show that the proposed approach achieves better colorization performance compared with the state-of-the-art methods on public databases with different quality metrics.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475385",
        "category": "Databases"
    },
    {
        "title": "A Recommender System for Tracking Vulnerabilities",
        "authors": "['Philip Huff', 'Kylie McClanahan', 'Thao Le', 'Qinghua Li']",
        "date": "August 2021",
        "source": "ARES '21: Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "abstract": "Mitigating vulnerabilities in software requires first identifying the vulnerabilities with an organization’s software assets. This seemingly trivial task involves maintaining vendor product vulnerability notification for a kludge of hardware and software packages from innumerable software publishers, coding projects, and third-party package managers. On the other hand, software vulnerability databases are often consistently reported and categorized in clean, standard formats and neatly tied to a common software product enumerator (i.e., CPE). Currently it is a heavy workload for cybersecurity analysts at organizations to match their hardware and software package inventory to target CPEs. This hinders organizations from getting notifications for new vulnerabilities, and identifying applicable vulnerabilities. In this paper, we present a recommender system to automatically identify a minimal candidate set of CPEs for software names to improve vulnerability identification and alerting accuracy. The recommender system uses a pipeline of natural language processing, fuzzy matching, and machine learning to significantly reduce the human effort needed for software product vulnerability matching.",
        "link": "https://dl.acm.org/doi/10.1145/3465481.3470039",
        "category": "Databases"
    },
    {
        "title": "An algorithm to interpolate concavities",
        "authors": "['José Duarte', 'Mark McKenney']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Region interpolation methods impose restrictions that have an influence on how concavities are transformed, potentially, causing their transformation to be unnatural, e.g., a concavity unexpectedly appears (disappears) from (to) a point. In this work we present an algorithm to transform a line segment to a concavity, and a line segment to a simple non-closed linestring with possibly several concavities. The algorithm is deterministic, it does not assume that an element in the source is transformed directly to an element (or set of elements) in the target, works in stages (steps), i.e., several different transformations can occur while an element in the source is transformed to an element in the target, and its output is a set of moving segments representing the transformation. The complexity of a non-optimized implementation of the transformation of a segment to a concavity using the algorithm is O (kn2), where k is the number of steps in the transformation (the number of intermediate transformations in the transformation) and n is the number of points in the target geometry. The algorithm is primarily devised to be integrated with the region interpolation methods proposed in the spatiotemporal databases literature.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441934",
        "category": "Databases"
    },
    {
        "title": "Transformer-based named entity recognition for parsing clinical trial eligibility criteria",
        "authors": "['Shubo Tian', 'Arslan Erdengasileng', 'Xi Yang', 'Yi Guo', 'Yonghui Wu', 'Jinfeng Zhang', 'Jiang Bian', 'Zhe He']",
        "date": "August 2021",
        "source": "BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "abstract": "The rapid adoption of electronic health records (EHRs) systems has made clinical data available in electronic format for research and for many downstream applications. Electronic screening of potentially eligible patients using these clinical databases for clinical trials is a critical need to improve trial recruitment efficiency. Nevertheless, manually translating free-text eligibility criteria into database queries is labor intensive and inefficient. To facilitate automated screening, free-text eligibility criteria must be structured and coded into a computable format using controlled vocabularies. Named entity recognition (NER) is thus an important first step. In this study, we evaluate 4 state-of-the-art transformer-based NER models on two publicly available annotated corpora of eligibility criteria released by Columbia University (i.e., the Chia data) and Facebook Research (i.e.the FRD data). Four transformer-based models (i.e., BERT, ALBERT, RoBERTa, and ELECTRA) pretrained with general English domain corpora vs. those pretrained with PubMed citations, clinical notes from the MIMIC-III dataset and eligibility criteria extracted from all the clinical trials on ClinicalTrials.gov were compared. Experimental results show that RoBERTa pretrained with MIMIC-III clinical notes and eligibility criteria yielded the highest strict and relaxed F-scores in both the Chia data (i.e., 0.658/0.798) and the FRD data (i.e., 0.785/0.916). With promising NER results, further investigations on building a reliable natural language processing (NLP)-assisted pipeline for automated electronic screening are needed.",
        "link": "https://dl.acm.org/doi/10.1145/3459930.3469560",
        "category": "Databases"
    },
    {
        "title": "A study on the probability of equipment failure in gas extraction well stations based on equipment reliability analysis",
        "authors": "['Kaiwu Liang', 'Maolin Wang', 'Xingyu Chen']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "In the risk analysis of gas extraction well stations, the failure probability of well station equipment is mostly obtained from international statistical databases, and the influence of the actual state of the equipment on the failure probability is not sufficiently considered, which is prone to errors in actual application. In order to improve the fit between the statistical data and actual situation of a gas extraction well station, this paper takes the mechanical seal equipment of a gas extraction well station as an example, and firstly, establishes a mechanical seal equipment failure event tree through the event tree analysis method; secondly, combines the reliability block diagram method and the typical equipment failure distribution function — the exponential function to calculate the equipment reliability and derives the equipment reliability function; finally, proposes a ratio calculation method with reference to the reliability bath curve. calculation method for the correction calculation of equipment failure probability. The results show that the method can effectively improve the degree of fit between statistical data and the actual situation, and can provide a reference for risk assessment work of gas extraction sites.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3502485",
        "category": "Databases"
    },
    {
        "title": "Threshold queries in theory and in the wild",
        "authors": "['Angela Bonifati', 'Stefania Dumbrava', 'George Fletcher', 'Jan Hidders', 'Matthias Hofer', 'Wim Martens', 'Filip Murlak', 'Joshua Shinavier', 'Sławek Staworko', 'Dominik Tomaszuk']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Threshold queries are an important class of queries that only require computing or counting answers up to a specified threshold value. To the best of our knowledge, threshold queries have been largely disregarded in the research literature, which is surprising considering how common they are in practice. In this paper, we present a deep theoretical analysis of threshold query evaluation and show that thresholds can be used to significantly improve the asymptotic bounds of state-of-the-art query evaluation algorithms. We also empirically show that threshold queries are significant in practice. In surprising contrast to conventional wisdom, we found important scenarios in real-world data sets in which users are interested in computing the results of queries up to a certain threshold, independent of a ranking function that orders the query results.",
        "link": "https://dl.acm.org/doi/10.14778/3510397.3510407",
        "category": "Databases"
    },
    {
        "title": "Messengers integration methods with corporate information systems: The introduction and application of instant messengers to the corporate information system",
        "authors": "['Viktoria Konovalova', 'Nikita Kazakov', 'Hamza Mohammed Ridha Al-Khafaji', 'Maxim Kovtsur', 'Anton Kistruga']",
        "date": "December 2021",
        "source": "ICFNDS 2021: The 5th International Conference on Future Networks &amp; Distributed Systems",
        "abstract": "Today we cannot imagine our life without the introduction of digital technologies into the life of society. Messengers have become popular not only among ordinary users, but also among IT companies. Such companies tend to use and support one or multiple corporate systems. In this case, events that require prompt response from users of those systems are possible to occur. To solve this problem, the corporate information system is integrated with other communication channels, including messengers, social networks, etc. The article discusses integration of corporate information systems with the Telegram messenger using bots. Having studied publications in one way or another related to this problem, it turned out that almost all of them do not reflect the topic of the introduction and application of instant messengers to the corporate information system. Most of them describe the importance of notifications, calculate the speed of notifications, or create separate information systems, but do not talk about the possibilities of using messengers as a method of interaction between user and system. Therefore, the task arose of studying approaches to integrating messengers.",
        "link": "https://dl.acm.org/doi/10.1145/3508072.3508191",
        "category": "Databases"
    },
    {
        "title": "A scalable causal broadcast that tolerates dynamics of mobile networks",
        "authors": "['Daniel Wilhelm', 'Luciana Arantes', 'Pierre Sens']",
        "date": "January 2022",
        "source": "ICDCN '22: Proceedings of the 23rd International Conference on Distributed Computing and Networking",
        "abstract": "Causal broadcast is at the core of collaborative applications, distributed databases, conferencing, or social networks. Existing causal broadcast algorithms are either not scalable or cannot be implemented on mobile networks because they do not take into account the features of these networks: limited capacities of nodes (computation, storage, energy), unreliable communication channels, and the dynamics of connections due to node mobility, node failure, and join/leave of nodes. This work presents a causal broadcast algorithm for mobile networks. The algorithm is scalable: control information piggybacked on messages and maintained on nodes is of small size. Experiments conducted on OMNeT++, a realistic network simulator, confirms the effectiveness of our causal broadcast protocol, rendering causal broadcast affordable in mobile networks.",
        "link": "https://dl.acm.org/doi/10.1145/3491003.3491010",
        "category": "Databases"
    },
    {
        "title": "Restricted Randomness DBSCAN : A faster DBSCAN Algorithm",
        "authors": "['Sashakt Pathak', 'Arushi Agarwal', 'Ankita Ankita', 'Mahendra Kumar Gurve']",
        "date": "August 2021",
        "source": "IC3-2021: Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing",
        "abstract": "Data Mining is the process of extracting useful and accurate information or patterns from large databases using different algorithms and methods of machine learning. To analyze the data, Clustering is one of the methods in which similar data is grouped together and DBSCAN clustering algorithm is the one, which is broadly used in numerous practical applications. This paper presents a more efficient density based clustering algorithm, which has the ability to discover cluster faster than the existing DBSCAN algorithm. The efficiency is achieved by restricting the randomness of choosing points from the dataset. Our proposed algorithm named Restricted Randomness DBSCAN (RR DBSCAN) is compared with conventional DBSCAN algorithm over 9 datasets on the basis of Silhouette Coefficient, Time taken in formation of clusters and accuracy. The results show that RR DBSCAN performs better than traditional DBSCAN in terms of accuracy and time taken to form clusters.",
        "link": "https://dl.acm.org/doi/10.1145/3474124.3474204",
        "category": "Databases"
    },
    {
        "title": "Convergence of Array DBMS and Cellular Automata: A Road Traffic Simulation Case",
        "authors": "['Ramon Antonio Rodriges Zalipynis']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Array DBMSs manage big N-d arrays, are not yet widely known, but are experiencing an R&D surge due to the rapid growth of array volumes. Cellular automata (CA) operate on a discrete lattice of cells that can be modeled by an N-d array. CA are successfully applied to model fire spread, land cover change, road traffic, and other processes. We made traffic CA simulations possible by array DBMS due to novel components: native UDF language, proactive exec plans, convolution operator, retiling strategy, array versioning, locks, virtual axes, etc. A database approach to CA brings powerful parallelization, data fusion, array processing, and interoperability to name a few. To our best knowledge, our work is the first to run end-to-end CA simulations completely inside array DBMS: we enable array DBMS to simulate the physical world for the first time. Paper homepage: http://sigmod2021.gis.gg/",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3458457",
        "category": "Databases"
    },
    {
        "title": "Human Pose Estimation for Training Assistance: a Systematic Literature Review",
        "authors": "['Gisela Miranda Difini', 'Marcio Garcia Martins', 'Jorge Luis Victória Barbosa']",
        "date": "November 2021",
        "source": "WebMedia '21: Proceedings of the Brazilian Symposium on Multimedia and the Web",
        "abstract": "Human pose estimation is an important field of Computer Vision that aims to predict poses of individuals from videos and images. It has been used in many different areas including human-computer interaction, motion analysis, surveillance, action prediction, action correction, augmented reality, virtual reality, and healthcare. This review is focused on the most significant contributions in human pose estimation for training assistance. Executing movements correctly is crucial in all kinds of physical activities, both to increase performance and reduce risk of injury. Human pose estimation is poised to help athletes better analyse the quality of their movements. The systematic review study was conducted in five databases including articles from January 2011 to March 2021. The initial search resulted in 129 articles, of which 8 were selected after applying the filtering criteria. Moreover this study presents the challenges related to pose estimation, which pose estimation methods have been used in recent years, in which specific activities the selected articles have focused on, and a taxonomy of human pose estimation methods.",
        "link": "https://dl.acm.org/doi/10.1145/3470482.3479633",
        "category": "Databases"
    },
    {
        "title": "Revisiting Gendered Web Forms: An Evaluation of Gender Inputs with (Non-)Binary People",
        "authors": "['Morgan Klaus Scheuerman', 'Aaron Jiang', 'Katta Spiel', 'Jed R. Brubaker']",
        "date": "May 2021",
        "source": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "abstract": "Gender input forms act as gates to accessing information, websites, and services online. Non-binary people regularly have to interact with them, though many do not offer non-binary gender options. This results in non-binary individuals having to either choose an incorrect gender category or refrain from using a site or service—which is occasionally infeasible (e.g., when accessing health services). We tested five different forms through a survey with binary and non-binary participants (n = 350) in three contexts—a digital health form, a social media website, and a dating app. Our results indicate that the majority of participants found binary “male or female” forms exclusive and uncomfortable to fill out across all contexts. We conclude with design considerations for improving gender input forms and consequently their underlying gender model in databases. Our work aims to sensitize designers of (online) gender web forms to the needs and desires of non-binary people.",
        "link": "https://dl.acm.org/doi/10.1145/3411764.3445742",
        "category": "Databases"
    },
    {
        "title": "Impact by acquisition of cargo insurance policy in international maritime transport",
        "authors": "['Carlos Cueva Clemente', 'Yampier Ramirez Macuri', 'Lucia Maribel Bautista Zúniga']",
        "date": "January 2022",
        "source": "ICCMB '22: Proceedings of the 2022 5th International Conference on Computers in Management and Business",
        "abstract": "This article aims to analyze the impact generated by contracting a cargo insurance policy in international maritime transport through the different research articles published during the years 2016 to 2021 in Latin America and Europe. Likewise, this article was developed through a systematic review of scientific articles found in various databases such as Redalyc, Scielo, Alicia, Academic Google and La Referencia, giving as results different approaches and authors' appreciations regarding the impact generated by hiring a Insurance policy in international trade and maritime transport, which are: it prevents risks that may occur with cargo, avoids economic effects, gives peace of mind to the exporter and importer and has current coverage throughout the world. Likewise, the authors mention three negative impacts that they consider relevant when contracting insurance: “Delay in recovery times for insurance funds”, “Not all products can be insured” and “Deductibles are usually very high.",
        "link": "https://dl.acm.org/doi/10.1145/3512676.3512697",
        "category": "Databases"
    },
    {
        "title": "Decentralized Integration Approach of Disjoint Modules in Factory Environments",
        "authors": "['Selvine George Mathias', 'Daniel Grossmann', 'Sebastian Schmied', 'Ralph Klaus Müller']",
        "date": "January 2022",
        "source": "IEIM 2022: 2022 The 3rd International Conference on Industrial Engineering and Industrial Management",
        "abstract": "Segregated analysis capable devices, modules and subsystems dominate processes in a factory environment. Examples are machining processes where different tools are used for cutting, shearing, bending etc., but further analysis can be done using vibration sensors or acoustic sensors fitted on the tool or placed on the workbench, not directly connected to it. Such a disjoint scenario would require an integration scheme where data acquisition entities and production objects along with other data sources such as databases can be synchronized and used for effective analysis. This paper proposes the use of a middle-ware entity such as Open Platform Communication Unified Automation (OPC UA) standard as a communication and trigger mechanism for the onset of multiple functions such as device recognition, data acquisition and control switches for these separate devices into a common web service space. The implemented demonstration focuses on the use of custom interoperability solutions as a means to explore the possibility of advanced analyses using sensors and devices not already integrated into the systems, and hence are detachable, portable and reusable as a subsystem to any production floor equipment.",
        "link": "https://dl.acm.org/doi/10.1145/3524338.3524368",
        "category": "Databases"
    },
    {
        "title": "PAN: Personalized Annotation-Based Networks for the Prediction of Breast Cancer Relapse",
        "authors": "['Thin Nguyen', 'Samuel C. Lee', 'Thomas P. Quinn', 'Buu Truong', 'Xiaomei Li', 'Truyen Tran', 'Svetha Venkatesh', 'Thuc Duy Le']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "The classification of clinical samples based on gene expression data is an important part of precision medicine. In this manuscript, we show how transforming gene expression data into a set of personalized (sample-specific) networks can allow us to harness existing graph-based methods to improve classifier performance. Existing approaches to personalized gene networks have the limitation that they depend on other samples in the data and must get re-computed whenever a new sample is introduced. Here, we propose a novel method, called Personalized Annotation-based Networks (PAN), that avoids this limitation by using curated annotation databases to transform gene expression data into a graph. Unlike competing methods, PANs are calculated for each sample independent of the population, making it a more efficient way to obtain single-sample networks. Using three breast cancer datasets as a case study, we show that PAN classifiers not only predict cancer relapse better than gene features alone, but also outperform PPI (protein-protein interactions) and population-level graph-based classifiers. This work demonstrates the practical advantages of graph-based classification for high-dimensional genomic data, while offering a new approach to making sample-specific networks. <italic>Supplementary information:</italic> PAN and the baselines are implemented in Python. Source code and data are available at <uri>https://github.com/thinng/PAN</uri>.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2021.3076422",
        "category": "Databases"
    },
    {
        "title": "A demonstration of NoDA: unified access to NoSQL stores",
        "authors": "['Nikolaos Koutroumanis', 'Nikolaos Kousathanas', 'Christos Doulkeridis', 'Akrivi Vlachou']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In this demo paper, we present a system prototype, called NoDA, that unifies access to NoSQL stores, by exposing a single interface to big data developers. This hides the heterogeneity of NoSQL stores, in terms of different query languages, non-standardized access, and different data models. NoDA comprises a layer positioned on top of NoSQL stores that defines a set of basic data access operators (filter, project, aggregate, etc.), implemented for different NoSQL engines. The provision of generic data access operators enables a declarative interface using SQL as query language. Furthermore, NoDA is extended to provide more complex operators, such as geospatial operators, which are only partially supported by NoSQL stores. We demonstrate NoDA by showcasing that the exact same query can be processed by different NoSQL stores, without any modification or transformation whatsoever.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476361",
        "category": "Databases"
    },
    {
        "title": "Probabilistic and Dynamic Molecule-Disease Interaction Modeling for Drug Discovery",
        "authors": "['Tianfan Fu', 'Cao Xiao', 'Cheng Qian', 'Lucas M. Glass', 'Jimeng Sun']",
        "date": "August 2021",
        "source": "KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Drug discovery aims at finding promising drug molecules for treating target diseases. Existing computational drug discovery methods mainly depend on molecule databases, ignoring valuable data collected from clinical trials. In this work, we propose PRIME to leverage high-quality drug molecules and drug-disease relations in historical clinical trials to narrow down the molecular search space in drug discovery. PRIME also introduces time dependency constraints to model evolving drug-disease relations using a probabilistic deep learning model that can quantify model uncertainty. We evaluated PRIME against leading models on both de novo design and drug repurposing tasks. Results show that compared with the best baselines, PRIME achieves 25.9% relative improvement (i.e., reduction) in average hit-ranking on drug repurposing and 47.6% relative improvement in success rate on de novo design.",
        "link": "https://dl.acm.org/doi/10.1145/3447548.3467286",
        "category": "Databases"
    },
    {
        "title": "Automatic Inference of BGP Location Communities",
        "authors": "['Brivaldo A. Silva', 'Paulo Mol', 'Osvaldo Fonseca', 'Italo Cunha', 'Ronaldo A. Ferreira', 'Ethan Katz-Bassett']",
        "date": "None",
        "source": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "abstract": "The Border Gateway Protocol (BGP) orchestrates Internet communications between and inside Autonomous Systems. BGP's flexibility allows operators to express complex policies and deploy advanced traffic engineering systems. A key mechanism to provide this flexibility is tagging route announcements with BGP communities, which have arbitrary, operator-defined semantics, to pass information or requests from router to router. Typical uses of BGP communities include attaching metadata to route announcements, such as where a route was learned or whether it was received from a customer, and controlling route propagation, for example to steer traffic to preferred paths or blackhole DDoS traffic. However, there is no standard for specifying the semantics nor a centralized repository that catalogs the meaning of BGP communities. The lack of standards and central repositories complicates the use of communities by the operator and research communities. In this paper, we present a set of techniques to infer the semantics of BGP communities from public BGP data. Our techniques infer communities related to the entities or locations traversed by a route by correlating communities with AS paths. We also propose a set of heuristics to filter incorrect inferences introduced by misbehaving networks, sharing of BGP communities among sibling autonomous systems, and inconsistent BGP dumps. We apply our techniques to billions of routing records from public BGP collectors and make available a public database with more than 15 thousand location communities. Our comparison with manually-built databases shows our techniques provide high precision (up to 93%), better coverage (up to 81% recall), and dynamic updates, complementing operators' and researchers' abilities to reason about BGP community semantics.",
        "link": "https://dl.acm.org/doi/10.1145/3508023",
        "category": "Databases"
    },
    {
        "title": "How Can We Cope with the Impact of Microservice Architecture Smells?",
        "authors": "['Xiang Ding', 'Cheng Zhang']",
        "date": "February 2022",
        "source": "ICSCA '22: Proceedings of the 2022 11th International Conference on Software and Computer Applications",
        "abstract": "Context: Software Architecture Smells (AS) are potential software structure problems and always impact software quality negatively. And with the development of Microservice architecture, the Microservice Architecture Smells (MAS) have been attracting more and more attention. The software scholars and developers have discovered the influence of MAS and performed some researches on them. However, the definition and category of MAS are still ambiguous. Objects: This paper aims to clarify the specific MAS categories and their definitions, and then further explores the issues caused by MAS in the migration process from a monolithic system to Microservice. Method: We performed a comprehensive systematic literature review about MAS. Specifically, we explored 13 white and 10 grey literature in detail to get MAS information by using the quantitative research method. To explore the issues that influence the migration process from a monolithic system to Microservice, we used the meta-ethnography qualitative research method to extract relevant information and get six three-order translations. Results: This study defined 22 Microservice Architecture Smells and classified them into five categories, namely Design, Deployment, Monitor & Log, Communication and Team & Tool, based on their characteristics. Simultaneously, the issues that influence the migration process are proposed, including service cutting, databases, communication, team and techniques. Finally, we matched the MAS to the issues they caused in the migration process and recommended solutions to these issues.",
        "link": "https://dl.acm.org/doi/10.1145/3524304.3524306",
        "category": "Databases"
    },
    {
        "title": "Vulnerabilities of Unattended Face Verification Systems to Facial Components-based Presentation Attacks: An Empirical Study",
        "authors": "['Le Qin', 'Fei Peng', 'Min Long', 'Raghavendra Ramachandra', 'Christoph Busch']",
        "date": "None",
        "source": "ACM Transactions on Privacy and Security",
        "abstract": "As face presentation attacks (PAs) are realistic threats for unattended face verification systems, face presentation attack detection (PAD) has been intensively investigated in past years, and the recent advances in face PAD have significantly reduced the success rate of such attacks. In this article, an empirical study on a novel and effective face impostor PA is made. In the proposed PA, a facial artifact is created by using the most vulnerable facial components, which are optimally selected based on the vulnerability analysis of different facial components to impostor PAs. An attacker can launch a face PA by presenting a facial artifact on his or her own real face. With a collected PA database containing various types of artifacts and presentation attack instruments (PAIs), the experimental results and analysis show that the proposed PA poses a more serious threat to face verification and PAD systems compared with the print, replay, and mask PAs. Moreover, the generalization ability of the proposed PA and the vulnerability analysis with regard to commercial systems are also investigated by evaluating unknown face verification and real-world PAD systems. It provides a new paradigm for the study of face PAs.",
        "link": "https://dl.acm.org/doi/10.1145/3491199",
        "category": "Databases"
    },
    {
        "title": "Features, Frameworks, and Benefits of Gamified Microlearning: A Systematic Literature Review",
        "authors": "['Amanda Putri Septiani', 'Yusep Rosmansyah']",
        "date": "May 2021",
        "source": "ICMET 2021: 2021 3rd International Conference on Modern Educational Technology",
        "abstract": "Gamified microlearning, as an approach that collaborates microlearning and gamification theory is widely adopted nowadays. Users report positive feedbacks on its utilization in various sectors. This paper provides a review of previous and current research to synthesize theory about gamified microlearning, including features, framework, and gamified microlearning benefits by using Systematic Literature Review (SLR) as a method. Reputable journal databases included ACM Digital Library, IEEE Xplore, ScienceDirect, ProQuest, and Scopus were discovered to conduct the SLR. As many as 24.689 articles were found by specific keyword and filtered to 20 eligible articles with good quality and high relevance with gamified microlearning research. This study found 15 main features of gamified microlearning in four main categories, seven frameworks used to develop gamified microlearning platforms, and seven interesting benefits of using gamified microlearning in several various sectors. From the SLR conducted, the use of a gamified microlearning platform in the learning process has increased the motivation and engage students to learn. It also motivates students to voluntarily contribute to learning and supporting gamified microlearning because of its fun environment.",
        "link": "https://dl.acm.org/doi/10.1145/3468978.3469000",
        "category": "Databases"
    },
    {
        "title": "PG-Keys: Keys for Property Graphs",
        "authors": "['Renzo Angles', 'Angela Bonifati', 'Stefania Dumbrava', 'George Fletcher', 'Keith W. Hare', 'Jan Hidders', 'Victor E. Lee', 'Bei Li', 'Leonid Libkin', 'Wim Martens', 'Filip Murlak', 'Josh Perryman', 'Ognjen Savković', 'Michael Schmidt', 'Juan Sequeda', 'Slawek Staworko', 'Dominik Tomaszuk']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We report on a community effort between industry and academia to shape the future of property graph constraints. The standardization for a property graph query language is currently underway through the ISO Graph Query Language (GQL) project. Our position is that this project should pay close attention to schemas and constraints, and should focus next on key constraints. The main purposes of keys are enforcing data integrity and allowing the referencing and identifying of objects. Motivated by use cases from our industry partners, we argue that key constraints should be able to have different modes, which are combinations of basic restriction that require the key to be exclusive, mandatory, and singleton. Moreover, keys should be applicable to nodes, edges, and properties since these all can represent valid real-life entities. Our result is PG-Keys, a flexible and powerful framework for defining key constraints, which fulfills the above goals. PG-Keys is a design by the Linked Data Benchmark Council's Property Graph Schema Working Group, consisting of members from industry, academia, and ISO GQL standards group, intending to bring the best of all worlds to property graph practitioners. PG-Keys aims to guide the evolution of the standardization efforts towards making systems more useful, powerful, and expressive.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457561",
        "category": "Databases"
    },
    {
        "title": "A study of task scheduling algorithms in cloud computing",
        "authors": "['Zakaria Benlalia', 'Abouelmehdi Karim', 'Abderrahim Beni Hssane', 'Abdellah Ezzati']",
        "date": "April 2021",
        "source": "NISS '21: Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security",
        "abstract": "Cloud computing is the provision of information technology (IT) services including servers, storage, databases, network management, etc. Like any new technology, cloud computing requires many improvements and the establishment of precise standards to avoid risks. Task scheduling can be seen as the management and handling of a set of tasks from their start to the step of execution. Scheduling is a negotiation mechanism between two objects, one representing the user (or the ap-plication) and the other the resources. In cloud computing, Task scheduling is of-ten considered as a real challenge to managers. In this paper, we will present some concepts and research papers that have proposed improvements or solutions to this challenge and we will compare some tasks scheduling algorithms in the CloudSim simulator.",
        "link": "https://dl.acm.org/doi/10.1145/3454127.3457616",
        "category": "Databases"
    },
    {
        "title": "TabReformer: Unsupervised Representation Learning for Erroneous Data Detection",
        "authors": "['Mona Nashaat', 'Aindrila Ghosh', 'James Miller', 'Shaikh Quader']",
        "date": "None",
        "source": "ACM/IMS Transactions on Data Science",
        "abstract": "Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.",
        "link": "https://dl.acm.org/doi/10.1145/3447541",
        "category": "Databases"
    },
    {
        "title": "IDEAS: the first quarter century",
        "authors": "['Bipin C Desai']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "This year marks the silver anniversary of IDEAS. It has been an exciting quarter century to shepherd this meeting through good times and not so good ones. We have survived Ebola, MERS and SARS. Whereas the others were local, the COVID pandemic, which still rages, has forced us to move to an on-line version, but thanks to the participants and the dedicated program committee we have continued. This paper is a photographic journey through the years of IDEAS. Unfortunately we have not been able to have the images of all participants over the quarter century oi IDEAS. This is just a sampling of some of the fond moments during the social gatherings of the IDEAS family.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472168",
        "category": "Databases"
    },
    {
        "title": "At-the-time and Back-in-time Persistent Sketches",
        "authors": "['Benwei Shi', 'Zhuoyue Zhao', 'Yanqing Peng', 'Feifei Li', 'Jeff M. Phillips']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In the era of big data, more and more applications require the information of historical data to support rich analytics, learning, and mining operations. In these cases, it is highly desirable to retrieve information of previous versions of data. Traditionally, multi-version databases can be used to store all historical values of the data in order to support historical queries. However, storing all the historical data can be impractical due to its large space consumption. In this paper, we propose the concept of at-the-time persistent (ATTP) and back-in-time persistent (BITP) sketches, which are sketches that approximately answer queries on previous versions of data with small space. We then provide several implementations of ATTP/BITP sketches which are shown to be more efficient compared to existing state-of-the-art solutions in our empirical studies.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452802",
        "category": "Databases"
    },
    {
        "title": "Analysis-oriented Metadata for Data Lakes",
        "authors": "['Yan Zhao', 'Franck Ravat', 'Julien Aligon', 'Chantal Soule-dupuy', 'Gabriel Ferrettini', 'Imen Megdiche']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented an application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472273",
        "category": "Databases"
    },
    {
        "title": "Interpolating concavities",
        "authors": "['José Duarte', 'Mark McKenney']",
        "date": "September 2021",
        "source": "ACM SIGAPP Applied Computing Review",
        "abstract": "Region interpolation methods impose restrictions that have an influence on how the evolution of concavities over time is represented, potentially, causing their evolution to look unnatural, e.g., a concavity unexpectedly appears (disappears) from (to) a point. In a previous work we presented an algorithm to transform a line segment to a concavity, and a line segment to a simple non-closed linestring with possibly several concavities. The algorithm is deterministic, it does not assume that an element in the source is transformed directly to an element (or set of elements) in the target, works in stages (steps), i.e., several different transformations can occur while an element in the source is transformed to an element in the target, and its output is a set of moving segments representing the transformation. The complexity of a non-optimized implementation of the transformation of a segment to a concavity using the algorithm is O(kn2), where k is the number of steps in the transformation (the number of intermediate transformations in the transformation) and n is the number of points in the target geometry. The algorithm is primarily devised to be integrated with the region interpolation methods proposed in the spatiotemporal databases literature. In this work we improve the complexity of the algorithm to be O(n2) in the wort case.",
        "link": "https://dl.acm.org/doi/10.1145/3493499.3493503",
        "category": "Databases"
    },
    {
        "title": "D³Net: Dual-Branch Disturbance Disentangling Network for Facial Expression Recognition",
        "authors": "['Rongyun Mo', 'Yan Yan', 'Jing-Hao Xue', 'Si Chen', 'Hanzi Wang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "One of the main challenges in facial expression recognition (FER) is to address the disturbance caused by various disturbing factors, including common ones (such as identity, pose, and illumination) and potential ones (such as hairstyle, accessory, and occlusion). Recently, a number of FER methods have been developed to explicitly or implicitly alleviate the disturbance involved in facial images. However, these methods either consider only a few common disturbing factors or neglect the prior information of these disturbing factors, thus resulting in inferior recognition performance. In this paper, we propose a novel Dual-branch Disturbance Disentangling Network (D3Net), mainly consisting of an expression branch and a disturbance branch, to perform effective FER. In the disturbance branch, a label-aware sub-branch (LAS) and a label-free sub-branch (LFS) are elaborately designed to cope with different types of disturbing factors. On the one hand, LAS explicitly captures the disturbance due to some common disturbing factors by transfer learning on a pretrained model. On the other hand, LFS implicitly encodes the information of potential disturbing factors in an unsupervised manner. In particular, we introduce an Indian buffet process (IBP) prior to model the distribution of potential disturbing factors in LFS. Moreover, we leverage adversarial training to increase the differences between disturbance features and expression features, thereby enhancing the disentanglement of disturbing factors. By disentangling the disturbance from facial images, we are able to extract discriminative expression features. Extensive experiments demonstrate that our proposed method performs favorably against several state-of-the-art FER methods on both in-the-lab and in-the-wild databases.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475249",
        "category": "Databases"
    },
    {
        "title": "Design And Implementation of Video Learning Platform Based on B / S Architecture",
        "authors": "['Jinlan Kong', 'Qinglei Zhou', 'Mengfei Lin']",
        "date": "October 2021",
        "source": "CSSE '21: Proceedings of the 4th International Conference on Computer Science and Software Engineering",
        "abstract": "With the further popularization of computer technology, communication technology and network technology, students can't live or study without the help of network. Video learning platform is such a benchmark. It has a large number of data resource databases, breaks the boundaries of traditional teaching, reveals the development direction of modern education, and makes the learning environment more diversified and enriched. The system is deeply rooted in the background and significance of today's online education, integrates the practical problems existing in the immature video learning platform in the past, analyzes the specific needs of current teachers and students for the learning platform, grasps the development trend of the future education industry from offline to online, and realizes the effective management of a series of categories such as member courses in online learning. It makes use of the advantages of the computer industry It integrates with models, concepts and methods, adopts MVC three-tier pattern design, based on B / S architecture and Struts + Spring + Hibernate framework design, MySQL and eclipse in parallel, using mature Dao mode to access MySQL database, and comprehensively realizing a complete set of video learning system.",
        "link": "https://dl.acm.org/doi/10.1145/3494885.3494934",
        "category": "Databases"
    },
    {
        "title": "End-to-End Task-Oriented Dialog Modeling With Semi-Structured Knowledge Management",
        "authors": "['Silin Gao', 'Ryuichi Takanobu', 'Antoine Bosselut', 'Minlie Huang']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "Current task-oriented dialog (TOD) systems mostly manage structured knowledge (e.g. databases and tables) to guide the goal-oriented conversations. However, they fall short of handling dialogs which also involve unstructured knowledge (e.g. reviews and documents). In this article, we formulate a task of modeling TOD grounded on a fusion of structured and unstructured knowledge. To address this task, we propose a TOD system with semi-structured knowledge management, SeKnow, which extends the belief state to manage knowledge with both structured and unstructured contents. Furthermore, we introduce two implementations of SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained language model, respectively. Both implementations use the end-to-end manner to jointly optimize dialog modeling grounded on structured and unstructured knowledge. We conduct experiments on a modified version of MultiWOZ 2.1 dataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve semi-structured knowledge. Experimental results show that SeKnow has strong performances in both end-to-end dialog and intermediate knowledge management, compared to existing TOD systems and their extensions with pipeline knowledge management schemes.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3153255",
        "category": "Databases"
    },
    {
        "title": "Open data in digital strategies against COVID-19: the case of Belgium",
        "authors": "['Robert Viseur']",
        "date": "September 2021",
        "source": "OpenSym '21: Proceedings of the 17th International Symposium on Open Collaboration",
        "abstract": "COVID-19 has highlighted the importance of digital in the fight against the pandemic (control at the border, automated tracing, creation of databases...). In this research, we analyze the Belgian response in terms of open data. First, we examine the open data publication strategy in Belgium (a federal state with a sometimes complex functioning, especially in health), second, we conduct a case study (anatomy of the pandemic in Belgium) in order to better understand the strengths and weaknesses of the main COVID-19 open data repository. And third, we analyze the obstacles to open data publication. Finally, we discuss the Belgian COVID-19 open data strategy in terms of data availability, data relevance and knowledge management. In particular, we show how difficult it is to optimize the latter in order to make the best use of governmental, private and academic open data in a way that has a positive impact on public health policy.",
        "link": "https://dl.acm.org/doi/10.1145/3479986.3479988",
        "category": "Databases"
    },
    {
        "title": "Diet Planning with Machine Learning: Teacher-forced REINFORCE for Composition Compliance with Nutrition Enhancement",
        "authors": "['Changhun Lee', 'Soohyeok Kim', 'Chiehyeon Lim', 'Jayun Kim', 'Yeji Kim', 'Minyoung Jung']",
        "date": "August 2021",
        "source": "KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Diet planning is a basic and regular human activity. Previous studies have considered diet planning a combinatorial optimization problem to generate solutions that satisfy a diet's nutritional requirements. However, this approach does not consider the composition of diets, which is critical for diet recipients' to accept and enjoy menus with high nutritional quality. Without this consideration, feasible solutions for diet planning could not be provided in practice. This suggests the necessity of diet planning with machine learning, which extracts implicit composition patterns from real diet data and applies these patterns when generating diets. This work is original research that defines diet planning as a machine learning problem; we describe diets as sequence data and solve a controllable sequence generation problem. Specifically, we develop the Teacher-forced REINFORCE algorithm to connect neural machine translation and reinforcement learning for composition compliance with nutrition enhancement in diet generation. Through a real-world application to diet planning for children, we validated the superiority of our work over the traditional combinatorial optimization and modern machine learning approaches, as well as human (i.e., professional dietitians) performance. In addition, we construct and open the databases of menus and diets to motivate and promote further research and development of diet planning with machine learning. We believe this work with data science will contribute to solving economic and social problems associated with diet planning.",
        "link": "https://dl.acm.org/doi/10.1145/3447548.3467201",
        "category": "Databases"
    },
    {
        "title": "ICIX: A Semantic Information Extraction Architecture",
        "authors": "['Angel Luis Garrido', 'Alvaro Peiro', 'Carlos Bobed', 'Eduardo Mena', 'Cristian Morte']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Public and private organizations produce and store huge amounts of documents which contain information about their domains in non-structured formats. Although from the final user’s point of view we can rely on different retrieval tools to access such data, the progressive structuring of such documents has important benefits for daily operations. While there exist many approaches to extract information in open domains, we lack tools flexible enough to adapt themselves to the particularities of different domains.  In this paper, we present the design and implementation of ICIX, an architecture to extract structured information from text documents. ICIX aims at obtaining specific information within a given domain, defined by means of an ontology which guides the extraction process. Besides, to optimize such an extraction, ICIX relies on document classification and data curation adapted to the particular domain. Our proposal has been implemented and evaluated in the specific context of managing legal documents, with promising results.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472174",
        "category": "Databases"
    },
    {
        "title": "Agile Transformation Challenges and Solutions in Bureaucratic Government: A Systematic Literature Review",
        "authors": "['Hendy Dwi Harfianto', 'Teguh Raharjo', 'Bob Hardian', 'Andi Wahbi']",
        "date": "January 2022",
        "source": "ICCMB '22: Proceedings of the 2022 5th International Conference on Computers in Management and Business",
        "abstract": "Information technology (IT) provides ease, convenience, and efficiency to run some errands. IT is not only facilitating services to customers, but also as the main backbone to drive business and management in an organization. A lot of IT projects were established to solve problems or improve business problems with IT. The urgency to quickly execute IT projects within a short time made some organizations have to consider changing their development methods from predictive (waterfall) to adaptive (agile). However, transforming to agile in public sector, especially a convoluted hierarchical organization like government, was not as easy as less bureaucratic ones. A layered bureaucracy could hinder the application of agile methods. This study was conducted to summarize challenges of agile transformation in a bureaucratic government and its solutions. A Systematic Literature Review (SLR) was applied as the main method of this research. There were 14 relevant papers from the past 5 years identified from six databases resulting in 20 challenges grouped into 4 categories using TOEI (technology-organization-environment-individual) framework. Several solutions to each challenge were mapped as a recommendation for other bureaucratic governments before transforming to agile. The findings can also be used for future research exploration of agile transformation challenges in other organization types.",
        "link": "https://dl.acm.org/doi/10.1145/3512676.3512679",
        "category": "Databases"
    },
    {
        "title": "Prognostic Potential of CCT5 in Hepatocellular Carcinoma",
        "authors": "['Jun Wang', 'Jiahao Chen', 'Huimin Zhang', 'You Huang', 'Chao Shen', 'Xinghua Liao']",
        "date": "May 2021",
        "source": "ICBBT '21: Proceedings of the 2021 13th International Conference on Bioinformatics and Biomedical Technology",
        "abstract": "Chaperonin containing TCP1 subunit 5)CCT5( is a member of the companion protein containing the TCP1 complex (CCT), also known as the TCP1 loop complex (TRiC). The expression of CCT5 is abnormal in many cancers, but the biological role of CCT5 in hepatocellular carcinoma (LIHC) remains to be determined. We analyzed the expression of CCT5 using multiple gene expression databases. The genes co-expressed with CCT5 were identified using linkedonomics. The correlation between CCT5 and tumor immune infiltrates was studied by using tumor immune estimation resource (TIMER). CCT5 was found to be up-regulated in tumor tissues in multiple LIHC queues. High CCT5 expression was associated with overall survival (OS), relapse-free survival (RFS), progression-free survival (PFS) and disease-specific survival (DSS). Functional network analysis indicated that CCT5 was correlated with rectome Gene Formation Sets of tubulin folding intermediates by CCT/TRIC, ribonuclei complex protein biogenesis, rectome Gene Hsp90 chaoperone cycle for steroid hormone receptors (SHR). Significantly, CCT5 expression was positively correlated with the infiltration levels of B cell, CD4+T and CD8+T cells, macrophages, neutrophil, and dendritic cell. The expression of CCT5 is closely related to multiple immune markers in hepatocellular carcinoma. These findings suggest that CCT5 is associated with the prognosis and immunologic invasion of LIHC, and provide a basis for further study of the immunoregulatory role of CCT5 in LIHC.",
        "link": "https://dl.acm.org/doi/10.1145/3473258.3473281",
        "category": "Databases"
    },
    {
        "title": "GraphScope: a unified engine for big graph processing",
        "authors": "['Wenfei Fan', 'Tao He', 'Longbin Lai', 'Xue Li', 'Yong Li', 'Zhao Li', 'Zhengping Qian', 'Chao Tian', 'Lei Wang', 'Jingbo Xu', 'Youyang Yao', 'Qiang Yin', 'Wenyuan Yu', 'Jingren Zhou', 'Diwen Zhu', 'Rong Zhu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "GraphScope is a system and a set of language extensions that enable a new programming interface for large-scale distributed graph computing. It generalizes previous graph processing frameworks (e.g., Pregel, GraphX) and distributed graph databases (e.g., Janus-Graph, Neptune) in two important ways: by exposing a unified programming interface to a wide variety of graph computations such as graph traversal, pattern matching, iterative algorithms and graph neural networks within a high-level programming language; and by supporting the seamless integration of a highly optimized graph engine in a general purpose data-parallel computing system.A GraphScope program is a sequential program composed of declarative data-parallel operators, and can be written using standard Python development tools. The system automatically handles the parallelization and distributed execution of programs on a cluster of machines. It outperforms current state-of-the-art systems by enabling a separate optimization (or family of optimizations) for each graph operation in one carefully designed coherent framework. We describe the design and implementation of GraphScope and evaluate system performance using several real-world applications.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476369",
        "category": "Databases"
    },
    {
        "title": "A Framework for Enhancing Deep Learning Based Recommender Systems with Knowledge Graphs",
        "authors": "['Sudhir P. Mudur', 'Serguei A Mokhov', 'Yuhao Mao']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Recommendation methods fall into three major categories, content based filtering, collaborative filtering and deep learning based. Information about products and the preferences of earlier users are used in an unsupervised manner to create models which help make personalized recommendations to a specific new user. The more information we provide to these methods, the more likely it is that they yield better recommendations. Deep learning based methods are relatively recent, and are generally more robust to noise and missing information. This is because deep learning models can be trained even when some of the information records have partial information. Knowledge graphs represent the current trend in recording information in the form of relations between entities, and can provide any available information about products and users. This information is used to train the recommendation model. In this work, we present a new generic recommender systems framework, that integrates knowledge graphs into the recommendation pipeline. We describe its design and implementation, and then show through experiments, how such a framework can be specialized, taking the domain of movies as an example, and the resulting improvements in recommendations made possible by using all the information obtained using knowledge graphs. Our framework, to be made publicly available, supports different knowledge graph representation formats, and facilitates format conversion, merging and information extraction needed for training recommendation models.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472183",
        "category": "Databases"
    },
    {
        "title": "Machine Unlearning: Its Need and Implementation Strategies",
        "authors": "['Aman Tahiliani', 'Vikas Hassija', 'Vinay Chamola', 'Mohsen Guizani']",
        "date": "August 2021",
        "source": "IC3-2021: Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing",
        "abstract": "Generally when users share information about themselves on some online platforms, they knowingly or unknowingly allow this data to be used by the companies behind these companies for various purposes including selling this information to advertisers as well as using it to better enrich their predictive models. In the event of the user changing their minds on allowing such data about them to be able to be used by the companies, it becomes a strenuous task for the companies to get rid of the influence of this collected data, especially when it has been used to train their machine learning models. Recent legislations by governing bodies, like the European Union, grant people the right to choose where data about them may be used, including a right to have their data and its resulting influence be completely removed from a company’s databases and machine learning models. To be able to do this at scale new machine unlearning solutions need to be invented. In this paper, we look at some of these early models of machine unlearning strategies that have been proposed.",
        "link": "https://dl.acm.org/doi/10.1145/3474124.3474158",
        "category": "Databases"
    },
    {
        "title": "Opinion Mining for Software Development: A Systematic Literature Review",
        "authors": "['Bin Lin', 'Nathan Cassee', 'Alexander Serebrenik', 'Gabriele Bavota', 'Nicole Novielli', 'Michele Lanza']",
        "date": "None",
        "source": "ACM Transactions on Software Engineering and Methodology",
        "abstract": "Opinion mining, sometimes referred to as sentiment analysis, has gained increasing attention in software engineering (SE) studies. SE researchers have applied opinion mining techniques in various contexts, such as identifying developers’ emotions expressed in code comments and extracting users’ critics toward mobile apps. Given the large amount of relevant studies available, it can take considerable time for researchers and developers to figure out which approaches they can adopt in their own studies and what perils these approaches entail.We conducted a systematic literature review involving 185 papers. More specifically, we present (1) well-defined categories of opinion mining-related software development activities, (2) available opinion mining approaches, whether they are evaluated when adopted in other studies, and how their performance is compared, (3) available datasets for performance evaluation and tool customization, and (4) concerns or limitations SE researchers might need to take into account when applying/customizing these opinion mining techniques. The results of our study serve as references to choose suitable opinion mining tools for software development activities and provide critical insights for the further development of opinion mining techniques in the SE domain.",
        "link": "https://dl.acm.org/doi/10.1145/3490388",
        "category": "Databases"
    },
    {
        "title": "Structural Generalizability: The Case of Similarity Search",
        "authors": "['Yodsawalai Chodpathumwan', 'Arash Termehchy', 'Stephen A. Ramsey', 'Aayam Shrestha', 'Amy Glen', 'Zheng Liu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Supervised and Unsupervised ML algorithms are widely used over graphs. They use the structural properties of the data to deliver effective results. It is known that the same information can be represented under various graph structures. Thus, these algorithms may be effective on some structural variations of the data and ineffective on others. One would like to have an algorithm that is effective and generalizes to all structural variations of a data graph. We define the concept of structural generalizability for algorithms over graphs. We focus on the problem of similarity search, which is a popular task and the building block of many ML algorithms on graphs, and propose a structurally generalizable similarity search algorithm. As this algorithm may require users to specify features in a rather complex language, we modify this algorithm so that it requires only simple guidance from the user. Our extensive empirical study show that our algorithms are structurally generalizable while being efficient and more effective than current algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457316",
        "category": "Databases"
    },
    {
        "title": "Mining High Utility Itemsets with Hill Climbing and Simulated Annealing",
        "authors": "['M. Saqib Nawaz', 'Philippe Fournier-Viger', 'Unil Yun', 'Youxi Wu', 'Wei Song']",
        "date": "None",
        "source": "ACM Transactions on Management Information Systems",
        "abstract": "High utility itemset mining (HUIM) is the task of finding all items set, purchased together, that generate a high profit in a transaction database. In the past, several algorithms have been developed to mine high utility itemsets (HUIs). However, most of them cannot properly handle the exponential search space while finding HUIs when the size of the database and total number of items increases. Recently, evolutionary and heuristic algorithms were designed to mine HUIs, which provided considerable performance improvement. However, they can still have a long runtime and some may miss many HUIs. To address this problem, this article proposes two algorithms for HUIM based on Hill Climbing (HUIM-HC) and Simulated Annealing (HUIM-SA). Both algorithms transform the input database into a bitmap for efficient utility computation and for search space pruning. To improve population diversity, HUIs discovered by evolution are used as target values for the next population instead of keeping the current optimal values in the next population. Through experiments on real-life datasets, it was found that the proposed algorithms are faster than state-of-the-art heuristic and evolutionary HUIM algorithms, that HUIM-SA discovers similar HUIs, and that HUIM-SA evolves linearly with the number of iterations.",
        "link": "https://dl.acm.org/doi/10.1145/3462636",
        "category": "Databases"
    },
    {
        "title": "Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity",
        "authors": "['Soror Sahri', 'Rim Moussa']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi’ trips.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472195",
        "category": "Databases"
    },
    {
        "title": "Mobile Automated Fingerprint Identification System (MAFIS): An Android-based Criminal Tracking System using Fingerprint Minutiae Structure",
        "authors": "['William P. Rey', 'Gisela V. Rolluqui']",
        "date": "August 2021",
        "source": "ICSET 2021: 2021 5th International Conference on E-Society, E-Education and E-Technology",
        "abstract": "The Philippines, being an archipelago consisting of 7 640 islands, faces challenges on criminal identification processes turnaround time. All fingerprints recovered from the crime scenes needed to be forwarded to the national headquarter for identification and resolution. These are being compared and matched against a database of known and unknown fingerprints. The result is then delivered back to the requesting agent. In this purview, this study aims to design and develop a Mobile Automated Fingerprint Identification System (MAFIS) – An Android-based Criminal Tracking System. In this study, the prototype developed consists of an android mobile app, a web-based app, and a desktop app extension of the web app. The android mobile app was designed for roaming scenes of the crime operative (SOCO) agents. This allowed remote users deployed on-site to perform an initial inquiry of the latent fingerprint recovered and processed from the crime scene. The mobile app can search, compare, and match against the latent and tenprint databases. If there is a match, the system will display the similarity percentage of the analysis and add or link it to the existing record; otherwise, it will be counted as a new record. It also permits users to view the system dashboard, access the crime management module, search history, and manage profiles. The web-based app has the same capability as the mobile app but with added features like tenprint identification. The web app was primarily designed for police sub-stations deployment used by duty investigators to capture tenprints from law-offenders during the suspect booking process. This can match and identify tenprint from the database. If there is a match, a new criminal record will be added to the existing criminal records of the law offenders. If there is no match, the system will create a new record and store it on the system database.",
        "link": "https://dl.acm.org/doi/10.1145/3485768.3485773",
        "category": "Databases"
    },
    {
        "title": "Integrating Line Weber Local Descriptor and Deep Feature for Tire Indentation Mark Image Classification",
        "authors": "['Ying Liu', 'Xin Che', 'Haitao Dong', 'Daxiang Li', 'Shyh Wei Teng', 'Guojun Lu']",
        "date": "September 2021",
        "source": "AIPR '21: Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition",
        "abstract": "Tire indentation mark matching is an essential tool used for the investigation of criminal cases and traffic incidents. As such images are unique and uncommon, there is a lack of dedicated databases and relevant research on this topic. This paper presents a feature extraction algorithm effective for tire indentation mark image description. The main contributions include: (1) Line feature Weber local descriptor (LWLD) is proposed, which uses the Gabor orientations instead of the original gradient orientation. This feature can describe texture information of tire indentation mark image more efficiently. (2) An attention model is constructed to produce attention feature map of tire indentation mark image. This attention feature map is then fused with LWLD resulting in a feature with more powerful representation capability. Experimental results prove that the combined use of LWLD and attention model greatly enhances the performance of tire indentation mark image matching tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3488933.3488948",
        "category": "Databases"
    },
    {
        "title": "Online Landmark-Based Batch Processing of Shortest Path Queries",
        "authors": "['Manuel Hotz', 'Theodoros Chondrogiannis', 'Leonard Wörteler', 'Michael Grossniklaus']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Processing shortest path queries is a basic operation in many graph problems. Both preprocessing-based and batch processing techniques have been proposed to speed up the computation of a single shortest path by amortizing its costs. However, both of these approaches suffer from limitations. The former techniques are prohibitively expensive in situations where the precomputed information needs to be updated frequently due to changes in the graph, while the latter require coordinates and cannot be used on non-spatial graphs. In this paper, we address both limitations and propose novel techniques for batch processing shortest paths queries using landmarks. We show how preprocessing can be avoided entirely by integrating the computation of landmark distances into query processing. Our experimental results demonstrate that our techniques outperform the state of the art on both spatial and non-spatial graphs with a maximum speedup of 3.61 × in online scenarios.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468844",
        "category": "Databases"
    },
    {
        "title": "Disaster Documentation Revisited: The Evolving Damage Assessments of Emergency Management in Oregon",
        "authors": "['Henry Covey']",
        "date": "October 2021",
        "source": "SIGDOC '21: Proceedings of the 39th ACM International Conference on Design of Communication",
        "abstract": "This report revisits a previous case study focused on the computing machinery and design of communication that are employed at the local, county, regional, state, and federal levels in Oregon to collect, review, and publish damage assessments of disasters and other emergency events. Since the last report, emergency managers throughout Oregon have faced numerous disaster incidents, including the COVID-19 pandemic, ice storms, flooding, and some of the worst heat waves, drought conditions, and megafires on record, with the threat of more to come in the years ahead. After years of research and development, fueled by lessons learned from a catastrophic wildfire season, a new generation of damage assessment tools and shared services has been pushed to the fore, ones which integrate geographic information systems and relational spatial databases not only to help assess damage but also automate and coordinate workflows. This revisitation explores the urgency and impetus for change and analyzes the Oregon Damage Assessment Project, a statewide initiative of the Office of Emergency Management to standardize shared tools and services for government agencies, partner organizations, and the public at large.",
        "link": "https://dl.acm.org/doi/10.1145/3472714.3473625",
        "category": "Databases"
    },
    {
        "title": "Motion recommendation for online character control",
        "authors": "['Kyungmin Cho', 'Chaelin Kim', 'Jungjin Park', 'Joonkyu Park', 'Junyong Noh']",
        "date": "10 December 2021",
        "source": "ACM Transactions on Graphics",
        "abstract": "Reinforcement learning (RL) has been proven effective in many scenarios, including environment exploration and motion planning. However, its application in data-driven character control has produced relatively simple motion results compared to recent approaches that have used large complex motion data without RL. In this paper, we provide a real-time motion control method that can generate high-quality and complex motion results from various sets of unstructured data while retaining the advantage of using RL, which is the discovery of optimal behaviors by trial and error. We demonstrate the results for a character achieving different tasks, from simple direction control to complex avoidance of moving obstacles. Our system works equally well on biped/quadruped characters, with motion data ranging from 1 to 48 minutes, without any manual intervention. To achieve this, we exploit a finite set of discrete actions, where each action represents full-body future motion features. We first define a subset of actions that can be selected in each state and store these pieces of information in databases during the preprocessing step. The use of this subset of actions enables the effective learning of control policy even from a large set of motion data. To achieve interactive performance at run-time, we adopt a proposal network and a k-nearest neighbor action sampler.",
        "link": "https://dl.acm.org/doi/10.1145/3478513.3480512",
        "category": "Databases"
    },
    {
        "title": "Big metadata: when metadata is big data",
        "authors": "['Pavan Edara', 'Mosha Pasumansky']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The rapid emergence of cloud data warehouses like Google BigQuery has redefined the landscape of data analytics. With the growth of data volumes, such systems need to scale to hundreds of EiB of data in the near future. This growth is accompanied by an increase in the number of objects stored and the amount of metadata such systems must manage. Traditionally, Big Data systems have tried to reduce the amount of metadata in order to scale the system, often compromising query performance. In Google BigQuery, we built a metadata management system that demonstrates that massive scale can be achieved without such tradeoffs. We recognized the benefits that fine grained metadata provides for query processing and we built a metadata system to manage it effectively. We use the same distributed query processing and data management techniques that we use for managing data to handle Big metadata. Today, BigQuery uses these techniques to support queries over billions of objects and their metadata.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476385",
        "category": "Databases"
    },
    {
        "title": "Full encryption: an end to end encryption mechanism in GaussDB",
        "authors": "['Jinwei Zhu', 'Kun Cheng', 'Jiayang Liu', 'Liang Guo']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In this paper, we present a novel mechanism called Full Encryption (FE) in GaussDB. FE-in-GaussDB provides column-level encryption for sensitive data, and secures the asset from any malicious cloud administrator or information leakage attack. It ensures not only the security of operations on ciphertext data, but also the efficiency of query execution, by combining the advantages of cryptography algorithms (i.e. software mode) and Trusted Execution Environment (i.e. hardware mode). With this, FE-in-GaussDB supports full-scene query processing including the matching, the comparison and other rich computing functionalities. We demonstrate the prototype of FE-in-GaussDB and an experimental performance evaluation to prove its availability and effectiveness.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476351",
        "category": "Databases"
    },
    {
        "title": "A Deep Learning-Based Method for Identification of Bacteriophage-Host Interaction",
        "authors": "['Menglu Li', 'Yanan Wang', 'Fuyi Li', 'Yun Zhao', 'Mengya Liu', 'Sijia Zhang', 'Yannan Bin', 'A. Ian Smith', 'Geoffrey I. Webb', 'Jian Li', 'Jiangning Song', 'Junfeng Xia']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Multi-drug resistance (MDR) has become one of the greatest threats to human health worldwide, and novel treatment methods of infections caused by MDR bacteria are urgently needed. Phage therapy is a promising alternative to solve this problem, to which the key is correctly matching target pathogenic bacteria with the corresponding therapeutic phage. Deep learning is powerful for mining complex patterns to generate accurate predictions. In this study, we develop PredPHI (<underline>Pred</underline>icting <underline>P</underline>hage-<underline>H</underline>ost <underline>I</underline>nteractions), a deep learning-based tool capable of predicting the host of phages from sequence data. We collect &#x003E;3000 phage-host pairs along with their protein sequences from PhagesDB and GenBank databases and extract a set of features. Then we select high-quality negative samples based on the K-Means clustering method and construct a balanced training set. Finally, we employ a deep convolutional neural network to build the predictive model. The results indicate that PredPHI can achieve a predictive performance of 81 percent in terms of the area under the receiver operating characteristic curve on the test set, and the clustering-based method is significantly more robust than that based on randomly selecting negative samples. These results highlight that PredPHI is a useful and accurate tool for identifying phage-host interactions from sequence data.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2020.3017386",
        "category": "Databases"
    },
    {
        "title": "Evaluation of Assistive Technologies from the perspective of Usability, User Experience and Accessibility: a Systematic Mapping Study",
        "authors": "['Tatiany Xavier de Godoi', 'Guilherme Corredato Guerino', 'Natasha Malveira C. Valentim']",
        "date": "October 2021",
        "source": "IHC '21: Proceedings of the XX Brazilian Symposium on Human Factors in Computing Systems",
        "abstract": "Assistive Technologies (AT) are technologies that promote and extend the functional abilities of the users. Unfortunately, several technologies are abandoned by their users' difficulties, such as handling, high cognitive load, need for follow-up service, and device quality. To improve the quality of ATs and minimize their failures, evaluations of these ATs are performed. When an AT is evaluated, one should think of every user who can use it, thus making these ATs more accessible, more usable, and providing a good user experience (UX). However, several studies have investigated aspects of Usability, UX, and Accessibility separately. Therefore, it is required to characterize state of the art about AT evaluation related to these three software quality factors. This paper aims to show the results of a Systematic Mapping Study (SMS) on technologies that can be employed to evaluate the factors of Usability, User Experience (UX), and Accessibility to add more quality to AT. From 368 initial papers found in three research databases, 112 relevant papers were extracted based on the inclusion criteria. None of the studied initiatives evaluate the AT from the perspective of Usability, Accessibility, and UX together. Also, few evaluation instruments are tested with the public with which AT is directed. There are also few evaluation instruments for deaf users.",
        "link": "https://dl.acm.org/doi/10.1145/3472301.3484323",
        "category": "Databases"
    },
    {
        "title": "Sub-trajectory Similarity Join with Obfuscation",
        "authors": "['Yanchuan Chang', 'Jianzhong Qi', 'Egemen Tanin', 'Xingjun Ma', 'Hanan Samet']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "User trajectory data is becoming increasingly accessible due to the prevalence of GPS-equipped devices such as smartphones. Many existing studies focus on querying trajectories that are similar to each other in their entirety. We observe that trajectories partially similar to each other contain useful information about users’ travel patterns which should not be ignored. Such partially similar trajectories are critical in applications such as epidemic contact tracing. We thus propose to query trajectories that are within a given distance range from each other for a given period of time. We formulate this problem as a sub-trajectory similarity join query named as the STS-Join. We further propose a distributed index structure and a query algorithm for STS-Join, where users retain their raw location data and only send obfuscated trajectories to a server for query processing. This helps preserve user location privacy which is vital when dealing with such data. Theoretical analysis and experiments on real data confirm the effectiveness and the efficiency of our proposed index structure and query algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468822",
        "category": "Databases"
    },
    {
        "title": "Distributed Enumeration of Four Node Graphlets at Quadrillion-Scale",
        "authors": "['Xiaozhou Liu', 'Yudi Santoso', 'Venkatesh Srinivasan', 'Alex Thomo']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Graphlet enumeration is a basic task in graph analysis with many applications. Thus it is important to be able to perform this task within a reasonable amount of time. However, this objective is challenging when the input graph is very large, with millions of nodes and edges. Known solutions are limited in terms of scalability. Distributed computing is often proposed as a solution to improve scalability. However, it has to be done carefully to reduce the overhead cost and to really benefit from the distributed solution. We study the enumeration of four-node graphlets in undirected graphs using a distributed platform. We propose an efficient distributed solution which significantly surpasses the existing solutions. With this method we are able to process larger graphs that have never been processed before and enumerate quadrillions of graphlets using a modest cluster of machines. We show the scalability of our solution through experimental results. Finally, we also extend our algorithm to enumerate graphlets in probabilistic graphs and demonstrate its suitability for this case.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468805",
        "category": "Databases"
    },
    {
        "title": "Defining Data Literacy Communities by Their Objectives: A Text Mining Analysis",
        "authors": "['Ahmed Mohamed Fahmy Yousef', 'Johanna Catherine Walker', 'Manuel Leon-Urrutia']",
        "date": "June 2021",
        "source": "WebSci '21 Companion: Companion Publication of the 13th ACM Web Science Conference 2021",
        "abstract": "Data literacy is a multidimensional concept that attracts the attention of a variety of communities of practice, from different angles. The authors grouped these communities of practice in three categories: education, fields and professions, and citizenship. The meaning of data literacy varies depending on who uses it, and its concept is often conveyed in terms other than data literacy. This paper addresses the problematization of data literacy as a term by examining academic literature around it. To this end, a desk study was carried out to gather sources where the term is used. After an extensive search in the main academic databases and a subsequent PRISMA selection process, automated content analysis was applied to the gathered sources. The findings suggest that the concept of data literacy has a different treatment in different communities of practice. For example, librarians and citizen scientists have a different understanding of the concept of data literacy.",
        "link": "https://dl.acm.org/doi/10.1145/3462741.3466663",
        "category": "Databases"
    },
    {
        "title": "Simultaneous temporal and spatial deep attention for imaged skeleton-based action recognition",
        "authors": "['Mohamed Lamine Rouali', 'Said Yacine Boulahia', 'Abdenour Amamra']",
        "date": "July 2021",
        "source": "PRIS '21: Proceedings of the 2021 International Conference on Pattern Recognition and Intelligent Systems",
        "abstract": "The use of skeletons as a modality to represent and recognize human actions has gained interest thanks to the compactness of the data, their reliable representativeness in addition to their strong robustness. The deep learning based recognition approaches which are based on it often propose to improve the recognition pipeline by integrating the concept of attention in their modeling. The idea is to allow the model to focus on the relevant information of the action instead of attempting some kind of blind modeling. In this article, we propose an action recognition approach integrating simultaneously both spatial and temporal attentions. We first perform a transformation of the input sequence data into a color matrix, called imaged skeleton, comprising Cartesian and rotational information. Then, this new representation is given as input to an architecture composed of a main trunk, that allows features extraction and classification, and several attention branches. Different experimental evaluations on two popular benchmark databases, namely UT-Kinect [1] and SBU Kinect Interaction [2], are conducted to verify the interest of our proposed approach, where better performances are reported.  Index: convolutional neural network, spatio-temporal, skeleton-based action recognition, deep attention.",
        "link": "https://dl.acm.org/doi/10.1145/3480651.3480668",
        "category": "Databases"
    },
    {
        "title": "DLRF-Net: A Progressive Deep Latent Low-Rank Fusion Network for Hierarchical Subspace Discovery",
        "authors": "['Zhao Zhang', 'Jiahuan Ren', 'Haijun Zhang', 'Zheng Zhang', 'Guangcan Liu', 'Shuicheng Yan']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Low-rank coding-based representation learning is powerful for discovering and recovering the subspace structures in data, which has obtained an impressive performance; however, it still cannot obtain deep hidden information due to the essence of single-layer structures. In this article, we investigate the deep low-rank representation of images in a progressive way by presenting a novel strategy that can extend existing single-layer latent low-rank models into multiple layers. Technically, we propose a new progressive Deep Latent Low-Rank Fusion Network (DLRF-Net) to uncover deep features and the clustering structures embedded in latent subspaces. The basic idea of DLRF-Net is to progressively refine the principal and salient features in each layer from previous layers by fusing the clustering and projective subspaces, respectively, which can potentially learn more accurate features and subspaces. To obtain deep hidden information, DLRF-Net inputs shallow features from the last layer into subsequent layers. Then, it aims at recovering the hierarchical information and deeper features by respectively congregating the subspaces in each layer of the network. As such, one can also ensure the representation learning of deeper layers to remove the noise and discover the underlying clean subspaces, which will be verified by simulations. It is noteworthy that the framework of our DLRF-Net is general and is applicable to most existing latent low-rank representation models, i.e., existing latent low-rank models can be easily extended to the multilayer scenario using DLRF-Net. Extensive results on real databases show that our framework can deliver enhanced performance over other related techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3402030",
        "category": "Databases"
    },
    {
        "title": "Capturing Expert Knowledge for Building Enterprise SME Knowledge Graphs",
        "authors": "['Martin Mansfield', 'Valentina Tamma', 'Phil Goddard', 'Frans Coenen']",
        "date": "December 2021",
        "source": "K-CAP '21: Proceedings of the 11th Knowledge Capture Conference",
        "abstract": "Whilst Knowledge Graphs (KGs) are increasingly used in business scenarios, the construction of enterprise ontologies and the population of KGs from existing relational data remains a significant challenge. In this paper we report our experience in supporting CSols (an SME operating in the analytical laboratory domain) in transitioning their data from legacy databases to a bespoke KG. We modelled the KG using a streamlined approach based on state of the art ontology engineering methodologies, that addresses the challenges faced by SMEs when transitioning to new technologies: lack of resources to devote to the transition, paucity of comprehensive data governance policies, and resistance within the organisation to accepting new practices and knowledge. Our approach uses a combination of UML diagrams and a controlled language glossary to support stakeholders in reaching consensus during the knowledge capture phase, thus reducing the intervention of the ontology engineer only to cases where no agreement can be found. We present a case study illustrating the generation of the KG from a UML specification of part of the analytical domain and from legacy relational data, and we discuss the benefits and challenges of the approach.",
        "link": "https://dl.acm.org/doi/10.1145/3460210.3493569",
        "category": "Databases"
    },
    {
        "title": "FLAT: fast, lightweight and accurate method for cardinality estimation",
        "authors": "['Rong Zhu', 'Ziniu Wu', 'Yuxing Han', 'Kai Zeng', 'Andreas Pfadler', 'Zhengping Qian', 'Jingren Zhou', 'Bin Cui']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Query optimizers rely on accurate cardinality estimation (CardEst) to produce good execution plans. The core problem of CardEst is how to model the rich joint distribution of attributes in an accurate and compact manner. Despite decades of research, existing methods either over-simplify the models only using independent factorization which leads to inaccurate estimates, or over-complicate them by lossless conditional factorization without any independent assumption which results in slow probability computation. In this paper, we propose FLAT, a CardEst method that is simultaneously <u>f</u>ast in probability computation, <u>l</u>ightweight in model size and <u>a</u>ccurate in es<u>t</u>imation quality. The key idea of FLAT is a novel unsupervised graphical model, called FSPN. It utilizes both independent and conditional factorization to adaptively model different levels of attributes correlations, and thus combines their advantages. FLAT supports efficient online probability computation in near linear time on the underlying FSPN model, provides effective offline model construction and enables incremental model updates. It can estimate cardinality for both single table queries and multi-table join queries. Extensive experimental study demonstrates the superiority of FLAT over existing CardEst methods: FLAT achieves 1--5 orders of magnitude better accuracy, 1--3 orders of magnitude faster probability computation speed and 1--2 orders of magnitude lower storage cost. We also integrate FLAT into Postgres to perform an end-to-end test. It improves the query execution time by 12.9% on the well-known IMDB benchmark workload, which is very close to the optimal result 14.2% using the true cardinality.",
        "link": "https://dl.acm.org/doi/10.14778/3461535.3461539",
        "category": "Databases"
    },
    {
        "title": "Mining High Utility Itemsets with Hill Climbing and Simulated Annealing",
        "authors": "['M. Saqib Nawaz', 'Philippe Fournier-Viger', 'Unil Yun', 'Youxi Wu', 'Wei Song']",
        "date": "None",
        "source": "ACM Transactions on Management Information Systems",
        "abstract": "High utility itemset mining (HUIM) is the task of finding all items set, purchased together, that generate a high profit in a transaction database. In the past, several algorithms have been developed to mine high utility itemsets (HUIs). However, most of them cannot properly handle the exponential search space while finding HUIs when the size of the database and total number of items increases. Recently, evolutionary and heuristic algorithms were designed to mine HUIs, which provided considerable performance improvement. However, they can still have a long runtime and some may miss many HUIs. To address this problem, this article proposes two algorithms for HUIM based on Hill Climbing (HUIM-HC) and Simulated Annealing (HUIM-SA). Both algorithms transform the input database into a bitmap for efficient utility computation and for search space pruning. To improve population diversity, HUIs discovered by evolution are used as target values for the next population instead of keeping the current optimal values in the next population. Through experiments on real-life datasets, it was found that the proposed algorithms are faster than state-of-the-art heuristic and evolutionary HUIM algorithms, that HUIM-SA discovers similar HUIs, and that HUIM-SA evolves linearly with the number of iterations.",
        "link": "https://dl.acm.org/doi/10.1145/3462636",
        "category": "Databases"
    },
    {
        "title": "Measuring Quality of Workers by Goodness-of-Fit of Machine Learning Model in Crowdsourcing",
        "authors": "['Yu Suzuki']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "In this paper, we propose a method for predicting the quality of crowdsourcing workers using the goodness-of-fit (GoF) of machine learning models. We assume a relationship between the quality of workers and the quality of machine-learning models using the outcomes of the workers as training data. This assumption means that if worker quality is high, a machine-learning classifier constructed using the worker’s outcomes can easily predict the outcomes of the worker. If this assumption is confirmed, we can measure the worker quality without using the correct answer sets, and then the requesters can reduce the time and effort. However, if the outcomes by workers are low quality, the input tweet does not correspond to the outcomes. Therefore, if we construct a tweet classifier using input tweets and the classified results by the worker, the prediction of the outcomes by the classifier and that by the workers should differ. We assume that the GoF scores, such as accuracy and F1 scores of the test set using this classifier, correlates to worker quality. Therefore, we can predict worker quality using the GoF scores. In our experiment, we did the tweet classification task using crowdsourcing. We confirmed that the GoF scores and the quality of workers correlate. These results show that we can predict the quality of workers using the GoF scores.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472279",
        "category": "Databases"
    },
    {
        "title": "UNSUPERVISED ANOMALY DETECTION FOR TIME SERIES WITH OUTLIER EXPOSURE",
        "authors": "['Jiaming Feng', 'Zheng Huang', 'Jie Guo', 'Weidong Qiu']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "It is of great practical significance to accurately model and analyze abnormal events in time series. For example, the identification of anomaly patterns on infrastructure sensor curves helps locate equipment failures. In this paper, we propose an unsupervised anomaly detection approach for time series, which can comprehensively consider both point anomalies and subsequence anomalies. We innovatively introduce RNN into the architecture of Adversarial Autoencoder to better analyze anomaly events based on overall relationship of time series. In addition, we innovatively apply the Outlier Exposure technique for the performance optimization of anomaly detector. Meanwhile, a WGAN-based method is utilized to generate anomaly datasets through normal distribution learning. Finally, we apply the proposed method for fraud detection on a financial statement dataset and intrusion detection on a network traffic dataset. Experimental results demonstrates that our model can comprehensively consider different anomaly types in time series, and achieve promising detection performance overall. In the experiment of fraud detection, the LSTM integrated AAE model achieves an F1 score of 0.810, while the Outlier Exposure enhanced model achieves an F1 score of 0.894. This indicates that our method can improve the performance of current audit systems and facilitate discovering malicious behaviors.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468793",
        "category": "Databases"
    },
    {
        "title": "Portkey: Adaptive Key-Value Placement over Dynamic Edge Networks",
        "authors": "['Joseph Noor', 'Mani Srivastava', 'Ravi Netravali']",
        "date": "November 2021",
        "source": "SoCC '21: Proceedings of the ACM Symposium on Cloud Computing",
        "abstract": "Owing to a need for low latency data accesses, emerging IoT and mobile applications commonly require distributed data stores (e.g., key-value or KV stores) to operate entirely at the network's edge. Unfortunately, existing KV stores employ randomized data placement policies (e.g., consistent hashing) that ignore the client mobility and resulting variance in client-server latencies that are inherent to edge applications---the effect is largely suboptimal and inefficient data placement. We present Portkey, a distributed KV store that dynamically adapts data placement according to time-varying client mobility and data access patterns. The key insight with Portkey is to lean into the inherent mobility and prioritize rapid but approximate placement decisions over delayed optimal ones. Doing so enables the efficient tracking of client-server latencies despite edge resource constraints, and the use of greedy placement heuristics that are self-correcting over short timescales. Results with a realistic autonomous vehicle dataset and two small-scale deployments reveal that Portkey reduces average and tail request latency by 21-82% and 26-77% compared to existing placement strategies.",
        "link": "https://dl.acm.org/doi/10.1145/3472883.3487004",
        "category": "Databases"
    },
    {
        "title": "Identification of the C2H2 Type Zinc Finger Transcription Factor Family in Aspergillus Flavus and Aspergillus Oryzae and Their Expression Profiles in Aspergillus Oryzae",
        "authors": "['Qing Liu', 'Jianhua Huang', 'Bin He']",
        "date": "July 2021",
        "source": "BIBE2021: The Fifth International Conference on Biological Information and Biomedical Engineering",
        "abstract": "C2H2 zinc protein transcription family members play an important biological role in the mitotic bacteria. The study identified 25 and 26 families of C2H2 zinc dead, respectively, in Aspergillus flavus and Aspergillus oryzae genome databases. According to phytogenetics and fantasy analysis, the families of C2H2 zinc can be divided into four subfamilies. Genes in this family are distributed across eight chromosomes. In addition, the relationship of structure and function of C2H2 zinc transfer factors was comprehensively analyzed to intuitively understand the features of protective drawings. It also analyzed the expression profiles of the transcription factors of C2H2 under salt stress treatment and different growth phase. As a result, under salt stress treatment, C2H2 zinc has a relatively low algebraic and downward regulation of transcription factors. This work has a deeper understanding of the evolution and function of C2H2 zinc transcription factors in Aspergillus flavus and Aspergillus oryzae, and helps improve gene improvement of strains to adapt to complex surroundings.",
        "link": "https://dl.acm.org/doi/10.1145/3469678.3469693",
        "category": "Databases"
    },
    {
        "title": "An Automatic Schema-Instance Approach for Merging Multidimensional Data Warehouses",
        "authors": "['Yuzhao Yang', 'Jérôme Darmont', 'Franck Ravat', 'Olivier Teste']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Using data warehouses to analyse multidimensional data is a significant task in company decision-making. The need for analyzing data stored in different data warehouses generates the requirement of merging them into one integrated data warehouse. The data warehouse merging process is composed of two steps: matching multidimensional components and then merging them. Current approaches do not take all the particularities of multidimensional data warehouses into account, e.g., only merging schemata, but not instances; or not exploiting hierarchies nor fact tables. Thus, in this paper, we propose an automatic merging approach for star schema-modeled data warehouses that works at both the schema and instance levels. We also provide algorithms for merging hierarchies, dimensions and facts. Eventually, we implement our merging algorithms and validate them with the use of both synthetic and benchmark datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472268",
        "category": "Databases"
    },
    {
        "title": "Towards a continuous forecasting mechanism of parking occupancy in urban environments",
        "authors": "['Miratul Khusna Mufida', 'Abdessamad Ait El Cadi', 'Thierry Delot', 'Martin Trépanier']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Searching for an available parking space is a stressful and time-consuming task, which leads to increasing traffic and environmental pollution due to the emission of gases. To solve these issues, various solutions relying on information technologies (e.g., wireless networks, sensors, etc.) have been deployed over the last years to help drivers identify available parking spaces. Several recent works have also considered the use of historical data about parking availability and applied learning techniques (e.g., machine learning, deep learning) to estimate the occupancy rates in the near future. In this paper, we not only focus on training forecasting models for different types of parking lots to provide the best accuracy, but also consider the deployment of such a service in real conditions, to solve actual parking occupancy problems. It is therefore needed to continuously provide accurate information to the drivers but also to handle the frequent updates of parking occupancy data. The underlying challenges addressed in the present work so concern (1) the self-tuning of the forecasting model hyper-parameters according to the characteristics of the considered parking lots and (2) the need to maintain the performance of the forecasting model over time.  To demonstrate the effectiveness of our approach, we present in the paper several evaluations using real data provided for different parking lots by the city of Lille in France. The results of these evaluations highlight the accuracy of the forecasts and the ability of our solution to maintain model performance over time.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472265",
        "category": "Databases"
    },
    {
        "title": "Sink Group Betweenness Centrality",
        "authors": "['Evangelia Fragkou', 'Dimitrios Katsaros', 'Yannis Manolopoulos']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "This article introduces the concept of Sink Group Node Betweenness centrality to identify those nodes in a network that can “monitor” the geodesic paths leading towards a set of subsets of nodes; it generalizes both the traditional node betweenness centrality and the sink betweenness centrality. We also provide extensions of the basic concept for node-weighted networks, and also describe the dual notion of Sink Group Edge Betweenness centrality. We exemplify the merits of these concepts and describe some areas where they can be applied.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472182",
        "category": "Databases"
    },
    {
        "title": "NF-GNN: Network Flow Graph Neural Networks for Malware Detection and Classification",
        "authors": "['Julian Busch', 'Anton Kocheturov', 'Volker Tresp', 'Thomas Seidl']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Malicious software (malware) poses an increasing threat to the security of communication systems as the number of interconnected mobile devices increases exponentially. While some existing malware detection and classification approaches successfully leverage network traffic data, they treat network flows between pairs of endpoints independently and thus fail to leverage rich communication patterns present in the complete network. Our approach first extracts flow graphs and subsequently classifies them using a novel edge feature-based graph neural network model. We present three variants of our base model, which support malware detection and classification in supervised and unsupervised settings. We evaluate our approach on flow graphs that we extract from a recently published dataset for mobile malware detection that addresses several issues with previously available datasets. Experiments on four different prediction tasks consistently demonstrate the advantages of our approach and show that our graph neural network model can boost detection performance by a significant margin.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468814",
        "category": "Databases"
    },
    {
        "title": "Hapi: A Domain-Specific Language for the Declaration of Access Policies",
        "authors": "['Vinícius Julião', 'Alexander Holmquist', 'Flávio Lúcio', 'Celso Simões', 'Fernando Pereira']",
        "date": "September 2021",
        "source": "SBLP '21: Proceedings of the 25th Brazilian Symposium on Programming Languages",
        "abstract": "Access policies specify what are the actions that different actors can perform on the available resources. Access policies are a core notion in multiuser environments, such as operating systems and distributed databases. Currently, most of these systems use general data specification languages, such as JSON, XML and YAML to describe access policies. Yet, domain-specific languages are also available for this task. One of such languages is Legalease, from Microsoft. This paper presents a new version of Legalease, called Hapi. Hapi replaces Legalease’s notion of a lattice with a partially ordered set (poset). We demonstrate that posets already give all the expressivity of Legalease, while simplifying its specification and the implementation of verification algorithms. Hapi is currently publicly available. The language is distributed with tools for translating programs to YAML and for visualizing access rights. Hapi provides developers with an Intermediary Representation of policies that allows this language to be easily embedded in any project. Said representation generalizes the notion of actors, actions and resources to user-defined entities; hence, being more flexible than typical data-access description languages.",
        "link": "https://dl.acm.org/doi/10.1145/3475061.3475084",
        "category": "Databases"
    },
    {
        "title": "Accelerating Depth-First Traversal by Graph Ordering",
        "authors": "['Qiuyi Lyu', 'Mo Sha', 'Bin Gong', 'Kuangda Lyu']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Cache efficiency is an important factor in the performance of graph processing due to the irregular memory access patterns caused by the sparse nature of graphs. To increase the cache hit rate, prior studies proposed a variety of preprocessing approaches based on the reordering, which permutes the vertexes’ labels to improve the locality of graph structures. However, the locality enhancement of existing reordering approaches does not bring much performance benefit in depth-first traversal, which is widely adopted in a majority of graph processing applications. Furthermore, the state-of-the-art reordering approach suffers from an obvious overhead on preprocessing which will greatly limit the application of their approach. In this paper, we propose SeqDFS, a depth-first graph traversal method that optimizes the cache efficiency by adjusting the order of vertexes visited and can be further extended to dynamic scenarios. We conduct extensive experiments on 16 real-world datasets and 3 representative depth-first graph applications, of which the results show that our proposal achieves a significant speed-up on both directed and undirected graphs.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468796",
        "category": "Databases"
    },
    {
        "title": "COVID-19 Concerns in US: Topic Detection in Twitter",
        "authors": "['Carmela Comito']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "COVID-19 pandemic is affecting the lives of the citizens worldwide. Epidemiologists, policy makers and clinicians need to understand public concerns and sentiment to make informed decisions and adopt preventive and corrective measures to avoid critical situations. In the last few years, social media become a tool for spreading the news, discussing ideas and comments on world events. In this context, social media plays a key role since represents one of the main source to extract insight into public opinion and sentiment. In particular, Twitter has been already recognized as an important source of health-related information, given the amount of news, opinions and information that is shared by both citizens and official sources. However, it is a challenging issue identifying interesting and useful content from large and noisy text-streams. The study proposed in the paper aims to extract insight from Twitter by detecting the most discussed topics regarding COVID-19. The proposed approach combines peak detection and clustering techniques. Tweets features are first modeled as time series. After that, peaks are detected from the time series, and peaks of textual features are clustered based on the co-occurrence in the tweets. Results, performed over real-world datasets of tweets related to COVID-19 in US, show that the proposed approach is able to accurately detect several relevant topics of interest, spanning from health status and symptoms, to government policy, economic crisis, COVID-19-related updates, prevention, vaccines and treatments.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472169",
        "category": "Databases"
    },
    {
        "title": "Graph-based Strategy for Establishing Morphology Similarity",
        "authors": "['Namit Juneja', 'Jaroslaw Zola', 'Varun Chandola', 'Olga Wodo']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Analysis of morphological data is central to a broad class of scientific problems in materials science, astronomy, bio-medicine, and many others. Understanding relationships between morphologies is a core analytical task in such settings. In this paper, we propose a graph-based framework for measuring similarity between morphologies. Our framework delivers a novel representation of a morphology as an augmented graph that encodes application-specific knowledge through the use of configurable signature functions. It provides also an algorithm to compute the similarity between a pair of morphology graphs. We present experimental results in which the framework is applied to morphology data from high-fidelity numerical simulations that emerge in materials science. The results demonstrate that our proposed measure is superior in capturing the semantic similarity between morphologies, compared to the state-of-the-art methods such as FFT-based measures.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468819",
        "category": "Databases"
    },
    {
        "title": "HInT: Hybrid and Incremental Type Discovery for Large RDF Data Sources",
        "authors": "['Nikolaos Kardoulakis', 'Kenza Kellou-Menouer', 'Georgia Troullinou', 'Zoubida Kedad', 'Dimitris Plexousakis', 'Haridimos Kondylakis']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "The rapid explosion of linked data has resulted into many weakly structured and incomplete data sources, where typing information might be missing. On the other hand, type information is essential for a number of tasks such as query answering, integration, summarization and partitioning. Existing approaches for type discovery, either completely ignore type declarations available in the dataset (implicit type discovery approaches), or rely only on existing types, in order to complement them (explicit type enrichment approaches). Implicit type discovery approaches are based on instance grouping, which requires an exhaustive comparison between the instances. This process is expensive and not incremental. Explicit type enrichment approaches on the other hand, are not able to identify new types and they can not process data sources that have little or no schema information. In this paper, we present HInT, the first incremental and hybrid type discovery system for RDF datasets, enabling type discovery in datasets where type declarations are missing. To achieve this goal, we incrementally identify the patterns of the various instances, we index and then group them to identify the types. During the processing of an instance, our approach exploits its type information, if available, to improve the quality of the discovered types by guiding the classification of the new instance in the correct group and by refining the groups already built. We analytically and experimentally show that our approach dominates in terms of efficiency, competitors from both worlds, implicit type discovery and explicit type enrichment while outperforming them in most of the cases in terms of quality.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468808",
        "category": "Databases"
    },
    {
        "title": "Looking for Jobs? Matching Adults with Autism with Potential Employers for Job Opportunities",
        "authors": "['Joseph Bills', 'Yiu-kai Dennis Ng']",
        "date": "July 2021",
        "source": "IDEAS '21: Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "abstract": "Adults with autism face many difficulties when finding employment, such as struggling with interviews and needing accommodating environments for sensory issues. Autistic adults, however, also have unique skills to contribute to the workplace that companies have recently started to seek after, such as loyalty, close attention to detail, and trustworthiness. To work around these difficulties and help companies find the talent they are looking for we have developed a job-matching system. Our system is based around the stable matching of the Gale-Shapley algorithm to match autistic adults with employers after estimating how both adults with autism and employers would rank the other group. The system also uses filtering to approximate a stable matching even with a changing pool of users and employers, meaning the results are resistant to change as the result of competition. Such a system would be of benefit to both adults with autism and employers and would advance knowledge in recommender systems that match two parties.",
        "link": "https://dl.acm.org/doi/10.1145/3472163.3472270",
        "category": "Databases"
    },
    {
        "title": "Truss Decomposition on Large Probabilistic Networks using H-Index",
        "authors": "['Fatemeh Esfahani', 'Mahsa Daneshmand', 'Venkatesh Srinivasan', 'Alex Thomo', 'Kui Wu']",
        "date": "July 2021",
        "source": "SSDBM '21: Proceedings of the 33rd International Conference on Scientific and Statistical Database Management",
        "abstract": "Truss decomposition is a popular approach for discovering cohesive subgraphs. However, truss decomposition on probabilistic graphs is challenging. State-of-the-art either do not scale to large graphs or use approximation techniques to achieve scalability. We present an exact and scalable algorithm for truss decomposition of probabilistic graphs. The algorithm is based on progressive tightening of the estimate of the truss value of each edge based on h-index computation and novel use of dynamic programming. Our proposed algorithm (1) is significantly faster than state-of-the-art and scales to much larger graphs, (2) is progressive by allowing the user to see near-results along the way, (3) does not sacrifice the exactness of final result, and (4) achieves all these while processing only an edge and its immediate neighbors at a time, thus resulting in smaller memory footprint. Our extensive experimental results confirm the scalability and efficiency of our algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3468791.3468817",
        "category": "Databases"
    },
    {
        "title": "Capturing Knowledge about Drug-Drug Interactions to Enhance Treatment Effectiveness",
        "authors": "['Ariam Rivas', 'Maria-Esther Vidal']",
        "date": "December 2021",
        "source": "K-CAP '21: Proceedings of the 11th Knowledge Capture Conference",
        "abstract": "Capturing knowledge about Drug-Drug Interactions (DDI) is a crucial factor to support clinicians in better treatments. Nowadays, public drug databases provide a wealth of information on drugs that can be exploited to enhance tasks, e.g., data mining, ranking, and query answering. However, all the interactions in the public database are focused on pairs of drugs. Since current treatments are composed of multi-drugs, it is extremely challenging to know which potential drugs affect the effectiveness of the treatment. In this work, we tackle the problem of discovering DDIs and reduce this problem to link prediction over a property graph represented in RDF-star. A deductive system captures knowledge about the conditions that define when a group of drugs interacts as Datalog rules. Extensional statements represent the property graph. Lastly, the intensional rules guide the deduction process to discover relationships in the graph and their properties. As a proof concept, we have implemented a graph traversal method on top of the property graph and the deduced edges. The technique aims to identify the combination of drugs whose interactions may reduce the effectiveness of a treatment or increase the number of toxicities. This traversal method relies on the computation of wedges in the property graph. Albeit illustrated in the context of DDI, this method could be generalized to other link traversal tasks. We conduct an experimental study on a DDIs property graph for different treatments. The results suggest that by capturing knowledge about DDIs, our approach can discover the drugs that decrease the effectiveness of the treatment. Our results are promising and suggest that clinicians can better understand the DDIs in treatment and prescribe improved treatments through the knowledge captured by our approach.",
        "link": "https://dl.acm.org/doi/10.1145/3460210.3493560",
        "category": "Databases"
    },
    {
        "title": "A Deep Learning Approach for Voice Disorder Detection for Smart Connected Living Environments",
        "authors": "['Laura Verde', 'Nadia Brancati', 'Giuseppe De Pietro', 'Maria Frucci', 'Giovanna Sannino']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "Edge Analytics and Artificial Intelligence are important features of the current smart connected living community. In a society where people, homes, cities, and workplaces are simultaneously connected through various devices, primarily through mobile devices, a considerable amount of data is exchanged, and the processing and storage of these data are laborious and difficult tasks. Edge Analytics allows the collection and analysis of such data on mobile devices, such as smartphones and tablets, without involving any cloud-centred architecture that cannot guarantee real-time responsiveness. Meanwhile, Artificial Intelligence techniques can constitute a valid instrument to process data, limiting the computation time, and optimising decisional processes and predictions in several sectors, such as healthcare. Within this field, in this article, an approach able to evaluate the voice quality condition is proposed. A fully automatic algorithm, based on Deep Learning, classifies a voice as healthy or pathological by analysing spectrogram images extracted by means of the recording of vowel /a/, in compliance with the traditional medical protocol. A light Convolutional Neural Network is embedded in a mobile health application in order to provide an instrument capable of assessing voice disorders in a fast, easy, and portable way. Thus, a straightforward mobile device becomes a screening tool useful for the early diagnosis, monitoring, and treatment of voice disorders. The proposed approach has been tested on a broad set of voice samples, not limited to the most common voice diseases but including all the pathologies present in three different databases achieving F1-scores, over the testing set, equal to 80%, 90%, and 73%. Although the proposed network consists of a reduced number of layers, the results are very competitive compared to those of other “cutting edge” approaches constructed using more complex neural networks, and compared to the classic deep neural networks, for example, VGG-16 and ResNet-50.",
        "link": "https://dl.acm.org/doi/10.1145/3433993",
        "category": "Databases"
    },
    {
        "title": "Utility Mining Across Multi-Dimensional Sequences",
        "authors": "['Wensheng Gan', 'Jerry Chun-Wei Lin', 'Jiexiong Zhang', 'Hongzhi Yin', 'Philippe Fournier-Viger', 'Han-Chieh Chao', 'Philip S. Yu']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Knowledge extraction from database is the fundamental task in database and data mining community, which has been applied to a wide range of real-world applications and situations. Different from the support-based mining models, the utility-oriented mining framework integrates the utility theory to provide more informative and useful patterns. Time-dependent sequence data are commonly seen in real life. Sequence data have been widely utilized in many applications, such as analyzing sequential user behavior on the Web, influence maximization, route planning, and targeted marketing. Unfortunately, all the existing algorithms lose sight of the fact that the processed data not only contain rich features (e.g., occur quantity, risk, and profit), but also may be associated with multi-dimensional auxiliary information, e.g., transaction sequence can be associated with purchaser profile information. In this article, we first formulate the problem of utility mining across multi-dimensional sequences, and propose a novel framework named MDUS to extract <underline>M</underline>ulti-<underline>D</underline>imensional <underline>U</underline>tility-oriented <underline>S</underline>equential useful patterns. To the best of our knowledge, this is the first study that incorporates the time-dependent sequence-order, quantitative information, utility factor, and auxiliary dimension. Two algorithms respectively named MDUSEM and MDUSSD are presented to address the formulated problem. The former algorithm is based on database transformation, and the later one performs pattern joins and a searching method to identify desired patterns across multi-dimensional sequences. Extensive experiments are carried on six real-life datasets and one synthetic dataset to show that the proposed algorithms can effectively and efficiently discover the useful knowledge from multi-dimensional sequential databases. Moreover, the MDUS framework can provide better insight, and it is more adaptable to real-life situations than the current existing models.",
        "link": "https://dl.acm.org/doi/10.1145/3446938",
        "category": "Databases"
    },
    {
        "title": "LANNS: a web-scale approximate nearest neighbor lookup system",
        "authors": "['Ishita Doshi', 'Dhritiman Das', 'Ashish Bhutani', 'Rajeev Kumar', 'Rushi Bhatt', 'Niranjan Balasubramanian']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Nearest neighbor search (NNS) has a wide range of applications in information retrieval, computer vision, machine learning, databases, and other areas. Existing state-of-the-art algorithm for nearest neighbor search, Hierarchical Navigable Small World Networks (HNSW), is unable to scale to large datasets of 100M records in high dimensions. In this paper, we propose LANNS, an end-to-end platform for Approximate Nearest Neighbor Search, which scales for web-scale datasets. Library for Large Scale Approximate Nearest Neighbor Search (LANNS) is deployed in multiple production systems for identifying top-K (100 ≤ k ≤ 200) approximate nearest neighbors with a latency of a few milliseconds per query, high throughput of ~2.5k Queries Per Second (QPS) on a single node, on large (e.g., ~ 180M data points) high dimensional (50-2048 dimensional) datasets.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503594",
        "category": "Databases"
    },
    {
        "title": "What is the price for joining securely?: benchmarking equi-joins in trusted execution environments",
        "authors": "['Kajetan Maliszewski', 'Jorge-Arnulfo Quiané-Ruiz', 'Jonas Traub', 'Volker Markl']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Protection of personal data has been raised to be among the top requirements of modern systems. At the same time, it is now frequent that the owner of the data and the owner of the computing infrastructure are two entities with limited trust between them (e. g., volunteer computing or the hybrid-cloud). Recently, trusted execution environments (TEEs) became a viable solution to ensure the security of systems in such environments. However, the performance of relational operators in TEEs remains an open problem. We conduct a comprehensive experimental study to identify the main bottlenecks and challenges when executing relational equi-joins in TEEs. For this, we introduce TEEbench, a framework for unified benchmarking of relational operators in TEEs, and use it for conducting our experimental evaluation. In a nutshell, we perform the following experimental analysis for eight core join algorithms: off-the-shelf performance; the performance implications of data sealing and obliviousness; sensitivity and scalability. The results show that all eight join algorithms significantly suffer from different performance bottlenecks in TEEs. They can be up to three orders of magnitude slower in TEEs than on plain CPUs. Our study also indicates that existing join algorithms need a complete, hardware-aware redesign to be efficient in TEEs, and that, in secure query plans, managing TEE features is equally important to join selection.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494146",
        "category": "Databases"
    },
    {
        "title": "Leveraging query logs and machine learning for parametric query optimization",
        "authors": "['Kapil Vaidya', 'Anshuman Dutt', 'Vivek Narasayya', 'Surajit Chaudhuri']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Parametric query optimization (PQO) must address two problems: identify a relatively small number of plans to cache for a parameterized query (populateCache), and efficiently select the best cached plan to use for executing any instance of the parameterized query (getPlan). Our approach decouples these two decisions. We formulate populateCache as an optimization problem with the goal of identifying a set of plans that minimizes the optimizer estimated cost of queries in the log, and present an efficient algorithm. For getPlan, we leverage query logs to train machine learning (ML) models to choose the lowest optimizer-estimated cost plan from the cached plans. We conduct extensive experiments using complex parameterized queries from benchmarks and real workloads. Our algorithm for populateCache achieves low geometric mean sub-optimality (1.2) even for complex queries using relatively few plans, and scales well to large query logs. The mean latency of our ML model based getPlan technique (~ 210μsec) is between one to four orders of magnitude faster compared to prior PQO techniques. The mean sub-optimality is low (1.05), and the 95th percentile sub-optimality (1.3) is between 1.1× and 25× lower compared to prior techniques. Finally, we present an efficient algorithm for getPlan that leverages execution time information in query logs to circumvent inaccuracies of the query optimizer's cost estimates.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494126",
        "category": "Databases"
    },
    {
        "title": "Local R-Symmetry Co-Occurrence: Characterising Leaf Image Patterns for Identifying Cultivars",
        "authors": "['Bin Wang', 'Yongsheng Gao', 'Xiaohui Yuan', 'Shengwu Xiong']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Leaf image recognition techniques have been actively researched for plant species identification. However it remains unclear whether analysing leaf patterns can provide sufficient information for further differentiating cultivars. This paper reports our attempt on cultivar recognition from leaves as a general very fine-grained pattern recognition problem, which is not only a challenging research problem but also important for cultivar evaluation, selection and production in agriculture. We propose a novel local R-symmetry co-occurrence method for characterising discriminative local symmetry patterns to distinguish subtle differences among cultivars. Through scalable and moving R-relation radius pairs, we generate a set of radius symmetry co-occurrence matrices (RsCoM)and their measures for describing the local symmetry properties of interior regions. By varying the size of the radius pair, the RsCoM measures local R-symmetry co-occurrence from global/coarse to fine scales. A new two-phase strategy of analysing the distribution of local RsCoM measures is designed to match the multiple scale appearance symmetry pattern distributions of similar cultivar leaf images. We constructed three leaf image databases, SoyCultivar, CottCultivar, and PeanCultivar, for an extensive experimental evaluation on recognition across soybean, cotton and peanut cultivars. Encouraging experimental results of the proposed method in comparison with the state-of-the-art leaf species recognition methods demonstrate the effectiveness of the proposed method for cultivar identification, which may advance the research in leaf recognition from species to cultivar.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2020.3031280",
        "category": "Databases"
    },
    {
        "title": "Design of Intelligent Educational Administration System Based on Big Data Technology",
        "authors": "['Jia Hao']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "The main task of educational administration is educational administration. Nowadays, many schools have thousands or even thousands of teachers, leading to the increase of a large amount of data such as various systems and databases operated by the school, such as management of school conditions and management performance analysis. How to use these data to transform existing management data into useful knowledge, improve school management decision-making, and improve school management level and quality is an urgent problem for schools to solve. This paper studies the intelligent educational administration management system based on big data technology. After understanding the relevant theories of big data technology and intelligent educational administration management system based on literature data, the intelligent educational administration management system based on big data technology is designed, and the designed system is tested, and the test results show that when the number of users in the system is 40, the average response time of the system is 0.31s, and the average response time of student score query is 0.63s, which basically meets the design requirements of the system.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495406",
        "category": "Databases"
    },
    {
        "title": "Digital Protection of Nanfeng Nuo Mask Based on AR Technology",
        "authors": "['Ying Liu', 'Wei Yu', 'Suhong Xiao']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "The sculpture of Nanfeng Nuo mask originated in the Han Dynasty, developed in the Tang and Song Dynasties, and prospered in the Ming and Qing Dynasties. The carving art has been passed down to this day. The sculptures of Nanfeng Nuo masks are famous for their simple and profound, vivid shapes and delicate techniques. This article first stated that under the background of AR technology, in terms of digital protection mode, limited by ideological understanding and technology, the current digital protection of Chinese traditional culture is still at the stage of digital information collection and preservation. How to enrich and perfect the existing digital protection mode with new digital technology is an urgent problem to be solved in the new era. Taking the Nanfeng Nuo mask as an example, this research analyzes the inheritance dilemma of the Nanfeng Nuo mask wood carving and the modern and innovative protection model through the reading of historical documents, field inspections of existing wood carvings, survey visits to the protection status, and understanding of digital technology. Through the fuzzy KNN algorithm and AR compared to the database, the various databases are related to form a complete protection system; in the \"live inheritance protection mode\", AR technology is proposed as the basic technology, and AR image acquisition technology and AR display technology are proposed. , AR human-computer interaction technology and digital protection mode are combined, and then a digital protection platform based on AR technology is designed to achieve the realization of the live inheritance mode by the way audiences participate in the use of the digital protection platform. Experimental research results show that people in their 30s to 40s may take a long time to receive and learn when facing a new interactive operating system, but now due to the popularity of social software such as WeChat, the 30 to 40 age group 24.4% of the population can also perform basic operations, which provides a prerequisite for the use of AR technology to digitize the Nanfeng Nuo mask.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495486",
        "category": "Databases"
    },
    {
        "title": "Supervised multi-specialist topic model with applications on large-scale electronic health record data",
        "authors": "['Ziyang Song', 'Xavier Sumba Toral', 'Yixin Xu', 'Aihua Liu', 'Liming Guo', 'Guido Powell', 'Aman Verma', 'David Buckeridge', 'Ariane Marelli', 'Yue Li']",
        "date": "August 2021",
        "source": "BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "abstract": "Motivation: Electronic health record (EHR) data provides a new venue to elucidate disease comorbidities and latent phenotypes for precision medicine. To fully exploit its potential, a realistic data generative process of the EHR data needs to be modelled. Materials and Methods: We present MixEHR-S to jointly infer specialist-disease topics from the EHR data. As the key contribution, we model the specialist assignments and ICD-coded diagnoses as the latent topics based on patient's underlying disease topic mixture in a novel unified supervised hierarchical Bayesian topic model. For efficient inference, we developed a closed-form collapsed variational inference algorithm to learn the model distributions of MixEHR-S. Results: We applied MixEHR-S to two independent large-scale EHR databases in Quebec with three targeted applications: (1) Congenital Heart Disease (CHD) diagnostic prediction among 154,775 patients; (2) Chronic obstructive pulmonary disease (COPD) diagnostic prediction among 73,791 patients; (3) future insulin treatment prediction among 78,712 patients diagnosed with diabetes as a mean to assess the disease exacerbation. In all three applications, MixEHR-S conferred clinically meaningful latent topics among the most predictive latent topics and achieved superior target prediction accuracy compared to the existing methods, providing opportunities for prioritizing high-risk patients for healthcare services. Availability and implementation: MixEHR-S source code and scripts of the experiments are freely available at https://github.com/li-lab-mcgill/mixehrS",
        "link": "https://dl.acm.org/doi/10.1145/3459930.3469543",
        "category": "Databases"
    },
    {
        "title": "Learning GraphQL query cost",
        "authors": "['Georgios Mavroudeas', 'Guillaume Baudart', 'Alan Cha', 'Martin Hirzel', 'Jim A. Laredo', 'Malik Magdon-Ismail', 'Louis Mandel', 'Erik Wittern']",
        "date": "November 2021",
        "source": "ASE '21: Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering",
        "abstract": "GraphQL is a query language for APIs and a runtime for executing those queries, fetching the requested data from existing microservices, REST APIs, databases, or other sources. Its expressiveness and its flexibility have made it an attractive candidate for API providers in many industries, especially through the web. A major drawback to blindly servicing a client's query in GraphQL is that the cost of a query can be unexpectedly large, creating computation and resource overload for the provider, and API rate-limit overages and infrastructure overload for the client. To mitigate these drawbacks, it is necessary to efficiently estimate the cost of a query before executing it. Estimating query cost is challenging, because GraphQL queries have a nested structure, GraphQL APIs follow different design conventions, and the underlying data sources are hidden. Estimates based on worst-case static query analysis have had limited success because they tend to grossly overestimate cost. We propose a machine-learning approach to efficiently and accurately estimate the query cost. We also demonstrate the power of this approach by testing it on query-response data from publicly available commercial APIs. Our framework is efficient and predicts query costs with high accuracy, consistently outperforming the static analysis by a large margin.",
        "link": "https://dl.acm.org/doi/10.1109/ASE51524.2021.9678513",
        "category": "Databases"
    },
    {
        "title": "An Enhanced Version of K-means Algorithm",
        "authors": "['Farah Sulaiman Atloba', 'Nuha Khalid Balkir', 'Faraj A. El-Mouadib']",
        "date": "October 2021",
        "source": "ICEMIS'21: The 7th International Conference on Engineering &amp; MIS 2021",
        "abstract": "Abstract- Recently, the world has seen significant advancements, particularly in the field of information technology. Over the last decades, massive amount of data has been accumulated; such massive data include implicit knowledge that cannot be discovered using typical data analysis technologies. The availability of data and the urgent need to transform it into knowledge has called for the emergence of new field called Knowledge Discovery in Databases (KDD). The Data Mining (DM) step is one of the KDD process' steps that employees intelligent methods in the discovery of patterns which are the essence of knowledge. Cluster analysis is one of the well-known descriptive DM functionalities. The topic of this paper is a study of cluster analysis, with emphasis on the K-means algorithm. The contribution of this work is to develop computer system software to combine two modified versions of the K-means algorithm into one to benefit from the proposed modifications. Our study was concluded with analysis and discussion of the experiments’ results on the basis of several criteria, which are shown that our modified version mostly was better than the other versions.",
        "link": "https://dl.acm.org/doi/10.1145/3492547.3492592",
        "category": "Databases"
    },
    {
        "title": "A Case Study on Handwritten Indic Script Classification: Benchmarking of the Results at Page, Block, Text-line, and Word Levels",
        "authors": "['Pawan Kumar Singh', 'Ram Sarkar', 'Ajith Abraham', 'Mita Nasipuri']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "Handwritten script classification is still considered as a challenging research problem in the domain of document image analysis. Although some research attempts have been made by the researchers for solving the challenging issues, a comprehensive solution is yet to be achieved. The case study, undertaken here, analyzes the performances of various state-of-the art handwritten script classification methods for Indian scripts where features, needed for the script classification task, are extracted from the script images at four different granularity levels, i.e., page, block, text line, or word. The results of handwritten script classification at each level have been obtained and compared using eight different feature sets and six different state-of-the-art classifiers. Based on the classification results, an ideal level for performing the handwritten script classification task is suggested among these four classification levels. The results have also been improved by using two feature dimensionality reduction methods. All these experiments are done on two different handwritten Indic script databases, of which one is an in-house developed dataset and the other one is a freely available dataset. Finally, some future research directions that may be undertaken by the researchers as an application of the handwritten Indic script classification problem are also highlighted. The work presented here provides a basic foundation for the construction of a comprehensive handwritten script classification method for official Indian scripts.",
        "link": "https://dl.acm.org/doi/10.1145/3476102",
        "category": "Databases"
    },
    {
        "title": "Isolation in Rust: What is Missing?",
        "authors": "['Anton Burtsev', 'Dan Appel', 'David Detweiler', 'Tianjiao Huang', 'Zhaofeng Li', 'Vikram Narayanan', 'Gerd Zellweger']",
        "date": "October 2021",
        "source": "PLOS '21: Proceedings of the 11th Workshop on Programming Languages and Operating Systems",
        "abstract": "Rust is the first practical programming language that has the potential to provide fine-grained isolation of untrusted computations at the language level. A combination of zero-overhead safety, i.e., safety without a managed runtime and garbage collection, and a unique ownership discipline enable isolation in systems with tight performance budgets, e.g., databases, network processing frameworks, browsers, and even operating system kernels. Unfortunately, Rust was not designed with isolation in mind. Today, implementing isolation in Rust is possible but requires complex, ad hoc, and arguably error-prone mechanisms to enforce it outside of the language. We examine several recent systems that implement isolation in Rust but struggle with the shortcomings of the language. As a result of our analysis we identify a collection of mechanisms that can enable isolation as a first class citizen in the Rust ecosystem and suggest directions for implementing them.",
        "link": "https://dl.acm.org/doi/10.1145/3477113.3487272",
        "category": "Databases"
    },
    {
        "title": "Digging into big provenance (with SPADE)",
        "authors": "['Ashish Gehani', 'Raza Ahmad', 'Hassaan Irshad', 'Jianqiao Zhu', 'Jignesh Patel']",
        "date": "December 2021",
        "source": "Communications of the ACM",
        "abstract": "A user interface for querying provenance.",
        "link": "https://dl.acm.org/doi/10.1145/3475358",
        "category": "Databases"
    },
    {
        "title": "Efficiently Answering Durability Prediction Queries",
        "authors": "['Junyang Gao', 'Yifan Xu', 'Pankaj K. Agarwal', 'Jun Yang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "We consider a class of queries called durability prediction queries that arise commonly in predictive analytics, where we use a given predictive model to answer questions about possible futures to inform our decisions. Examples of durability prediction queries include \"what is the probability that this financial product will keep losing money over the next 12 quarters before turning in any profit?\" and \"what is the chance for our proposed server cluster to fail the required service-level agreement before its term ends?\" We devise a general method called Multi-Level Splitting Sampling (MLSS) that can efficiently handle complex queries and complex models---including those involving black-box functions---as long as the models allow us to simulate possible futures step by step. Our method addresses the inefficiency of standard Monte Carlo (MC) methods by applying the idea of importance splitting to let one \"promising\" sample path prefix generate multiple \"offspring\" paths, thereby directing simulation efforts toward more promising paths. We propose practical techniques for designing splitting strategies, freeing users from manual tuning. Experiments show that our approach is able to achieve unbiased estimates and the same error guarantees as standard MC while offering an order-of-magnitude cost reduction.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457305",
        "category": "Databases"
    },
    {
        "title": "Towards just-in-time compilation of SQL queries with OMR JitBuilder",
        "authors": "['Debajyoti Datta', 'Mark Stoodley', 'Suprio Ray']",
        "date": "November 2021",
        "source": "CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering",
        "abstract": "The evaluation of SQL expressions and tuple materialization can consume a significant portion of the overall execution time of a query. The goal of our work is to generate efficient machine code for scan, filter, join and group-by operations for a given SQL expression by Just-in-time (JIT) compilation using the OMR JitBuilder compiler framework. Our approach creates a blend of specialized code consisting of compile-time constants and JIT computation for parts of the same SQL expression. The implementation is based on a light-weight integration of JitBuilder into PostgreSQL 12.5, where both the JIT compiled code and interpreted evaluation co-exist for different opcodes in the same bytecode interpreter. We demonstrate with our enhanced PostgreSQL 12.5 that our approach offers improved query performance over purely interpreted execution.",
        "link": "https://dl.acm.org/doi/10.5555/3507788.3507829",
        "category": "Databases"
    },
    {
        "title": "PATSQL: efficient synthesis of SQL queries from example tables with quick inference of projected columns",
        "authors": "['Keita Takenouchi', 'Takashi Ishio', 'Joji Okada', 'Yuji Sakata']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "SQL is one of the most popular tools for data analysis, and it is now used by an increasing number of users without having expertise in databases. Several studies have proposed programming-by-example approaches to help such non-experts to write correct SQL queries. While existing methods support a variety of SQL features such as aggregation and nested query, they suffer a significant increase in computational cost as the scale of example tables increases. In this paper, we propose an efficient algorithm utilizing properties known in relational algebra to synthesize SQL queries from input and output tables. Our key insight is that a projection operator in a program sketch can be lifted above other operators by applying transformation rules in relational algebra, while preserving the semantics of the program. This enables a quick inference of appropriate columns in the projection operator, which is an essential component in synthesis but causes combinatorial explosions in prior work. We also introduce a novel form of constraints and its top-down propagation mechanism for efficient sketch completion. We implemented this algorithm in our tool PATSQL and evaluated it on 226 queries from prior benchmarks and Kaggle's tutorials. As a result, PATSQL solved 68% of the benchmarks and found 89% of the solutions within a second. Our tool is available at https://naist-se.github.io/patsql/.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476253",
        "category": "Databases"
    },
    {
        "title": "SAM: Accelerating Strided Memory Accesses",
        "authors": "['Xin Xin', 'Yanan Guo', 'Youtao Zhang', 'Jun Yang']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Strided memory accesses are an important type of operations for In-Memory Databases (IMDB) applications. Strided memory accesses often demand data at word granularity with fixed strides. Hence, they tend to produce sub-optimal performance on DRAM memory (the de facto standard memory in modern computer systems) that accesses data at cacheline granularity. Recently proposed optimizations either introduce significant reliability degradation or are limited to non-volatile crossbar memory structures.  In this paper, we propose a low-cost DRAM-based optimization scheme SAM for accelerating strided memory accesses. SAM consists of several designs. The primary design, termed SAM-IO, is to exploit under-utilized I/O resources in commodity DRAM chips to support high-performance strided memory accesses with near-zero hardware overhead. Based on SAM-IO, an enhanced design, termed SAM-en, is further proposed by combining several innovations to achieve overall efficiency on energy and area. Our evaluation of the proposed designs shows that SAM not only achieves high performance improvement (up to ∼ 4.2 ×) but also maintains high-level reliability protection for server systems.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480091",
        "category": "Databases"
    },
    {
        "title": "RisGraph: A Real-Time Streaming System for Evolving Graphs to Support Sub-millisecond Per-update Analysis at Millions Ops/s",
        "authors": "['Guanyu Feng', 'Zixuan Ma', 'Daixuan Li', 'Shengqi Chen', 'Xiaowei Zhu', 'Wentao Han', 'Wenguang Chen']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Evolving graphs in the real world are large-scale and constantly changing, as hundreds of thousands of updates may come every second. Monotonic algorithms such as Reachability and Shortest Path are widely used in real-time analytics to gain both static and temporal insights and can be accelerated by incremental computing. Existing streaming systems adopt the incremental computing model and achieve either low latency or high throughput, but not both. However, both high throughput and low latency are required in real scenarios such as financial fraud detection. This paper presents RisGraph, a real-time streaming system that provides low-latency analysis for each update with high throughput. RisGraph addresses the challenge with localized data access and inter-update parallelism. We propose a data structure named Indexed Adjacency Lists and use sparse arrays and Hybrid Parallel Mode to enable localized data access. To achieve inter-update parallelism, we propose a domain-specific concurrency control mechanism based on the classification of safe and unsafe updates. Experiments show that RisGraph can ingest millions of updates per second for graphs with several hundred million vertices and billions of edges, and the P999 processing time latency is within 20 milliseconds. RisGraph achieves orders-of-magnitude improvement on throughput when analyses are executed for each update without batching and performs better than existing systems with batches of up to 20 million updates.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457263",
        "category": "Databases"
    },
    {
        "title": "Computing how-provenance for SPARQL queries via query rewriting",
        "authors": "['Daniel Hernández', 'Luis Galárraga', 'Katja Hose']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Over the past few years, we have witnessed the emergence of large knowledge graphs built by extracting and combining information from multiple sources. This has propelled many advances in query processing over knowledge graphs, however the aspect of providing provenance explanations for query results has so far been mostly neglected. We therefore propose a novel method, SPARQLprov, based on query rewriting, to compute how-provenance polynomials for SPARQL queries over knowledge graphs. Contrary to existing works, SPARQLprov is system-agnostic and can be applied to standard and already deployed SPARQL engines without the need of customized extensions. We rely on spm-semirings to compute polynomial annotations that respect the property of commutation with homomorphisms on monotonic and non-monotonic SPARQL queries without aggregate functions. Our evaluation on real and synthetic data shows that SPARQLprov over standard engines incurs an acceptable runtime overhead w.r.t. the original query, competing with state-of-the-art solutions for how-provenance computation.",
        "link": "https://dl.acm.org/doi/10.14778/3484224.3484235",
        "category": "Databases"
    },
    {
        "title": "SAM: Accelerating Strided Memory Accesses",
        "authors": "['Xin Xin', 'Yanan Guo', 'Youtao Zhang', 'Jun Yang']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Strided memory accesses are an important type of operations for In-Memory Databases (IMDB) applications. Strided memory accesses often demand data at word granularity with fixed strides. Hence, they tend to produce sub-optimal performance on DRAM memory (the de facto standard memory in modern computer systems) that accesses data at cacheline granularity. Recently proposed optimizations either introduce significant reliability degradation or are limited to non-volatile crossbar memory structures.  In this paper, we propose a low-cost DRAM-based optimization scheme SAM for accelerating strided memory accesses. SAM consists of several designs. The primary design, termed SAM-IO, is to exploit under-utilized I/O resources in commodity DRAM chips to support high-performance strided memory accesses with near-zero hardware overhead. Based on SAM-IO, an enhanced design, termed SAM-en, is further proposed by combining several innovations to achieve overall efficiency on energy and area. Our evaluation of the proposed designs shows that SAM not only achieves high performance improvement (up to ∼ 4.2 ×) but also maintains high-level reliability protection for server systems.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480091",
        "category": "Databases"
    },
    {
        "title": "A unified storage layer for supporting distributed workflows in kubernetes",
        "authors": "['Antony Chazapis', 'Christian Pinto', 'Yiannis Gkoufas', 'Christos Kozanitis', 'Angelos Bilas']",
        "date": "April 2021",
        "source": "CHEOPS '21: Proceedings of the Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems",
        "abstract": "The distributed computing landscape has been undergoing radical changes: High-Performance Computing (HPC) applications are moving to the Cloud, as a way of simplifying development, deployment, and migration across computing systems. Meanwhile, cloud applications are becoming increasingly complex and computationally intensive, with the advent of High-Performance Data Analytics (HPDA) pipelines --- highly distributed workflows dealing with enormous and diverse datasets, heavily relying on virtualization and containers, that would benefit from technologies used in \"traditional\" HPC. The foreseeable convergence, demands new abstractions to cope with the increased heterogeneity, so that differing workload classes can coexist seamlessly on the same infrastructure. In this paper, we propose a Unified Storage Layer (USL), to enable cloud-native applications to transparently access a wide spectrum of storage solutions, ranging from high-performance filesystems to cloud-based object stores and key-value databases. Allowing software to exploit the best out of the storage services available with no need for manual intervention from users or programmers, simplifies development and execution of workflows, and boosts overall productivity. This work has been motivated by the requirements of real-world, industry-driven applications running on Kubernetes, the industry standard for cloud infrastructure orchestration.",
        "link": "https://dl.acm.org/doi/10.1145/3439839.3458735",
        "category": "Databases"
    },
    {
        "title": "VD-tree: how to build an efficient and fit metric access method using voronoi diagrams",
        "authors": "['Andre Moriyama', 'Lucas S. Rodrigues', 'Lucas C. Scabora', 'Mirela T. Cazzolato', 'Agma J. M. Traina', 'Caetano Traina']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Efficient similarity search is a core issue for retrieval operations on large amounts of complex data, often relying on Metric Access Methods (MAMs) to speed up the Range and k-NN queries. Among the most used MAMs are those based on covering radius, which create balanced structures, and enable efficient data retrieval and dynamic maintenance. MAMs typically suffer from node overlapping, which increases retrieval costs. Some strategies aim to reduce node over-lapping by employing global pivots to improve the filtering process during queries, but result at significant costs to maintain the pivots, whereas not completely removing the overlaps, which impacts queries over large databases. Other strategies use hyper-plane-based MAMs, which can get rid of overlaps but with large costs to create and update the index. We propose VD-Tree, a MAM which combines a covering radius strategy with a Voronoi-like organization. VD-Tree retains index flexibility for updates whereas reducing the node overlap using dynamic swap of elements among nodes. The method relies on only the solid organization fostered by Voronoi, and does not require storing further information to the tree. Experimental analysis using five real-world image datasets and four feature extractors shows that VD-Tree reduced node overlaps up to 43% and the average time needed to answer similarity queries by up to 28%, when compared to its closest competitor.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441915",
        "category": "Databases"
    },
    {
        "title": "Federated Learning in a Medical Context: A Systematic Literature Review",
        "authors": "['Bjarne Pfitzner', 'Nico Steckhan', 'Bert Arnrich']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "Data privacy is a very important issue. Especially in fields like medicine, it is paramount to abide by the existing privacy regulations to preserve patients’ anonymity. However, data is required for research and training machine learning models that could help gain insight into complex correlations or personalised treatments that may otherwise stay undiscovered. Those models generally scale with the amount of data available, but the current situation often prohibits building large databases across sites. So it would be beneficial to be able to combine similar or related data from different sites all over the world while still preserving data privacy. Federated learning has been proposed as a solution for this, because it relies on the sharing of machine learning models, instead of the raw data itself. That means private data never leaves the site or device it was collected on. Federated learning is an emerging research area, and many domains have been identified for the application of those methods. This systematic literature review provides an extensive look at the concept of and research into federated learning and its applicability for confidential healthcare datasets.",
        "link": "https://dl.acm.org/doi/10.1145/3412357",
        "category": "Databases"
    },
    {
        "title": "Distributed Data Persistency",
        "authors": "['Apostolos Kokolis', 'Antonis Psistakis', 'Benjamin Reidys', 'Jian Huang', 'Josep Torrellas']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Distributed applications such as key-value stores and databases avoid frequent writes to secondary storage devices to minimize performance degradation. They provide fault tolerance by replicating variables in the memories of different nodes, and using data consistency protocols to ensure consistency across replicas. Unfortunately, the reduced data durability guarantees provided can cause data loss or slow data recovery. In this environment, non-volatile memory (NVM) offers the ability to attain both high performance and data durability in distributed applications. However, it is unclear how to tie NVM memory persistency models to the existing data consistency frameworks, and what are the durability guarantees that the combination will offer to distributed applications.  In this paper, we propose the concept of Distributed Data Persistency (DDP) model, which is the binding of the memory persistency model with the data consistency model in a distributed system. We reason about the interaction between consistency and persistency by using the concepts of Visibility Point and Durability Point. We design low-latency distributed protocols for DDP models that combine five consistency models with five persistency models. For the resulting DDP models, we investigate the trade-offs between performance, durability, and intuition provided to the programmer.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480060",
        "category": "Databases"
    },
    {
        "title": "New query optimization techniques in the Spark engine of Azure synapse",
        "authors": "['Abhishek Modi', 'Kaushik Rajan', 'Srinivas Thimmaiah', 'Prakhar Jain', 'Swinky Mann', 'Ayushi Agarwal', 'Ajith Shetty', 'Shahid K I', 'Ashit Gosalia', 'Partho Sarthi']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The cost of big-data query execution is dominated by stateful operators. These include sort and hash-aggregate that typically materialize intermediate data in memory, and exchange that materializes data to disk and transfers data over the network. In this paper we focus on several query optimization techniques that reduce the cost of these operators. First, we introduce a novel exchange placement algorithm that improves the state-of-the-art and significantly reduces the amount of data exchanged. The algorithm simultaneously minimizes the number of exchanges required and maximizes computation reuse via multi-consumer exchanges. Second, we introduce three partial push-down optimizations that push down partial computation derived from existing operators (group-bys, intersections and joins) below these stateful operators. While these optimizations are generically applicable we find that two of these optimizations (partial aggregate and partial semi-join push-down) are only beneficial in the scale-out setting where exchanges are a bottleneck. We propose novel extensions to existing literature to perform more aggressive partial push-downs than the state-of-the-art and also specialize them to the big-data setting. Finally we propose peephole optimizations that specialize the implementation of stateful operators to their input parameters. All our optimizations are implemented in the spark engine that powers azure synapse. We evaluate their impact on TPCDS and demonstrate that they make our engine 1.8X faster than Apache Spark 3.0.1.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503601",
        "category": "Databases"
    },
    {
        "title": "Demand-Driven Data Provisioning in Data Lakes: BARENTS — A Tailorable Data Preparation Zone",
        "authors": "['Christoph Stach', 'Julia Bräcker', 'Rebecca Eichler', 'Corinna Giebler', 'Bernhard Mitschang']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "Data has never been as significant as it is today. It can be acquired virtually at will on any subject. Yet, this poses new challenges towards data management, especially in terms of storage (data is not consumed during processing, i. e., the data volume keeps growing), flexibility (new applications emerge), and operability (analysts are no IT experts). The goal has to be a demand-driven data provisioning, i. e., the right data must be available in the right form at the right time. Therefore, we introduce a tailorable data preparation zone for Data Lakes called BARENTS. It enables users to model in an ontology how to derive information from data and assign the information to use cases. The data is automatically processed based on this model and the refined data is made available to the appropriate use cases. Here, we focus on a resource-efficient data management strategy. BARENTS can be embedded seamlessly into established Big Data infrastructures, e. g., Data Lakes.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487784",
        "category": "Databases"
    },
    {
        "title": "Construction of Personalized Recommendation System of University Library Based on SOM Neural Network",
        "authors": "['Yuan Liu']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "Unreasonable resource classification and imperfect resource retrieval mechanism are important issues in the digital construction of university libraries. Based on the characteristics of SOM neural network clustering algorithm with no parameters, high accuracy and strong objectivity, the article firstly clusters and optimizes the Web access behavior of the library users of the Nanfang College of Sun Yat-sen University. Secondly, based on the output of user analysis results, the user's personal feature information, user behavior data, and literature databases and other related data resources are filtered and integrated to form a related data set with higher reliability and availability, combined with semantic retrieval and attribute value matching. Technology to build a personalized recommendation service system for university library users. Finally, the effectiveness of the system is verified, and the coordination of the three subsystems of library theme recommendation, book recommendation and expert recommendation is realized. Through the calculation of the correlation between the user and the characteristics of the document resources, the user's points of interest and the cluster set where they are located are further identified.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3484129",
        "category": "Databases"
    },
    {
        "title": "PMNet: in-network data persistence",
        "authors": "['Korakit Seemakhupt', 'Sihang Liu', 'Yasas Senevirathne', 'Muhammad Shahbaz', 'Samira Khan']",
        "date": "June 2021",
        "source": "ISCA '21: Proceedings of the 48th Annual International Symposium on Computer Architecture",
        "abstract": "To guarantee data persistence, storage workloads (such as key-value stores and databases) typically use a synchronous protocol that places the network and server stack latency on the critical path of request processing. The use of the fast and byte-addressable persistent memory (PM) has helped mitigate the storage overhead of the server stack; yet, networking is still a dominant factor in the end-to-end latency of request processing. Emerging programmable network devices can reduce network latency by moving parts of the applications' compute into the network (e.g., caching results for read requests); however, for update requests, the client still has to stall on the server to commit the updates, persistently. In this work, we introduce in-network data persistence that extends the data-persistence domain from servers to the network, and present PMNet, a programmable data plane (e.g., switch or NIC) with PM for persisting data in the network. PMNet logs incoming update requests and acknowledges clients directly without having them wait on the server to commit the request. In case of a failure, the logged requests act as redo logs for the server to recover. We implement PMNet on an FPGA and evaluate its performance using common PM workloads, including key-value stores and PM-backed applications. Our evaluation shows that PMNet can improve the throughput of update requests by 4.31X on average, and the 99th-percentile tail latency by 3.23X.",
        "link": "https://dl.acm.org/doi/10.1109/ISCA52012.2021.00068",
        "category": "Databases"
    },
    {
        "title": "Query predicate selectivity using machine learning in Db2®",
        "authors": "['Waikeat Tan', 'Mohammed Alhamid', 'Mohamad Kalil', 'Ronghao Yang', 'Vincent Corvinelli', 'Calisto Zuzarte', 'Liam Finnie']",
        "date": "November 2021",
        "source": "CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering",
        "abstract": "The accuracy of cardinality estimation or the number of rows flowing through the query execution plan operators plays an important role in SQL query optimization. Cost-based optimizers depend on cardinality estimation to evaluate execution costs to select an optimal access plan. Achieving accurate cardinality estimation is difficult or expensive on tables that have correlated or skewed columns. Inaccurate cardinality estimation can lead to slow or unstable query performance. Although collecting statistics on multiple column combinations can minimize estimation errors with multiple predicates, it is hard to cover all column combinations. This paper presents a novel integrated approach using Machine Learning (ML) to learn and approximate the multivariate Cumulative Frequency Function (CFF) of column values, which is used to estimate cardinality for predicates with various relational operators. The key idea is that a model can learn the distribution of the data in the relation and can be used to predict cardinality for the query predicates accurately. The CFF model is also extended to estimate join cardinalities between tables. Experimental results demonstrate a significant improvement of cardinality estimation accuracy, computation efficiency, and amount of input required to train the model. Integration with the traditional optimizer is key to a smooth transition towards use in a production environment. This paper covers earlier technology previews shipped with Db2®.",
        "link": "https://dl.acm.org/doi/10.5555/3507788.3507808",
        "category": "Databases"
    },
    {
        "title": "HUGE: An Efficient and Scalable Subgraph Enumeration System",
        "authors": "['Zhengyi Yang', 'Longbin Lai', 'Xuemin Lin', 'Kongzhang Hao', 'Wenjie Zhang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Subgraph enumeration is a fundamental problem in graph analytics, which aims to find all instances of a given query graph on a large data graph. In this paper, we propose a system called HUGE to efficiently process subgraph enumeration at scale in the distributed context. HUGE features 1) an optimiser to compute an advanced execution plan without the constraints of existing works; 2) a hybrid communication layer that supports both pushing and pulling communication; 3) a novel two-stage execution mode with a lock-free and zero-copy cache design; 4) a BFS/DFS-adaptive scheduler to bound memory consumption; and 5) two-layer intra- and inter-machine load balancing. HUGE is generic such that all existing distributed subgraph enumeration algorithms can be plugged in to enjoy automatic speed up and bounded-memory execution.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457237",
        "category": "Databases"
    },
    {
        "title": "Logical Schema Design that Quantifies Update Inefficiency and Join Efficiency",
        "authors": "['Sebastian Link', 'Ziheng Wei']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The goal of classical normalization is to maintain data consistency under updates, with a minimum level of effort. Given functional dependencies (FDs) alone, this goal is only achievable in the special case an FD-preserving Boyce-Codd Normal Form (BCNF) decomposition exists. As we show, in all other cases the level of effort can be neither controlled nor quantified. In response, we establish the l-Bounded Cardinality Normal Form, parameterized by a positive integer l. For every l, the normal form condition requires from every instance that every value combination over the left-hand side of every non-trivial FD does not occur in more than l tuples. BCNF is captured when l=1. We demonstrate that schemata in this normal form characterize the instances that are i) free from level l data redundancy and update inefficiency, and ii) permit level l join efficiency. We establish algorithms that compute schemata in l-Bounded Cardinality Normal Form for the smallest level l attainable across all FD-preserving decompositions. Additional algorithms i) attain even smaller levels of effort based on the loss of some FDs, and ii) decompose schemata based on prioritized FDs that cause high levels of effort. Our framework informs de-normalization already during logical design. In particular, level l quantifies both the incremental maintenance and join support of materialized views. Experiments with synthetic and real-world data illustrate which properties the schemata have that result from our algorithms, and how these properties predict the performance of update and query operations on instances over the schemata, without and with materialized views.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3459238",
        "category": "Databases"
    },
    {
        "title": "Fifer: Practical Acceleration of Irregular Applications on Reconfigurable Architectures",
        "authors": "['Quan M. Nguyen', 'Daniel Sanchez']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "Coarse-grain reconfigurable arrays (CGRAs) can achieve much higher performance and efficiency than general-purpose cores, approaching the performance of a specialized design while retaining programmability. Unfortunately, CGRAs have so far only been effective on applications with regular compute patterns. However, many important workloads like graph analytics, sparse linear algebra, and databases, are irregular applications with unpredictable access patterns and control flow. Since CGRAs map computation statically to a spatial fabric of functional units, irregular memory accesses and control flow cause frequent stalls and load imbalance.  We present Fifer, an architecture and compilation technique that makes irregular applications efficient on CGRAs. Fifer first decouples irregular applications into a feed-forward network of pipeline stages. Each resulting stage is regular and can efficiently use the CGRA fabric. However, irregularity causes stages to have widely varying loads, resulting in high load imbalance if they execute spatially in a conventional CGRA. Fifer solves this by introducing dynamic temporal pipelining: it time-multiplexes multiple stages onto the same CGRA, and dynamically schedules stages to avoid load imbalance. Fifer makes time-multiplexing fast and cheap to quickly respond to load imbalance while retaining the efficiency and simplicity of a CGRA design. We show that Fifer improves performance by gmean 2.8 × (and up to 5.5 ×) over a conventional CGRA architecture (and by gmean 17 × over an out-of-order multicore) on a variety of challenging irregular applications.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480048",
        "category": "Databases"
    },
    {
        "title": "SVHAN: Sequential View Based Hierarchical Attention Network for 3D Shape Recognition",
        "authors": "['Yue Zhao', 'Weizhi Nie', 'An-An Liu', 'Zan Gao', 'Yuting Su']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "As an important field of multimedia, 3D shape recognition has attracted much research attention in recent years. A lot of deep learning models have been proposed for effective 3D shape representation. The view-based methods show the superiority due to the comprehensive exploration of the visual characteristics with the help of established 2D CNN architectures. Generally, the current approaches contain the following disadvantages: First, the most majority of methods lack the consideration for sequential information among the multiple views, which can provide descriptive characteristics for shape representation. Second, the incomprehensive exploration for the multi-view correlations directly affects the discrimination of shape descriptors. Finally, roughly aggregating multi-view features leads to the loss of descriptive information, which limits the shape representation effectiveness. To handle these issues, we propose a novel sequential view based hierarchical attention network (SVHAN) for 3D shape recognition. Specifically, we first divide the view sequence into several view blocks. Then, we introduce a novel hierarchical feature aggregation module (HFAM), which hierarchically exploits the view-level, block-level, and shape-level features, the intra- and inter- view-block correlations are also captured to improve the discrimination of learned features. Subsequently, a novel selective fusion module (SFM) is designed for feature aggregation, considering the correlations between different levels and preserving effective information. Finally, discriminative and informative shape descriptors are generated for the recognition task. We validate the effectiveness of our proposed method on two public databases. The experimental results show the superiority of SVHAN against the current state-of-the-art approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475371",
        "category": "Databases"
    },
    {
        "title": "Randomized Evaluation of Reading Skills: an Opportunity for Systematic Literature Review",
        "authors": "['Jesus Honorato-Errazuriz', 'Maria Soledad Ramirez-Montoya']",
        "date": "October 2021",
        "source": "TEEM'21: Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM&apos;21)",
        "abstract": "In the context of public policies, randomized evaluation of the impact of public programs and policies is fundamental because it provides the most credible and reliable way to learn what works and what does not in education to reduce poverty and improve the well-being of individuals and society. There is an urgency in the literature emerging during the pandemic to develop each student's reading skills since there is no development without literacy. For this reason, research studies report randomized control trials (RCT) for comprehensive reading programs that are of great interest to the educational system. This article aims to analyze the characteristics of these studies and the trends of new contributions to education. To achieve this, we conducted a systematic literature review (SLR) of 63 articles published between January 2015 and January 2019 in the Web of Science (WoS) and Scopus databases. We explored three themes using seven questions. The themes were a) Impact evaluation and randomized control trial (RCT), its approach and criteria; b) innovation and technology in reading programs; and c) the use of technology in the social appropriation of knowledge. The findings showed an increase in randomized control trials in impact evaluation, the need to optimize the quality of these studies, and the challenge of integrating innovation and technology in reading programs. We concluded that increasing and optimizing the impact evaluation approach in these topic research contributes in a substantive way to researchers and government decision making and to advance in the path of achieving a fair, equitable and quality education for all.",
        "link": "https://dl.acm.org/doi/10.1145/3486011.3486527",
        "category": "Databases"
    },
    {
        "title": "A-Tree: A Dynamic Data Structure for Efficiently Indexing Arbitrary Boolean Expressions",
        "authors": "['Shuping Ji', 'Hans-Arno Jacobsen']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Efficiently evaluating a large number of arbitrary Boolean expressions is needed in many applications such as advertising exchanges, complex event processing, and publish/subscribe systems. However, most solutions can support only conjunctive Boolean expression matching. The limited number of solutions that can directly work on arbitrary Boolean expressions present performance and flexibility limitations. Moreover, normalizing arbitrary Boolean expressions into conjunctive forms and then using existing methods for evaluating such expressions is not effective because of the potential exponential increase in the size of the expressions. Therefore, we propose the A-Tree data structure to efficiently index arbitrary Boolean expressions. A-Tree is a multirooted tree, in which predicates and subexpressions from different arbitrary Boolean expressions are aggregated and shared. A-Tree employs dynamic self-adjustment policies to adapt itself as the workload changes. Moreover, A-Tree adopts different event matching optimizations. Our comprehensive experiments show that A-Tree-based matching outperforms existing arbitrary Boolean expression matching algorithms in terms of memory use, matching time, and index construction time by up to 71%, 99% and 75%, respectively. Even on conjunctive expression workloads, A-Tree achieves a lower matching time than state-of-the-art conjunctive expression matching algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457266",
        "category": "Databases"
    },
    {
        "title": "Early Results from Automating Voice-based Question-Answering Services Among Low-income Populations in India",
        "authors": "['Aman Khullar', 'M Santosh', 'Praveen Kumar', 'Shoaib Rahman', 'Rajeshwari Tripathi', 'Deepak Kumar', 'Sangeeta Saini', 'Rachit Pandey', 'Aaditeshwar Seth']",
        "date": "June 2021",
        "source": "COMPASS '21: Proceedings of the 4th ACM SIGCAS Conference on Computing and Sustainable Societies",
        "abstract": "Question-answering systems where users can ask questions based on emergent needs which are then answered by experts or peers, have emerged as an important information seeking modality on digital platforms. Automating this process has been an active area of research since many years, to identify relevant answers from pre-existing question-answer databases. We report on the feasibility of running automated question-answering systems in the context of rural and less-literate users in India, accessed through IVR (Interactive Voice Response) systems. We use commercial speech recognition APIs to convert audio questions asked by users into their equivalent transcripts in real time, in Hindi, and use deep-learning based architectures to retrieve corresponding candidate answers which are instantly played to the users. We report several insights from an earlier phase of running question-answering programmes through a manual operation, to how it was transitioned to an automated setup, and document the user experiences during this journey.",
        "link": "https://dl.acm.org/doi/10.1145/3460112.3471946",
        "category": "Databases"
    },
    {
        "title": "Online Topic-Aware Entity Resolution Over Incomplete Data Streams",
        "authors": "['Weilong Ren', 'Xiang Lian', 'Kambiz Ghazinour']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In many real applications such as the data integration, social network analysis, and the Semantic Web, the entity resolution (ER) is an important and fundamental problem, which identifies and links the same real-world entities from various data sources. While prior works usually consider ER over static and complete data, in practice, application data are usually collected in a streaming fashion, and often incur missing attributes (due to the inaccuracy of data extraction techniques). Therefore, in this paper, we will formulate and tackle a novel problem, topic-aware entity resolution over incomplete data streams (TER-iDS), which online imputes incomplete tuples and detects pairs of topic-related matching entities from incomplete data streams. In order to effectively and efficiently tackle the TER-iDS problem, we propose an effective imputation strategy, carefully design effective pruning strategies, as well as indexes/synopsis, and develop an efficient TER-iDS algorithm via index joins. Extensive experiments have been conducted to evaluate the effectiveness and efficiency of our proposed TER-iDS approach over real data sets.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457238",
        "category": "Databases"
    },
    {
        "title": "Learning to Understand Traffic Signs",
        "authors": "['Yunfei Guo', 'Wei Feng', 'Fei Yin', 'Tao Xue', 'Shuqi Mei', 'Cheng-Lin Liu']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "One of the intelligent transportation system's critical tasks is to understand traffic signs and convey traffic information to humans. However, most related works are focused on the detection and recognition of traffic sign texts or symbols, which is not sufficient for understanding. Besides, there has been no public dataset for traffic sign understanding research. Our work takes the first step towards addressing this problem. First, we propose a \"CASIA-Tencent Chinese Traffic Sign Understanding Dataset\" (CTSU Dataset), which contains 5000 images of traffic signs with rich semantic descriptions. Second, we introduce a novel multi-task learning architecture that extracts text and symbol information from traffic signs, reasons the relationship between texts and symbols, classifies signs into different categories, and finally, composes the descriptions of the signs. Experiments show that the task of traffic sign understanding is achievable, and our architecture demonstrates state-of-the-art and superior performance. The CTSU Dataset is available at http://www.nlpr.ia.ac.cn/databases/CASIA-Tencent%20CTSU/index.html.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475362",
        "category": "Databases"
    },
    {
        "title": "Applying GA-SVM for Optimizing Statistical and Semantic Features in Document Classification",
        "authors": "['Upasana Pandey', 'Geeta Rani', 'Vijaypal Singh Dhaka']",
        "date": "August 2021",
        "source": "DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence",
        "abstract": "The objective of this research is to develop a hybrid model for optimizing the performance of text classification techniques. The authors applied the Genetic Algorithm and Multi-Class Support Vector Machine on the publicly available datasets viz. 20 Newsgroup corpus, and the Reuters 21,578 corpus. They also used their handcrafted 'Creative corpus' prepared by collecting news articles from the Times of India news portal. They evaluated the performance of their model on large as well as small corpora. They employed the Genetic Algorithm that dynamically decides the weights of the contextual features to achieve the highest classification accuracy. The model achieves the highest accuracy of 100 % on small datasets of Reuters 21,578 and Creative corpus. The authors also presented a comparative analysis of the statistical and context-based approaches applied for the text classification. Based on the experimental results they proved that statistical approaches are better for text classification in the case of small-sized documents. Whereas the context-based approaches are efficient in the classification of huge documents enriched with text. This showed the importance of the hybrid approach. The hybrid approach taps the power of ontological databases and can adapt to varying corpora flawlessly. Thus, it makes effective use of textual data available in reports for crime detection, crime classification, and disease diagnosis, etc.",
        "link": "https://dl.acm.org/doi/10.1145/3484824.3484912",
        "category": "Databases"
    },
    {
        "title": "A blockchain-based method and system for trading technological achievements (take the Chinese market as an example)",
        "authors": "['Man Xiang', 'Wenlin Zhang', 'Fan Jiang', 'Yuchong Fang', 'Tao An', 'Ken Lin', 'Tianhao Wu', 'Cai Zeng']",
        "date": "December 2021",
        "source": "ICIT '21: Proceedings of the 2021 9th International Conference on Information Technology: IoT and Smart City",
        "abstract": "With the development of a new generation of information and network technology, the number and types of patent infringements continue to increase, and the types of patented products are also more extensive, which hinders the certification and confirmation of patented products, authorized transactions, and patent evaluation. However, the traditional centralized information system is difficult to meet the demand. Blockchain technology provides a shared ledger mechanism and lays the foundation for the construction of credible distributed transaction management. Therefore, this platform is based on blockchain technology to create a safe, stable, transparent and efficient patent service system to provide customers with accurate, objective and efficient quality services.",
        "link": "https://dl.acm.org/doi/10.1145/3512576.3512610",
        "category": "Databases"
    },
    {
        "title": "Factors affecting Engagement in Digital Educational Games",
        "authors": "['Chaitanya Solanki', 'Deepak John Mathew']",
        "date": "November 2021",
        "source": "IndiaHCI '21: Proceedings of the 12th Indian Conference on Human-Computer Interaction",
        "abstract": "Games, for the purpose of education, have been used in the world since the 1970s. The mediums used for these activities range from board games to Virtual Reality(VR), and the topics covered under them can range from history to STEM subjects. The increasing use of educational games has made it essential to verify these tools for their efficacy by evaluating them from several dimensions. This paper presents a Systematic Literature Review (SLR) on studies where educational games were applied to focus groups and tries to report on the various aspects that reportedly contribute to increased engagement. The review was according to the ‘Preferred Reporting Items for Systematic Reviews and Meta-Analyses’ (PRISMA) guidelines. The research was initiated with a sample of 60 articles, out of which a total of 10 relevant articles were identified and further examined. The extent of the research was to highlight the aspects of the educational game which were catalyzing the level of engagement. Other secondary findings were about research topics, type of resources, and evaluation approaches. The findings of this paper provide insights for researchers and evaluators into current practices of making an educational game engaging. The research was carried out using the databases from SCOPUS, Springer, and Taylor & Francis. Considering keywords, we included articles that showed the terms: ‘Video’ and/or ‘Games’ and/or ‘Digital’ and/or ‘Educational’ and/or Learning’ and/or ‘Games’ and/or ‘STEM’ and/or ‘Subjects’. The paper concludes by identifying several aspects in educational games which could influence the engagement levels, such as the progression of difficulty, visual cues for better navigation, modular templates, personalized avatars, etc. Additional research is needed to individually analyze all the observations noted in this paper to accurately model the effects of a game.",
        "link": "https://dl.acm.org/doi/10.1145/3506469.3506491",
        "category": "Databases"
    },
    {
        "title": "Health Care Workers' Use of Electronic Medical Information Systems: Benefits and Challenges",
        "authors": "['Abdelbaset M. Elghriani', 'Abdelwanis A. Alabbar', 'Abdelsalam M. Maatuk', 'Ehab A. Elfallah']",
        "date": "April 2021",
        "source": "DATA'21: International Conference on Data Science, E-learning and Information Systems 2021",
        "abstract": "Although Medical (or patient) records are important as they constitute the documentation of patients' health status. The advances in information technologies are reflected in electronic patient records so that patient data can be stored in databases, which could have a positive impact on patient care. Therefore, identifying the important benefits, and studying the obstacles that limit the effectiveness of using Electronic Medical Information Systems (EMISs) needs further investigation. This study aims to investigate the use of EMISs by medical and ancillary medical staff at Benghazi Medical Center, Libya. The descriptive-analytical method has been applied along with the analysis of results by the statistical methods. The study identifies the skills of the medical staff in using computers and determines the extent of their awareness of using EMISs. The study also determines the level of their knowledge of the effectiveness of using this type of system, while identifying the benefits and obstacles that prevent their use in the fields of the medical workplace. The results show that, despite some difficulties in their usage, the benefits of EMISs compensate for possible obstacles.",
        "link": "https://dl.acm.org/doi/10.1145/3460620.3460627",
        "category": "Databases"
    },
    {
        "title": "CoRM: Compactable Remote Memory over RDMA",
        "authors": "['Konstantin Taranov', 'Salvatore Di Girolamo', 'Torsten Hoefler']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Distributed memory systems are becoming increasingly important since they provide a system-scale abstraction where physically separated memories can be addressed as a single logical one. This abstraction enables memory disaggregation, allowing systems as in-memory databases, caching services, and ephemeral storage to be naturally deployed at large scales. While this abstraction effectively increases the memory capacity of these systems, it faces additional overheads for remote memory accesses. To narrow the difference between local and remote accesses, low latency RDMA networks are a key element for efficient memory disaggregation. However, RDMA acceleration poses new obstacles to efficient memory management and particularly to memory compaction: network controllers and CPUs can concurrently access memory, potentially leading to inconsistencies if memory management operations are not synchronized. To ensure consistency, most distributed memory systems do not provide memory compaction and are exposed to memory fragmentation. We introduce CoRM, an RDMA-accelerated shared memory system that supports memory compaction and ensures strict consistency while providing one-sided RDMA accesses. We show that CoRM sustains high read throughput during normal operations, comparable to similar systems not providing memory compaction while experiencing minimal overheads during compaction. CoRM never disrupts RDMA connections and can reduce applications' active memory up to 6x by performing memory compaction.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452817",
        "category": "Databases"
    },
    {
        "title": "Technology-Assisted Problem-Based Learning against Common Problem-Based Learning in Cultivating Mathematical Critical Thinking Skills: A Meta-Analysis",
        "authors": "['Debi S. Fuadi', 'Suparman Suparman', 'Dadang Juandi', 'Bambang Avip Priatna Martadiputra']",
        "date": "December 2021",
        "source": "ICETM '21: Proceedings of the 2021 4th International Conference on Education Technology Management",
        "abstract": "This study aims to compare the effect of technology-assisted problem-based learning (PBL) and the common PBL in cultivating students' mathematical critical thinking skills (MCTS). A meta-analysis was used to conduct this study by selecting the random effect model. The Q Cochrane and Z test were employed to analyze data supported by Comprehensive Meta-Analysis (CMA) software. Literature search using some databases such as Google Scholar, Semantic Scholar, and DOAJ found 155 documents. The final process of literature selection established 30 documents published from 2017 to 2021 and indexed by Scopus, Web of Science, and Google Scholar. Results showed that the effect size of technology-assisted PBL on students’ MCTS was g = 0,979 (moderate effect), while the effect size of common PBL on students’ MCTS was g = 0,606 (moderate effect). In addition, there was a significant difference of MCTS between students who got technology-assisted PBL and students who got common PBL. It interprets that in cultivating students’ MCTS, the implementation of technology-assisted PBL was better than the implementation of common PBL. So, the use of technology in mathematics learning to support PBL should be employed by mathematics teachers in cultivating students’ MCTS.",
        "link": "https://dl.acm.org/doi/10.1145/3510309.3510335",
        "category": "Databases"
    },
    {
        "title": "A comparison of ETL (extract, transform, and load) tools: Python vs. Microsoft SQL server integration services (SSIS)",
        "authors": "['Guangyan Li', 'Mary Donahoo', 'Mario Guimaraes']",
        "date": "None",
        "source": "Journal of Computing Sciences in Colleges",
        "abstract": "ETL (extract, transform, and load) is a process of gathering large volumes of raw data from multiple (internal and external) sources, transforming the data to meet certain needs, and consolidating data into a single, centralized location such as data warehouse database for analytics or storage. There are many ETL tools available in the data warehousing market, and the choice of ETL tools depends on many factors such as company data needs, existing technology, and budget. Microsoft SQL Server Integration Services (SSIS) is an integrated ETL tool for building enterprise-level data integration and data transformations solutions, and it is a natural choice if SQL Server and other related Microsoft technologies are used. Python is a popular programming language choice for data analytics and data science. Python offers a variety of free ETL tools such as Apache Airflow, Petl and Pandas.",
        "link": "https://dl.acm.org/doi/10.5555/3512469.3512488",
        "category": "Databases"
    },
    {
        "title": "Identifying Legal Party Members from Legal Opinion Documents using Natural Language Processing",
        "authors": "['Melonie de Almeida', 'Chamodi Samarawickrama', 'Nisansa de Silva', 'Gathika Ratnayaka', 'Shehan Perera']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "Law and order is a field that can highly benefit from the contribution of Natural Language Processing (NLP) to its betterment. An area in which NLP can be of immense help is, information retrieval from legal documents which function as legal databases. The extraction of legal parties from the aforementioned legal documents can be identified as a task of high importance since it has a significant impact on the proceedings of contemporary legal cases. This study proposes a novel deep learning methodology which can be effectively used to find a solution to the problem of identifying legal party members in legal documents. In addition to that, in this paper, we introduce a novel data set which is annotated with legal party information by an expert in the legal domain. The deep learning model proposed in this study provides a benchmark for the legal party identification task on this data set. Evaluations for the solution presented in the paper show that our system has 90.89% precision and 91.69% recall for an unseen paragraph from a legal document, thus conforming the success of our attempt.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487700",
        "category": "Databases"
    },
    {
        "title": "An in-depth study of continuous subgraph matching",
        "authors": "['Xibo Sun', 'Shixuan Sun', 'Qiong Luo', 'Bingsheng He']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Continuous subgraph matching (CSM) algorithms find the occurrences of a given pattern on a stream of data graphs online. A number of incremental CSM algorithms have been proposed. However, a systematical study on these algorithms is missing to identify their advantages and disadvantages on a wide range of workloads. Therefore, we first propose to model CSM as incremental view maintenance (IVM) to capture the design space of existing algorithms. Then, we implement six representative CSM algorithms, including InclsoMatch, SJ-Tree, Graphflow, IEDyn, TurboFlux, and SymBi, in a common framework based on IVM. We further conduct extensive experiments to evaluate the overall performance of competing algorithms as well as study the effectiveness of individual techniques to pinpoint the key factors leading to the performance differences. We obtain the following new insights into the performance: (1) existing algorithms start the search from an edge in the query graph that maps to an updated data edge, potentially leading to many invalid partial results; (2) all matching orders are based on simple heuristics, which appear ineffective at times; (3) index updates dominate the query time on some queries; and (4) the algorithm with constant delay enumeration bears significant index update cost. Consequently, no algorithm dominate the others in all cases. Therefore, we give a few recommendations based on our experiment results. In particular, the SymBi index is useful for sparse queries or long running queries. The matching orders of IEDyn and TurboFlux work well on tree queries, those of Graphflow on dense queries or when both query and data graphs are sparse, and otherwise, we recommend SymBi's matching orders.",
        "link": "https://dl.acm.org/doi/10.14778/3523210.3523218",
        "category": "Databases"
    },
    {
        "title": "Building secure, resilient and compliant solutions on the IBM cloud for financial services",
        "authors": "['Neil De Lima']",
        "date": "November 2021",
        "source": "CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering",
        "abstract": "The IBM Cloud for Financial services is an industry specialized public cloud, grounded in enterprise grade security and open technology. It provides the security, compliance and resiliency required by financial institutions to confidently operate mission critical workloads in a public cloud environment. At the core of the IBM Cloud for Financial services is the IBM Cloud Framework for Financial Services. This controls framework defines technical and operational controls that \"validated\" IBM Cloud Services comply to. Independent software vendors can then leverage these cloud services to build FS Cloud compliant solutions that also need to comply to the controls of the controls framework. This enables financial institutions, and their technology ecosystem partners, to securely and confidently transact with each other. The purpose of this 90 minute workshop was to provide attendees guidance on how to build and operate secure, resilient, and regulatory compliant solutions by using the IBM Cloud for Financial Services. In this workshop, attendees were introduced to the IBM Cloud for Financial Services and the types of security and compliance controls included in the controls framework. They learned about the IBM Cloud infrastructure and platform services that have been designated as FS validated. Examples of these services include the Virtual Private Cloud compute and related networking and storage options, Hyper Protect Crypto Services, and IBM Cloud databases to name a few. Guidance was provided on how to leverage these validated services and other 3rd party software components to architect secure and resilient solutions on the IBM Cloud for Financial Services. The information provided during the workshop along with useful references that provide detailed information, would enable attendees develop and operate solutions on the IBM Cloud for Financial Services and ensure that they are compliant with the controls framework.",
        "link": "https://dl.acm.org/doi/10.5555/3507788.3507852",
        "category": "Databases"
    },
    {
        "title": "A comparative study of cell type annotation methods for immune cells using single-cell sequencing technology",
        "authors": "['Tianai Zhang']",
        "date": "October 2021",
        "source": "ICBBS '21: Proceedings of the 2021 10th International Conference on Bioinformatics and Biomedical Science",
        "abstract": "Abstract: Single-cell sequencing is an emerging technique that allows high-throughput data analysis at an individual cell resolution and is applied in diverse fields of biology. Due to the large amount of data, downstream analysis is very complicated, and cell type annotation is a critical step; however, it is currently difficult to obtain good results. Single-cell sequencing has resulted in new breakthroughs in multi-omics, such as CITE-seq (Cellular Indexing of Transcriptomes and Epitopes by sequencing), which allows the measurement of surface marker proteins simultaneously with the sequencing of mRNA at the single-cell level. In this study, a CITE-seq dataset of human PBMCs (peripheral blood mononuclear cells) was annotated using the most popular reference-based annotation methods, including SingleR, Seurat with the RNA-seq dataset, and Seurat with both the RNA and protein datasets; the results were then compared with RNA and protein expression levels to determine the role of proteins in cell annotation. The results indicate that protein expression can supplement datasets with some mRNAs with low expression to improve accuracy. With the verification of single-cell biomarkers, the multi-omics annotation method Seurat with both the RNA and protein databases showed the best performance, especially in the differentiation of NK cells and T cells and of dendritic cells and monocytes. This study shows the significance of multi-omics information for improving cell annotation and has great potential for perfecting these annotations with more data support.",
        "link": "https://dl.acm.org/doi/10.1145/3498731.3498735",
        "category": "Databases"
    },
    {
        "title": "Effect of Sensory-based Technologies on Atypical Sensory Responses of Children with Autism Spectrum Disorder: A Systematic Review",
        "authors": "['Lingling Deng', 'Prapa Rattadilok', 'Gabrielle Saputra Hadian', 'Haoyang Liu']",
        "date": "August 2021",
        "source": "ICSET 2021: 2021 5th International Conference on E-Society, E-Education and E-Technology",
        "abstract": "Atypical sensory responses are one of the most common issues observed in Autism Spectrum Disorder (ASD), affecting the development of a child's capability for social interaction, independent living and learning. In the past two decades, there has been a growing number of studies of technology-based interventions for atypical sensory responses of individuals with ASD. However, their effects and limitations have not been fully examined. This systematic review investigates the effects of sensory-based technologies (SBTs) on atypical sensory responses of children with ASD. Publications that report on the use of a SBT as an intervention tool were retrieved from four academic databases: “PubMed”, “IEEE Xplore”, “ACM Digital Library” and “Web of Science”. The search finally yielded 18 articles. The results indicated an emerging trend of studies investigating the effects of SBTs on atypical sensory responses over the past decade. Challenges and limitations were found in studies, mainly because the literatures adopted different methods and indicators, small sample sizes, and varying experimental designs. Findings were that the use of SBTs could effectively improve auditory and visual recognition, and some other behavioural outcomes such as attention in children with ASD. Future development of SBTs could further integrate more advanced techniques, such as machine learning, in order to widen the scope of SBTs usage to help more ASD children.",
        "link": "https://dl.acm.org/doi/10.1145/3485768.3485782",
        "category": "Databases"
    },
    {
        "title": "On Optimizing the Trade-off between Privacy and Utility in Data Provenance",
        "authors": "['Daniel Deutch', 'Ariel Frankenthal', 'Amir Gilad', 'Yuval Moskovitch']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Organizations that collect and analyze data may wish or be mandated by regulation to justify and explain their analysis results. At the same time, the logic that they have followed to analyze the data, i.e., their queries, may be proprietary and confidential. Data provenance, a record of the transformations that data underwent, was extensively studied as means of explanations. In contrast, only a few works have studied the tension between disclosing provenance and hiding the underlying query. This tension is the focus of the present paper, where we formalize and explore for the first time the tradeoff between the utility of presenting provenance information and the breach of privacy it poses with respect to the underlying query. Intuitively, our formalization is based on the notion of provenance abstraction, where the representation of some tuples in the provenance expressions is abstracted in a way that makes multiple tuples indistinguishable. The privacy of a chosen abstraction is then measured based on how many queries match the obfuscated provenance, in the same vein as k-anonymity. The utility is measured based on the entropy of the abstraction, intuitively how much information is lost with respect to the actual tuples participating in the provenance. Our formalization yields a novel optimization problem of choosing the best abstraction in terms of this tradeoff. We show that the problem is intractable in general, but design greedy heuristics that exploit the provenance structure towards a practically efficient exploration of the search space. We experimentally prove the effectiveness of our solution using the TPC-H benchmark and the IMDB dataset.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452835",
        "category": "Databases"
    },
    {
        "title": "KTabulator: Interactive Ad hoc Table Creation using Knowledge Graphs",
        "authors": "['Siyuan Xia', 'Nafisa Anzum', 'Semih Salihoglu', 'Jian Zhao']",
        "date": "May 2021",
        "source": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "abstract": "The need to find or construct tables arises routinely to accomplish many tasks in everyday life, as a table is a common format for organizing data. However, when relevant data is found on the web, it is often scattered across multiple tables on different web pages, requiring tedious manual searching and copy-pasting to collect data. We propose KTabulator, an interactive system to effectively extract, build, or extend ad hoc tables from large corpora, by leveraging their computerized structures in the form of knowledge graphs. We developed and evaluated KTabulator using Wikipedia and its knowledge graph DBpedia as our testbed. Starting from an entity or an existing table, KTabulator allows users to extend their tables by finding relevant entities, their properties, and other relevant tables, while providing meaningful suggestions and guidance. The results of a user study indicate the usefulness and efficiency of KTabulator in ad hoc table creation.",
        "link": "https://dl.acm.org/doi/10.1145/3411764.3445227",
        "category": "Databases"
    },
    {
        "title": "TQEL: framework for query-driven linking of top-k entities in social media blogs",
        "authors": "['Abdulrahman Alsaudi', 'Yasser Altowim', 'Sharad Mehrotra', 'Yaming Yu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Social media analysis over blogs (such as tweets) often requires determining top-k mentions of a certain category (e.g., movies) in a collection (e.g., tweets collected over a given day). Such queries require entity linking (EL) function to be executed that is often expensive. We propose TQEL, a framework that minimizes the joint cost of EL calls and top-k query processing. The paper presents two variants - TQEL-exact and TQEL-approximate that retrieve the exact / approximate top-k results. TQEL-approximate, using a weaker stopping condition, achieves significantly improved performance (with the fraction of the cost of TQEL-exact) while providing strong probabilistic guarantees (over 2 orders of magnitude lower EL calls with 95% confidence threshold compared to TQEL-exact). TQEL-exact itself is orders of magnitude better compared to a naive approach that calls EL functions on the entire dataset.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476309",
        "category": "Databases"
    },
    {
        "title": "SharPer: Sharding Permissioned Blockchains Over Network Clusters",
        "authors": "['Mohammad Javad Amiri', 'Divyakant Agrawal', 'Amr El Abbadi']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Scalability is one of the main roadblocks to business adoption of blockchain systems. Despite recent intensive research on using sharding techniques to enhance the scalability of blockchain systems, existing solutions do not efficiently address cross-shard transactions. In this paper, we introduce SharPer, a scalable permissioned blockchain system. In SharPer, nodes are clustered and each data shard is replicated on the nodes of a cluster. SharPer supports networks consisting of either crash-only or Byzantine nodes. In SharPer, the blockchain ledger is formed as a directed acyclic graph and each cluster maintains only a view of the ledger. SharPer incorporates decentralized flattened protocols to establish cross-shard consensus. The decentralized nature of the cross-shard consensus in SharPer enables parallel processing of transactions with nonoverlapping clusters. Furthermore, SharPer provides deterministic safety guarantees. The experimental results reveal the efficiency of SharPer in terms of performance and scalability especially in workloads with a low percentage of cross-shard transactions.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452807",
        "category": "Databases"
    },
    {
        "title": "A learned query rewrite system using Monte Carlo tree search",
        "authors": "['Xuanhe Zhou', 'Guoliang Li', 'Chengliang Chai', 'Jianhua Feng']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Query rewrite transforms a SQL query into an equivalent one but with higher performance. However, SQL rewrite is an NP-hard problem, and existing approaches adopt heuristics to rewrite the queries. These heuristics have two main limitations. First, the order of applying different rewrite rules significantly affects the query performance. However, the search space of all possible rewrite orders grows exponentially with the number of query operators and rules and it is rather hard to find the optimal rewrite order. Existing methods apply a pre-defined order to rewrite queries and will fall in a local optimum. Second, different rewrite rules have different benefits for different queries. Existing methods work on single plans but cannot effectively estimate the benefits of rewriting a query. To address these challenges, we propose a policy tree based query rewrite framework, where the root is the input query and each node is a rewritten query from its parent. We aim to explore the tree nodes in the policy tree to find the optimal rewrite query. We propose to use Monte Carlo Tree Search to explore the policy tree, which navigates the policy tree to efficiently get the optimal node. Moreover, we propose a learning-based model to estimate the expected performance improvement of each rewritten query, which guides the tree search more accurately. We also propose a parallel algorithm that can explore the tree search in parallel in order to improve the performance. Experimental results showed that our method significantly outperformed existing approaches.",
        "link": "https://dl.acm.org/doi/10.14778/3485450.3485456",
        "category": "Databases"
    },
    {
        "title": "Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets",
        "authors": "['Markus Schröder', 'Christian Jilek', 'Andreas Dengel']",
        "date": "December 2021",
        "source": "K-CAP '21: Proceedings of the 11th Knowledge Capture Conference",
        "abstract": "The RDF Mapping Language (RML) allows to map semi-structured data to RDF knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of spreadsheet tables. Since spreadsheets have a complex data model and can become rather messy, their mapping creation tends to be very time consuming. In order to reduce such efforts, this paper presents Spread2RML which predicts RML mappings on messy spreadsheets. This is done with an extensible set of RML object map templates which are applied for each column based on heuristics. In our evaluation, three datasets are used ranging from very messy synthetic data to spreadsheets from data.gov which are less messy. We obtained first promising results especially with regard to our approach being fully automatic and dealing with rather messy data.",
        "link": "https://dl.acm.org/doi/10.1145/3460210.3493544",
        "category": "Databases"
    },
    {
        "title": "Detecting Intrusion in Cloud using Snort: An Application towards Cyber-Security",
        "authors": "['Fahmida Rafa', 'Zinnet Rahman', 'Md Mahbub Mishu', 'Morad Hasan', 'Raiyan Rahman', 'Dip Nandi']",
        "date": "March 2022",
        "source": "ICCA '22: Proceedings of the 2nd International Conference on Computing Advancements",
        "abstract": "Internet, various kinds of services are delivered using cloud computing. These resources include storing data, servers, databases records, networking systems, and software. Many people choose cloud computing for businesses because of it’s budget-friendly, excellent efficiency and performance features. But lately, the intrusion on cloud-based system raised a significant concern for choosing this platform. Due to the increasing rate of computer networks and their usage, the global IT infrastructure is prone to attacks. If the issue is left unattended, it can cause significant trouble for IT sector. Intrusion detection systems are signature-based, means it would look in a certain type of data packet, and it would watch all the traffic in a network which has visibility to and make a decision if it is good traffic or bad. It alerts analyst to look at the traffic that could be malicious. Snort is an open-source intrusion detection system that can monitor any traffic going in or out of the network and also monitor malicious behavior or any violation. To enable secure and trustworthy information transmission across diverse cloud companies in today’s networked business settings, a high level of security is required. Because cyber-attacks are only getting more complicated these days, it is critical that defense technology keeps up. After traditional security technologies fail, an intrusion detection system functions as an adaptive safeguard device for system security. In our research, we have shown by using this protocol traffic from source 0.0.0.0:68 to DST 255.255.255:67 (UDP) is being blocked on our WAN. It prevents the user from obtaining a DHCP address.",
        "link": "https://dl.acm.org/doi/10.1145/3542954.3542984",
        "category": "Databases"
    },
    {
        "title": "Top-K Deep Video Analytics: A Probabilistic Approach",
        "authors": "['Ziliang Lai', 'Chenxia Han', 'Chris Liu', 'Pengfei Zhang', 'Eric Lo', 'Ben Kao']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The impressive accuracy of deep neural networks (DNNs) has created great demands on practical analytics over video data. Although efficient and accurate, the latest video analytic systems have not supported analytics beyond selection and aggregation queries. In data analytics, Top-K is a very important analytical operation that enables analysts to focus on the most important entities. In this paper, we present Everest, the first system that supports efficient and accurate Top-K video analytics. Everest ranks and identifies the most interesting frames/moments from videos with probabilistic guarantees. Everest is a system built with a careful synthesis of deep computer vision models, uncertain data management, and Top-K query processing. Evaluations on real-world videos and the latest Visual Road benchmark show that Everest achieves between 14.3x to 20.6x higher efficiency than baseline approaches with high result accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452786",
        "category": "Databases"
    },
    {
        "title": "Research on the Application of Artificial Intelligence Technology and Cloud Computing in Smart Elderly Care Information Platform",
        "authors": "['Huifen Sun']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "After entering the 21st century, China's aging situation has become more and more intense, and the embarrassing situation of \"getting old before getting rich\" has appeared. The traditional old-age care model can no longer meet today's needs. Therefore, relying on the \"Internet +\", the traditional home care, community old-age care, smart old-age care equipment, and medical resources are effectively integrated to create a community intelligent old-age care service model under the \"Internet +\" The future development trend of elderly care. This article introduces the \"cloud service platform\" into the elderly care service industry, and builds a new type of community development model based on the \"cloud service system\" that is suitable for aging. First, use sensing, Internet and cloud computing technologies to develop an information platform for the elderly at home, communities, and institutions, and establish service files and databases for smart community elderly care. Secondly, establish a smart community elderly care comprehensive service system to effectively connect medical institutions, service providers, the elderly, and families to provide living, medical, entertainment, and caring services. Finally, build a feedback mechanism using cloud computing as a means to achieve dynamic data updates. The system not only provides high-quality old-age life for the elderly, but also provides an information management platform for community old-age care, and provides a reference for future urban community old-age care.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3501120",
        "category": "Databases"
    },
    {
        "title": "Remember and Reuse: Cross-Task Blind Image Quality Assessment via Relevance-aware Incremental Learning",
        "authors": "['Rui Ma', 'Hanxiao Luo', 'Qingbo Wu', 'King Ngi Ngan', 'Hongliang Li', 'Fanman Meng', 'Linfeng Xu']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Existing blind image quality assessment (BIQA) methods have made great progress in various task-specific applications, including the synthetic, authentic, or over-enhanced distortion evaluations. However, limited by the static model and once-for-all learning strategy, they failed to perform the cross-task evaluations in many practical applications, where diverse evaluation criteria and distortion types are constantly emerging. To address this issue, in this paper, we propose a dynamic Remember and Reuse (R&R) network, which efficiently performs the cross-task BIQA based on a novel relevance-aware incremental learning strategy. Given multiple evaluation tasks across different distortion types or databases, our R&R network sequentially updates the parameters for every task one by one. After each update step, part of task-specific parameters is settled, which ensures R&R Remembers their dedicated evaluation preferences. The remaining parameters are pruned for the dynamic usage of the subsequent tasks. To further exploit the correlation between different tasks, we feed the training data of a new task to previously settled parameters. Better prediction accuracy is considered as higher task relevance and vice versa. Then, we selectively Reuse parts of previously settled parameters, whose proportion is adaptively determined by the task relevance. Extensive experiments show that the proposed method efficiently achieves the cross-task BIQA without catastrophic forgetting, and significantly outperforms many state-of-the-art methods. Code is available at https://github.com/maruiperfect/R-R-Net.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475642",
        "category": "Databases"
    },
    {
        "title": "cuTS: scaling subgraph isomorphism on distributed multi-GPU systems using trie based data structure",
        "authors": "['Lizhi Xiang', 'Arif Khan', 'Edoardo Serra', 'Mahantesh Halappanavar', 'Aravind Sukumaran-Rajam']",
        "date": "November 2021",
        "source": "SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
        "abstract": "Subgraph isomorphism is a pattern-matching algorithm widely used in many domains such as chem-informatics, bioinformatics, databases, and social network analysis. It is computationally expensive and is a proven NP-hard problem. The massive parallelism in GPUs is well suited for solving subgraph isomorphism. However, current GPU implementations are far from the achievable performance. Moreover, the enormous memory requirement of current approaches limits the problem size that can be handled. This work analyzes the fundamental challenges associated with processing subgraph isomorphism on GPUs and develops an efficient GPU implementation. We also develop a GPU-friendly trie-based data structure to drastically reduce the intermediate storage space requirement, enabling large benchmarks to be processed. We also develop the first distributed sub-graph isomorphism algorithm for GPUs. Our experimental evaluation demonstrates the efficacy of our approach by comparing the execution time and number of cases that can be handled against the state-of-the-art GPU implementations.",
        "link": "https://dl.acm.org/doi/10.1145/3458817.3476214",
        "category": "Databases"
    },
    {
        "title": "Implementation of Blockchain for Secured Criminal Records",
        "authors": "['MD. Nashif Iftekhar', 'MD Sajid Bin- Faisal', 'Dip Nandi']",
        "date": "March 2022",
        "source": "ICCA '22: Proceedings of the 2nd International Conference on Computing Advancements",
        "abstract": "Criminal activities are occurring every day in society. People all over the world are either victims, witnessing or criminals. As a citizen of a nation, anyone has the right to fill a case against anyone based on their instant evidence (either an audio, video, or image). Although it should be done perfectly but, in many countries, especially in developing countries local criminals are powerful in the society and thus the victims cannot fill a case due to fear of losing their lives. In the digital era, it is easier to raise the voice against any antisocial activity using digital platforms. Not only this problem is an issue but a way of organized and secured data management system is required to help the police. As it is a huge amount of national data, the main concern in this study is to ensure that no one can hack, steal or manipulate the data in any way. According to Shukla, P., Tyagi, R., & Tyagi, A. the system architecture was different for police officers and general people, yet the system perspective were not narrowed down for further implementation [1]. Along with other technologies Blockchain has been used in order to produce a criminal data record management in other system. The concept of the research is based on the confidentiality of criminal data and maintenance from the local Police Officers perspective. The study aims to contribute to the security protocol of the criminal record data through Blockchain. The work focuses over both the security protocols and the system architecture from both the user and implementers’ side.",
        "link": "https://dl.acm.org/doi/10.1145/3542954.3542987",
        "category": "Databases"
    },
    {
        "title": "Secure Yannakakis: Join-Aggregate Queries over Private Data",
        "authors": "['Yilei Wang', 'Ke Yi']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In this paper, we describe a secure version of the classical Yannakakis algorithm for computing free-connex join-aggregate queries. This protocol can be used in the secure two-party computation model, where the parties would like to evaluate a query without revealing their own data. Our protocol presents a dramatic improvement over the state-of-the-art protocol based on Yao's garbled circuit. In theory, its cost (both running time and communication) is linear in data size and polynomial in query size, whereas that of the garbled circuit is polynomial in data size and exponential in query size. This translates to a reduction in running time in practice from years to minutes, as tested on a number of TPC-H queries of varying complexity.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452808",
        "category": "Databases"
    },
    {
        "title": "Design features and health outcomes of mHealth applications for patient self-management of asthma: a systematic review: mHealth apps for asthma self-management",
        "authors": "['Ting Song', 'Ping Yu', 'Zhenyu Zhang']",
        "date": "February 2022",
        "source": "ACSW '22: Proceedings of the 2022 Australasian Computer Science Week",
        "abstract": "Despite the potential of mobile health (mHealth) apps to provide patients with an effective means to self-manage asthma, to date, little is known about the key design features and health outcomes brought by such apps. Therefore, this study aims to systematically review and examine the research evidence on health outcomes of mHealth apps that support patient self-management of asthma and the key design features to achieve these outcomes. Four databases were searched using the keywords against the eligibility criteria, yielding 446 studies with ten finally included. The thematic analysis identified five key features of the mHealth app – communication, assessment, planning, education and reminder – abbreviated as a CAPER model. Of the nine studies that measured asthma control, eight showed significant positive changes and one remained unchanged. Seven studies measured asthma-related quality of life, seeing three positive changes, three without change, and one only with positive changes in physical measurements but not in mental measurements. Three studies measured asthma medication use with two positive changes and one unchanged. Lung function was measured in two studies while exacerbation, asthma symptoms, asthma education and self-efficacy were measured in only one study and significant positive outcomes only be seen in asthma symptoms and education; the other three remained unchanged. It appears that most mHealth apps for self-management of asthma can improve asthma control, asthma-related quality of life and medication use. Further research is needed to gather sound evidence about the relationship between the key design features and patient outcomes.",
        "link": "https://dl.acm.org/doi/10.1145/3511616.3513110",
        "category": "Databases"
    },
    {
        "title": "Towards a knowledge graph-based approach for context-aware points-of-interest recommendations",
        "authors": "['Lavdim Halilaj', 'Jürgen Lüttin', 'Susanne Rothermel', 'Santhosh Kumar Arumugam', 'Ishan Dindorkar']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Context-aware Recommender Systems (CARS) are becoming an integral part of the everyday life by providing users the ability to retrieve relevant information based on their contextual situation. To increase the predictive power considering many parameters, such as mood, hunger level and user preferences, information from heterogeneous sources should be leveraged. However, these data sources are typically isolated and unexplored and the efforts for integrating them are exacerbated by variety of data structures used for their modelling and costly pre-processing operations. We propose a Knowledge Graph-based approach to allow integration of data according to abstract semantic models for Points-of-Interests (POI)s recommendation scenarios. By enriching data with information about attributes, relationships and their meaning, additional knowledge can be derived from what already exists. We demonstrate the applicability of the proposed approach with a concrete example showing benefits of the retrieving the dispersed data with a unified access mechanism.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3442056",
        "category": "Databases"
    },
    {
        "title": "Bidirectionally Densifying LSH Sketches with Empty Bins",
        "authors": "['Peng Jia', 'Pinghui Wang', 'Junzhou Zhao', 'Shuo Zhang', 'Yiyan Qi', 'Min Hu', 'Chao Deng', 'Xiaohong Guan']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "As an efficient tool for approximate similarity computation and search, Locality Sensitive Hashing (LSH) has been widely used in many research areas including databases, data mining, information retrieval, and machine learning. Classical LSH methods typically require to perform hundreds or even thousands of hashing operations when computing the LSH sketch for each input item (e.g., a set or a vector); however, this complexity is still too expensive and even impractical for applications requiring processing data in real-time. To address this issue, several fast methods such as OPH and BCWS have been proposed to efficiently compute the LSH sketches; however, these methods may generate many sketches with empty bins, which may introduce large errors for similarity estimation and also limit their usage for fast similarity search. To solve this issue, we propose a novel densification method, i.e., BiDens. Compared with existing densification methods, our BiDens is more efficient to fill a sketch's empty bins with values of its non-empty bins in either the forward or backward directions. Furthermore, it also densifies empty bins to satisfy the densification principle (i.e., the LSH property). Theoretical analysis and experimental results on similarity estimation, fast similarity search, and kernel linearization using real-world datasets demonstrate that our BiDens is up to 106 times faster than state-of-the-art methods while achieving the same or even better accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452833",
        "category": "Databases"
    },
    {
        "title": "HedgeCut: Maintaining Randomised Trees for Low-Latency Machine Unlearning",
        "authors": "['Sebastian Schelter', 'Stefan Grafberger', 'Ted Dunning']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Software systems that learn from user data with machine learning (ML) have become ubiquitous over the last years. Recent law such as the \"General Data Protection Regulation\" (GDPR) requires organisations that process personal data to delete user data upon request (enacting the \"right to be forgotten\"). However, this regulation does not only require the deletion of user data from databases, but also applies to ML models that have been learned from the stored data. We therefore argue that ML applications should offer users to unlearn their data from trained models in a timely manner. We explore how fast this unlearning can be done under the constraints imposed by real world deployments, and introduce the problem of low-latency machine unlearning: maintaining a deployed ML model in-place under the removal of a small fraction of training samples without retraining. We propose HedgeCut, a classification model based on an ensemble of randomised decision trees, which is designed to answer unlearning requests with low latency. We detail how to efficiently implement HedgeCut with vectorised operators for decision tree learning. We conduct an experimental evaluation on five privacy-sensitive datasets, where we find that HedgeCut can unlearn training samples with a latency of around 100 microseconds and answers up to 36,000 prediction requests per second, while providing a training time and predictive accuracy similar to widely used implementations of tree-based ML models such as Random Forests.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457239",
        "category": "Databases"
    },
    {
        "title": "Becoming Interdisciplinary: Fostering Critical Engagement With Disaster Data",
        "authors": "['Robert Soden', 'David Lallemant', 'Perrine Hamel', 'Karen Barns']",
        "date": "None",
        "source": "Proceedings of the ACM on Human-Computer Interaction",
        "abstract": "ICTs such as mapping platforms, algorithms, and databases are a central component of how society responds to the threats posed by disasters. However, these systems have come under increasing criticism in recent years for prioritizing technical disciplines over insights from the humanities and social science and failing to adequately incorporate the perspectives of at-risk or affected communities. This paper describes a unique month-long workshop that convened interdisciplinary experts to collaborate on projects related to flood data. In addition to findings about the practical accomplishment of interdisciplinary collaboration, we offer three interrelated contributions. First, we position interdisciplinarity as a critical practice and offer a detailed example of how we staged this process. We then discuss the benefits to interdisciplinarity of expanding the range of temporal logics normally deployed in design workshops. Finally, we reflect on approaches to evaluating the event's contributions toward sustained critique and reform of expert practice.",
        "link": "https://dl.acm.org/doi/10.1145/3449242",
        "category": "Databases"
    },
    {
        "title": "PTHash: Revisiting FCH Minimal Perfect Hashing",
        "authors": "['Giulio Ermanno Pibiri', 'Roberto Trani']",
        "date": "July 2021",
        "source": "SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "abstract": "Given a set S of n distinct keys, a function f that bijectively maps the keys of S into the range (0,...,n-1) is called a minimal perfect hash function for S. Algorithms that find such functions when n is large and retain constant evaluation time are of practical interest; for instance, search engines and databases typically use minimal perfect hash functions to quickly assign identifiers to static sets of variable-length keys such as strings. The challenge is to design an algorithm which is efficient in three different aspects: time to find f (construction time), time to evaluate f on a key of S (lookup time), and space of representation for f. Several algorithms have been proposed to trade-off between these aspects. In 1992, Fox, Chen, and Heath (FCH) presented an algorithm at SIGIR providing very fast lookup evaluation. However, the approach received little attention because of its large construction time and higher space consumption compared to other subsequent techniques. Almost thirty years later we revisit their framework and present an improved algorithm that scales well to large sets and reduces space consumption altogether, without compromising the lookup time. We conduct an extensive experimental assessment and show that the algorithm finds functions that are competitive in space with state-of-the art techniques and provide 2-4x better lookup time.",
        "link": "https://dl.acm.org/doi/10.1145/3404835.3462849",
        "category": "Databases"
    },
    {
        "title": "PTHash: Revisiting FCH Minimal Perfect Hashing",
        "authors": "['Giulio Ermanno Pibiri', 'Roberto Trani']",
        "date": "July 2021",
        "source": "SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "abstract": "Given a set S of n distinct keys, a function f that bijectively maps the keys of S into the range (0,...,n-1) is called a minimal perfect hash function for S. Algorithms that find such functions when n is large and retain constant evaluation time are of practical interest; for instance, search engines and databases typically use minimal perfect hash functions to quickly assign identifiers to static sets of variable-length keys such as strings. The challenge is to design an algorithm which is efficient in three different aspects: time to find f (construction time), time to evaluate f on a key of S (lookup time), and space of representation for f. Several algorithms have been proposed to trade-off between these aspects. In 1992, Fox, Chen, and Heath (FCH) presented an algorithm at SIGIR providing very fast lookup evaluation. However, the approach received little attention because of its large construction time and higher space consumption compared to other subsequent techniques. Almost thirty years later we revisit their framework and present an improved algorithm that scales well to large sets and reduces space consumption altogether, without compromising the lookup time. We conduct an extensive experimental assessment and show that the algorithm finds functions that are competitive in space with state-of-the art techniques and provide 2-4x better lookup time.",
        "link": "https://dl.acm.org/doi/10.1145/3404835.3462849",
        "category": "Databases"
    },
    {
        "title": "SI3DP: Source Identification Challenges and Benchmark for Consumer-Level 3D Printer Forensics",
        "authors": "['Bo Seok Shim', 'Yoo Seung Shin', 'Seong Wook Park', 'Jong-Uk Hou']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "This paper lays the foundation for a new 3D content market by establishing a content security framework using databases and benchmarks for in-depth research on source identification of 3D printed objects. The proposed benchmark, SI3DP dataset, offers a more generalized multimedia forensic technique. Assuming that identifying the source of a 3D printing object can arise from various invisible traces occurring in the printing process, we obtain close-up images, full object images from 252 printed objects from 18 different printing setups. We then propose a benchmark with five challenging tasks such as device-level identification and scan-and-reprint detection using the provided dataset. Our baseline shows that the printer type and its attributes can be identified based on the microscopic difference of surface texture. Contrary to the conventional belief that only microscopic views such as close-up images are useful to identify printer model, we also achieved a certain level of performance even at a relatively macroscopic point of view. We then propose a multitask-multimodal architecture for device-level identification task to exploit rich knowledge from different image modality and task. The SI3DP dataset can promote future in-depth research studies related to digital forensics and intellectual property protection.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475316",
        "category": "Databases"
    },
    {
        "title": "Reimagining Account and Allocation Management for Advanced Research Computing",
        "authors": "['Adam Howard', 'Tabitha Samuel']",
        "date": "July 2021",
        "source": "PEARC '21: Practice and Experience in Advanced Research Computing",
        "abstract": "Though they are the backbone of a center’s infrastructure and store some of its most vital information, often times, the databases responsible for tracking project allocations, job accounting, storage commitments, and other core center information aren’t designed with the proper attention and thought to scale and adapt with the center as it grows and changes. Off-the-shelf solutions for these data stores are limited, and come with their own assumptions. As the field of research computing and thus the demands on the center continue to evolve, so to must the accounting solutions be able to accommodate the changing mission and goals. Many centers develop their own solutions in house, which requires additional effort to build and maintain. In this work, we present a data model which attempts to provide a more generalized solution to this problem, as well as a feature-rich set of tools provided by leveraging the powerful and widely-used Django web framework and ORM, which we call OpenAcct. We explore other solutions to this data management problem including both vendor-provided and in-house developed options. We then discuss the details of the data model, before demonstrating the capabilities of the application layer. Finally, future plans for the project are discussed.",
        "link": "https://dl.acm.org/doi/10.1145/3437359.3465572",
        "category": "Databases"
    },
    {
        "title": "Effective and scalable clustering of SARS-CoV-2 sequences",
        "authors": "['Sarwan Ali', 'Tamkanat E Ali', 'Muhammad Asad Khan', 'Imdadullah Khan', 'Murray Patterson']",
        "date": "September 2021",
        "source": "ICBDR '21: Proceedings of the 5th International Conference on Big Data Research",
        "abstract": "SARS-CoV-2, like any other virus, continues to mutate as it spreads, according to an evolutionary process. Unlike any other virus, the number of currently available sequences of SARS-CoV-2 in public databases such as GISAID is already several million. This amount of data has the potential to uncover the evolutionary dynamics of a virus like never before. However, a million is already several orders of magnitude beyond what can be processed by the traditional methods designed to reconstruct a virus’s evolutionary history, such as those that build a phylogenetic tree. Hence, new and scalable methods will need to be devised in order to make use of the ever increasing number of viral sequences being collected.  Since identifying variants is an important part of understanding the evolution of a virus, in this paper, we propose an approach based on clustering sequences to identify the current major SARS-CoV-2 variants. Using a k-mer based feature vector generation and efficient feature selection methods, our approach is effective in identifying variants, as well as being efficient and scalable to millions of sequences. Such a clustering method allows us to show the relative proportion of each variant over time, giving the rate of spread of each variant in different locations — something which is important for vaccine development and distribution. We also compute the importance of each amino acid position of the spike protein in identifying a given variant in terms of information gain. Positions of high variant-specific importance tend to agree with those reported by the USA’s Centers for Disease Control and Prevention (CDC), further demonstrating our approach.",
        "link": "https://dl.acm.org/doi/10.1145/3505745.3505752",
        "category": "Databases"
    },
    {
        "title": "Vulnerability detection with fine-grained interpretations",
        "authors": "['Yi Li', 'Shaohua Wang', 'Tien N. Nguyen']",
        "date": "August 2021",
        "source": "ESEC/FSE 2021: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "abstract": "Despite the successes of machine learning (ML) and deep learning (DL)-based vulnerability detectors (VD), they are limited to providing only the decision on whether a given code is vulnerable or not, without details on what part of the code is relevant to the detected vulnerability. We present IVDetect, an interpretable vulnerability detector with the philosophy of using Artificial Intelligence (AI) to detect vulnerabilities, while using Intelligence Assistant (IA) to provide VD interpretations in terms of vulnerable statements.   For vulnerability detection, we separately consider the vulnerable statements and their surrounding contexts via data and control dependencies. This allows our model better discriminate vulnerable statements than using the mixture of vulnerable code and contextual code as in existing approaches. In addition to the coarse-grained vulnerability detection result, we leverage interpretable AI to provide users with fine-grained interpretations that include the sub-graph in the Program Dependency Graph (PDG) with the crucial statements that are relevant to the detected vulnerability. Our empirical evaluation on vulnerability databases shows that IVDetect outperforms the existing DL-based approaches by 43%–84% and 105%–255% in top-10 nDCG and MAP ranking scores. IVDetect correctly points out the vulnerable statements relevant to the vulnerability via its interpretation in 67% of the cases with a top-5 ranked list. IVDetect improves over the baseline interpretation models by 12.3%–400% and 9%–400% in accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3468264.3468597",
        "category": "Databases"
    },
    {
        "title": "Releasing Locks As Early As You Can: Reducing Contention of Hotspots by Violating Two-Phase Locking",
        "authors": "['Zhihan Guo', 'Kan Wu', 'Cong Yan', 'Xiangyao Yu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Hotspots, a small set of tuples frequently read/written by a large number of transactions, cause contention in a concurrency control protocol. While a hotspot may comprise only a small fraction of a transaction's execution time, conventional strict two-phase locking allows a transaction to release lock only after the transaction completes, which leaves significant parallelism unexploited. Ideally, a concurrency control protocol serializes transactions only for the duration of the hotspots, rather than the duration of transactions. We observe that exploiting such parallelism requires violating two-phase locking. In this paper, we propose Bamboo, a new concurrency control protocol that can enable such parallelism by modifying the conventional two-phase locking, while maintaining the same guarantees in correctness. We thoroughly analyzed the effect of cascading aborts involved in reading uncommitted data and discussed optimizations that can be applied to further improve the performance. Our evaluation on TPC-C shows a performance improvement up to 4x compared to the best of pessimistic and optimistic baseline protocols. On synthetic workloads that contain a single hotspot, Bamboo achieves a speedup up to 19x over baselines.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457294",
        "category": "Databases"
    },
    {
        "title": "Ember: no-code context enrichment via similarity-based keyless joins",
        "authors": "['Sahaana Suri', 'Ihab F. Ilyas', 'Christopher Ré', 'Theodoros Rekatsinas']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Structured data, or data that adheres to a pre-defined schema, can suffer from fragmented context: information describing a single entity can be scattered across multiple datasets or tables tailored for specific business needs, with no explicit linking keys. Context enrichment, or rebuilding fragmented context, using keyless joins is an implicit or explicit step in machine learning (ML) pipelines over structured data sources. This process is tedious, domain-specific, and lacks support in now-prevalent no-code ML systems that let users create ML pipelines using just input data and high-level configuration files. In response, we propose Ember, a system that abstracts and automates keyless joins to generalize context enrichment. Our key insight is that Ember can enable a general keyless join operator by constructing an index populated with task-specific embeddings. Ember learns these embeddings by leveraging Transformer-based representation learning techniques. We describe our architectural principles and operators when developing Ember, and empirically demonstrate that Ember allows users to develop no-code context enrichment pipelines for five domains, including search, recommendation and question answering, and can exceed alternatives by up to 39% recall, with as little as a single line configuration change.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494149",
        "category": "Databases"
    },
    {
        "title": "ARM-Net: Adaptive Relation Modeling Network for Structured Data",
        "authors": "['Shaofeng Cai', 'Kaiping Zheng', 'Gang Chen', 'H. V. Jagadish', 'Beng Chin Ooi', 'Meihui Zhang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Relational databases are the de facto standard for storing and querying structured data, and extracting insights from structured data requires advanced analytics. Deep neural networks (DNNs) have achieved super-human prediction performance in particular data types, e.g., images. However, existing DNNs may not produce meaningful results when applied to structured data. The reason is that there are correlations and dependencies across combinations of attribute values in a table, and these do not follow simple additive patterns that can be easily mimicked by a DNN. The number of possible such cross features is combinatorial, making them computationally prohibitive to model. Furthermore, the deployment of learning models in real-world applications has also highlighted the need for interpretability, especially for high-stakes applications, which remains another issue of concern to DNNs. In this paper, we present ARM-Net, an adaptive relation modeling network tailored for structured data, and a lightweight framework ARMOR based on ARM-Net for relational data analytics. The key idea is to model feature interactions with cross features selectively and dynamically, by first transforming the input features into exponential space, and then determining the interaction order and interaction weights adaptively for each cross feature. We propose a novel sparse attention mechanism to dynamically generate the interaction weights given the input tuple, so that we can explicitly model cross features of arbitrary orders with noisy features filtered selectively. Then during model inference, ARM-Net can specify the cross features being used for each prediction for higher accuracy and better interpretability. Our extensive experiments on real-world datasets demonstrate that ARM-Net consistently outperforms existing models and provides more interpretable predictions for data-driven decision making.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457321",
        "category": "Databases"
    },
    {
        "title": "Prediction of Drug Permeability to the Blood-Brain Barrier using Deep Learning",
        "authors": "['Abena Achiaa Atwereboannah', 'Wei-Ping Wu', 'Ebenezer Nanor']",
        "date": "May 2021",
        "source": "ICBEA '21: 4th International Conference on Biometric Engineering and Applications",
        "abstract": "In our quest to design new drugs, conventional means of designing drugs characterized by experiments is a time and resource demanding process. It involves exploring many molecular combinations, to find potential drugs to target indications. Machine Learning (ML) techniques upon their fantastic achievement in many application domains such as Computer vision and Natural Language Processing etc., are recently, being utilized in the research of drugs. With the surge in compound databases, ML and Deep Learning (DL) have shown great promise in various fields of drug discovery including pharmaceutics, biotechnology and physical chemistry. It is not incredible that these techniques are being incorporated in BBB studies. Predicting the permeability of drug compounds to the Blood-Brain Barrier (BBB) is indispensable for Central Nervous System (CNS) drug discovery, thus this work leverages two DL models in distinguishing between drugs that are BBB permeable and those that are not. ​The first architecture is a Fully-Connected Neural Network (FCNN) and the second is based on Convolutional Neural Network (CNN). Both algorithms were evaluated using two distinct CNS drug datasets. Our DL models generalize well, on both datasets and achieve competitive advantage over other ML and in silico techniques used in drug Blood–Brain Barrier Permeability (BBBP) prediction studies. The best performing model was the FCNN achieving AUROC of 99.5% on the first benchmark dataset.",
        "link": "https://dl.acm.org/doi/10.1145/3476779.3476797",
        "category": "Databases"
    },
    {
        "title": "Computing Views of OWL Ontologies for the Semantic Web",
        "authors": "['Jiaqi Li', 'Xuan Wu', 'Chang Lu', 'Wenxing Deng', 'Yizheng Zhao']",
        "date": "April 2021",
        "source": "WWW '21: Proceedings of the Web Conference 2021",
        "abstract": "This paper tackles the problem of computing views of OWL ontologies using a forgetting-based approach. In traditional relational databases, a view is a subset of a database, whereas in ontologies, a view is more than a subset; it contains not only axioms contained in the original ontology, but may also contain newly-derived axioms entailed by the original ontology (implicitly contained in the original ontology). Specifically, given an ontology , the signature of is the set of all the names in , and a view of is a new ontology obtained from using only part of ’s signature, namely the target signature, while preserving all logical entailments up to the target signature. Computing views of OWL ontologies is useful for Semantic Web applications such as ontology-based query answering, in a way that the view can be used as a substitute of the original ontology to answer queries formulated with the target signature, and information hiding, in the sense that it restricts users from viewing certain information of an ontology.  Forgetting is a form of non-standard reasoning concerned with eliminating from an ontology a subset of its signature, namely the forgetting signature, in such a way that all logical entailments are preserved up to the target signature. Forgetting can thus be used as a means for computing views of OWL ontologies — the solution of forgetting a set of names from an ontology is the view of for the target signature .  In this paper, we present a forgetting-based method for computing views of OWL ontologies specified in the description logic , the basic extended with role hierarchy, nominals and inverse roles. The method is terminating and sound. Despite the method not being complete, an evaluation with a prototype implementation of the method on a corpus of real-world ontologies has shown very good success rates. This is very useful from the perspective of the Semantic Web, as it provides knowledge engineers with a powerful tool for creating views of OWL ontologies.",
        "link": "https://dl.acm.org/doi/10.1145/3442381.3449881",
        "category": "Databases"
    },
    {
        "title": "Fast and Accurate Optimizer for Query Processing over Knowledge Graphs",
        "authors": "['Jingqi Wu', 'Rong Chen', 'Yubin Xia']",
        "date": "November 2021",
        "source": "SoCC '21: Proceedings of the ACM Symposium on Cloud Computing",
        "abstract": "This paper presents Gpl, a fast and accurate optimizer for query processing over knowledge graphs. Gpl is novel in three ways. First, Gpl proposes a type-centric approach to enhance the accuracy of cardinality estimation prominently, which naturally embeds the correlation of multiple query conditions into the existing type system of knowledge graphs. Second, to predict execution time accurately, Gpl constructs a specialized cost model for graph exploration scheme and tunes the coefficients with target hardware platform and graph data. Third, Gpl further uses a budget-aware strategy for plan enumeration with a greedy heuristic to boost the overall performance (i.e., optimization time and execution time) for various workloads. Evaluations with representative knowledge graphs and query benchmarks show that Gpl can select optimal plans for 33 of 39 queries and only incurs less than 5% slowdown on average compared to optimal results. In contrast, the state-of-the-art optimizer and manually tuned results will cause 100% and 36% slowdown, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3472883.3486991",
        "category": "Databases"
    },
    {
        "title": "MERANet: facial micro-expression recognition using 3D residual attention network",
        "authors": "['Viswanatha Reddy Gajjala', 'Sai Prasanna Teja Reddy', 'Snehasis Mukherjee', 'Shiv Ram Dubey']",
        "date": "December 2021",
        "source": "ICVGIP '21: Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing",
        "abstract": "Micro-expression has emerged as a promising modality in affective computing due to its high objectivity in emotion detection. Despite the higher recognition accuracy provided by the deep learning models, there are still significant scope for improvements in micro-expression recognition techniques. The presence of micro-expressions in small-local regions of the face, as well as the limited size of available databases, continue to limit the accuracy in recognizing micro-expressions. In this work, we propose a facial micro-expression recognition model using 3D residual attention network named MERANet to tackle such challenges. The proposed model takes advantage of spatial-temporal attention and channel attention together, to learn deeper fine-grained subtle features for classification of emotions. Further, the proposed model encompasses both spatial and temporal information simultaneously using the 3D kernels and residual connections. Moreover, the channel features and spatio-temporal features are re-calibrated using the channel and spatio-temporal attentions, respectively in each residual module. Our attention mechanism enables the model to learn to focus on different facial areas of interest. The experiments are conducted on benchmark facial micro-expression datasets. A superior performance is observed as compared to the state-of-the-art for facial micro-expression recognition on benchmark data.",
        "link": "https://dl.acm.org/doi/10.1145/3490035.3490260",
        "category": "Databases"
    },
    {
        "title": "Phytochemicals as potential inhibitors for novel coronavirus 2019-nCoV/SARS-CoV-2: a graph-based computational analysis",
        "authors": "['Monalisa Mandal']",
        "date": "June 2021",
        "source": "IAIT '21: Proceedings of the 12th International Conference on Advances in Information Technology",
        "abstract": "Corona viruses (CoVs) are a group of infectious viruses that causes the regular cold to more extreme illnesses like Middle East Respiratory Syndrome (MERS-CoV), Severe Acute Respiratory Syndrome (SARS-CoV) and epic Covid (nCoV) is another strain that has been recently recognized in people. The formulation of effective drugs and treatment strategies are desperately required for 2019-nCoV/SARS-CoV-2 outbreak. Reducing the clinical trial period of existing as well as new drugs, the phytochemicals present in natural products would be helpful to get a quick treatment solution for this pandemic. Here, computationally some of the effective phytochemicals are identified for treating Covid. Publicly available databases have been used for collecting the phytochemicals and their associated genes that also interact with Corona viruses. Then a bipartite graph has been built with two sets of inputs; one set is the set of phytochemicals and the second set is the set of viruses. Thereafter, the eigen vector centrality which is the measure of most influential node in a graph has been calculated for each phytochemical. We found four such phytochemicals which have the top four eigen vector score. Then again, all possible cliques from the bipartite graph have been calculated and it has been seen that the same top four phytochemicals are present in almost all the bicliques. Finally, these top four phytochemicals have been investigated for their molecular and drug likeliness properties. Also the ADMET profile of the top phytochemicals are explored and analyzed.",
        "link": "https://dl.acm.org/doi/10.1145/3468784.3468886",
        "category": "Databases"
    },
    {
        "title": "The IoMT and Cloud in Healthcare: Use, Impact and Efficiency of Contemporary Sensor Devices Used by Patients and Clinicians",
        "authors": "['Tonmoy Roy', 'Md.Mehzabul Nahid']",
        "date": "March 2022",
        "source": "ICCA '22: Proceedings of the 2nd International Conference on Computing Advancements",
        "abstract": "The aim of this research is to undertake a systematic review of the literature on cloud-based Internet of Medical Things (IoMT) in healthcare, to summarise the examined contexts and research focuses, to identify gaps in the literature, and to recommend new directions for future research. The authors searched electronic databases such as Scopus, Elsevier, ACM library, IEEE Xplore, Emerald, and ScienceDirect for articles relating to (IoMT) and cloud technologies, as well as did manual journal searches in Google Scholar, PMC and ResearchGate. A total of 442 papers were examined using a combination of quantitative and qualitative approaches. A systematic mapping study was undertaken as part of the qualitative investigation. The study identified and classified contemporary IoMT devices and applications used in healthcare that have evolved over time, including remote monitoring apps, diagnostic tool apps, personal wellness and healthy living apps, consolidated healthcare apps, medication adherence apps, appointment scheduling and reminder apps, and various types of critical decision-making systems applications. Additionally, potential themes for future study were identified, including the impact of cloud-based IoMT devices and the obstacles associated with deploying IoMT in healthcare. The review focuses on how patients and clinicians apply IoMT. Further study may want to examine the adoption, effectiveness, and usability of cloud based IoMT in healthcare from the perspective of various stakeholders, such as families, caregivers, healthcare institutions, researchers, policy actors, payors, and buyers.",
        "link": "https://dl.acm.org/doi/10.1145/3542954.3543015",
        "category": "Databases"
    },
    {
        "title": "Robust voice querying with MUVE: optimally visualizing results of phonetically similar queries",
        "authors": "['Ziyun Wei', 'Immanuel Trummer', 'Connor Anderson']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Recently proposed voice query interfaces translate voice input into SQL queries. Unreliable speech recognition on top of the intrinsic challenges of text-to-SQL translation makes it hard to reliably interpret user input. We present MUVE (Multiplots for Voice quEries), a system for robust voice querying. MUVE reduces the impact of ambiguous voice queries by filling the screen with multiplots, capturing results of phonetically similar queries. It maps voice input to a probability distribution over query candidates, executes a selected subset of queries, and visualizes their results in a multiplot.Our goal is to maximize probability to show the correct query result. Also, we want to optimize the visualization (e.g., by coloring a subset of likely results) in order to minimize expected time until users find the correct result. Via a user study, we validate a simple cost model estimating the latter overhead. The resulting optimization problem is NP-hard. We propose an exhaustive algorithm, based on integer programming, as well as a greedy heuristic. As shown in a corresponding user study, MUVE enables users to identify accurate results faster, compared to prior work.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476289",
        "category": "Databases"
    },
    {
        "title": "Eth2Vec: Learning Contract-Wide Code Representations for Vulnerability Detection on Ethereum Smart Contracts",
        "authors": "['Nami Ashizawa', 'Naoto Yanai', 'Jason Paul Cruz', 'Shingo Okamura']",
        "date": "May 2021",
        "source": "BSCI '21: Proceedings of the 3rd ACM International Symposium on Blockchain and Secure Critical Infrastructure",
        "abstract": "Ethereum smart contracts are programs that run on the Ethereum blockchain, and many smart contract vulnerabilities have been discovered in the past decade. Many security analysis tools have been created to detect such vulnerabilities, but their performance decreases drastically when codes to be analyzed are being rewritten. In this paper, we propose Eth2Vec, a machine-learning-based static analysis tool for vulnerability detection in smart contracts. It is also robust against code rewrites, i.e., it can detect vulnerabilities even in rewritten codes. Existing machine-learning-based static analysis tools for vulnerability detection need features, which analysts create manually, as inputs. In contrast, Eth2Vec automatically learns features of vulnerable Ethereum Virtual Machine (EVM) bytecodes with tacit knowledge through a neural network for natural language processing. Therefore, Eth2Vec can detect vulnerabilities in smart contracts by comparing the code similarity between target EVM bytecodes and the EVM bytecodes it already learned. We conducted experiments with existing open databases, such as Etherscan, and our results show that Eth2Vec outperforms a recent model based on support vector machine in terms of well-known metrics, i.e., precision, recall, and F1-score.",
        "link": "https://dl.acm.org/doi/10.1145/3457337.3457841",
        "category": "Databases"
    },
    {
        "title": "Optimizing One-time and Continuous Subgraph Queries using Worst-case Optimal Joins",
        "authors": "['Amine Mhedhbi', 'Chathura Kankanamge', 'Semih Salihoglu']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "We study the problem of optimizing one-time and continuous subgraph queries using the new worst-case optimal join plans. Worst-case optimal plans evaluate queries by matching one query vertex at a time using multiway intersections. The core problem in optimizing worst-case optimal plans is to pick an ordering of the query vertices to match. We make two main contributions:1. A cost-based dynamic programming optimizer for one-time queries that (i) picks efficient query vertex orderings for worst-case optimal plans and (ii) generates hybrid plans that mix traditional binary joins with worst-case optimal style multiway intersections. In addition to our optimizer, we describe an adaptive technique that changes the query vertex orderings of the worst-case optimal subplans during query execution for more efficient query evaluation. The plan space of our one-time optimizer contains plans that are not in the plan spaces based on tree decompositions from prior work.2. A cost-based greedy optimizer for continuous queries that builds on the delta subgraph query framework. Given a set of continuous queries, our optimizer decomposes these queries into multiple delta subgraph queries, picks a plan for each delta query, and generates a single combined plan that evaluates all of the queries. Our combined plans share computations across operators of the plans for the delta queries if the operators perform the same intersections. To increase the amount of computation shared, we describe an additional optimization that shares partial intersections across operators.Our optimizers use a new cost metric for worst-case optimal plans called intersection-cost. When generating hybrid plans, our dynamic programming optimizer for one-time queries combines intersection-cost with the cost of binary joins. We demonstrate the effectiveness of our plans, adaptive technique, and partial intersection sharing optimization through extensive experiments. Our optimizers are integrated into GraphflowDB.",
        "link": "https://dl.acm.org/doi/10.1145/3446980",
        "category": "Databases"
    },
    {
        "title": "Objective Object Segmentation Visual Quality Evaluation: Quality Measure and Pooling Method",
        "authors": "['Ran Shi', 'Jing Ma', 'King Ngi Ngan', 'Jian Xiong', 'Tong Qiao']",
        "date": "None",
        "source": "ACM Transactions on Multimedia Computing, Communications, and Applications",
        "abstract": "Objective object segmentation visual quality evaluation is an emergent member of the visual quality assessment family. It aims to develop an objective measure instead of a subjective survey to evaluate the object segmentation quality in agreement with human visual perception. It is an important benchmark for assessing and comparing the performances of object segmentation methods in terms of visual quality. Despite its essential role, sufficient study compared with other visual quality evaluation studies is still lacking. In this article, we propose a novel full-reference objective measure that includes a two-level single object segmentation visual quality measure and a pooling method for multiple object segmentation overall visual quality. The single object segmentation visual quality measure combines a pixel-level sub-measure and a region-level sub-measure for evaluating the similarity of area, shape, and object completeness between the segmentation result and the ground truth in terms of human visual perception. For the proposed multiple object segmentation overall visual quality pooling method, the rank of each object’s segmentation quality as a novel factor is integrated into the weighted harmonic mean to evaluate the overall quality. To evaluate the performance of our proposed measure, we tested it on an object segmentation subjective visual quality assessment database. The experimental results demonstrate that our proposed two-level measure and pooling method with good robustness perform better in matching subjective assessments compared with other state-of-the-art objective measures.",
        "link": "https://dl.acm.org/doi/10.1145/3491229",
        "category": "Databases"
    },
    {
        "title": "Towards Benchmarking Feature Type Inference for AutoML Platforms",
        "authors": "['Vraj Shah', 'Jonathan Lacanlale', 'Premanand Kumar', 'Kevin Yang', 'Arun Kumar']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The paradigm of AutoML has created an opportunity to enable ML for the masses. Emerging industrial-scale cloud AutoML platforms aim to automate the end-to-end ML workflow. While many works have looked into automated feature engineering, model selection, or hyper-parameter search in AutoML, little work has studied a crucial step that serves as an entry point to this workflow: ML feature type inference. The semantic gap between attribute types (e.g., strings, numbers) in databases/files and ML feature types (e.g., Numeric, Categorical) necessitates type inference. In this work, we formalize and standardize this task by creating the first ever benchmark labeled dataset, which we use to objectively evaluate existing AutoML tools. Our dataset has 9921 examples and a 9-class label vocabulary. Our labeled data also offers an alternative approach to automate this task than existing rule-based or syntax-based approaches: use ML itself to predict feature types. We collate a benchmark suite of 30 classification and regression tasks to assess the importance of type inference for downstream models. Empirical comparison on our labeled data shows that an ML-based approach delivers a lift of an average 14% and up to 38% in accuracy for identifying feature types compared to prominent industrial tools. Our downstream benchmark suite reveals that the ML-based approach outperforms existing industrial-strength tools for 47 out of 60 downstream models. We release our labeled dataset, models, and downstream benchmarks in a public repository with a leaderboard.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457274",
        "category": "Databases"
    },
    {
        "title": "Deterministic Constant-Amortized-RMR Abortable Mutex for CC and DSM",
        "authors": "['Prasad Jayanti', 'Siddhartha Jayanti']",
        "date": "None",
        "source": "ACM Transactions on Parallel Computing",
        "abstract": "The abortable mutual exclusion problem, proposed by Scott and Scherer in response to the needs in real-time systems and databases, is a variant of mutual exclusion that allows processes to abort from their attempt to acquire the lock. Worst-case constant remote memory reference algorithms for mutual exclusion using hardware instructions such as Fetch&Add or Fetch&Store have long existed for both cache coherent (CC) and distributed shared memory multiprocessors, but no such algorithms are known for abortable mutual exclusion. Even relaxing the worst-case requirement to amortized, algorithms are only known for the CC model.In this article, we improve this state of the art by designing a deterministic algorithm that uses Fetch&Store to achieve amortized O(1) remote memory reference in both the CC and distributed shared memory models. Our algorithm supports Fast Abort (a process aborts within six steps of receiving the abort signal) and has the following additional desirable properties: it supports an arbitrary number of processes of arbitrary names, requires only O(1) space per process, and satisfies a novel fairness condition that we call Airline FCFS. Our algorithm is short with fewer than a dozen lines of code.",
        "link": "https://dl.acm.org/doi/10.1145/3490559",
        "category": "Databases"
    },
    {
        "title": "A Semantic Data Lake Model for Analytic Query-Driven Discovery",
        "authors": "['Claudia Diamantini', 'Domenico Potena', 'Emanuele Storti']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "Data Lake (DL) architectures have recently emerged as an effective solution to the problem of data analytics with big, highly heterogeneous, and quickly changing data sources. However, novel challenges arise too, including how to make sense of disparate raw data and how to identify the sources that satisfy a data need. In the paper, we introduce a semantic model for a Data Lake aimed to support data discovery and integration in data analytics scenarios. By formally modeling indicators of interest, their computation formulas, and dimensions of analysis in a knowledge graph, and by seamlessly mapping them to relevant source metadata, the framework is suited for identifying the sources and the required transformation steps according to the analytical request.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487783",
        "category": "Databases"
    },
    {
        "title": "Information Hiding in Cyber Physical Systems: Challenges for Embedding, Retrieval and Detection using Sensor Data of the SWAT Dataset",
        "authors": "['Kevin Lamshöft', 'Tom Neubert', 'Christian Krätzer', 'Claus Vielhauer', 'Jana Dittmann']",
        "date": "June 2021",
        "source": "IH&amp;MMSec '21: Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security",
        "abstract": "In this paper, we present an Information Hiding approach that would be suitable for exfiltrating sensible information of Industrial Control Systems (ICS) by leveraging the long-term storage of process data in historian databases. We show how hidden messages can be embedded in sensor measurements as well as retrieved asynchronously by accessing the historian. We evaluate this approach at the example of water-flow and water-level sensors of the Secure Water Treatment (SWAT) dataset from iTrust. To generalize from specific cover channels (sensors and their transmitted data), we reflect upon general challenges that arise in such Information Hiding scenarios creating network covert channels and discuss aspects of cover channel selection and and sender receiver synchronisation as well as temporal aspects such as the potential persistence of hidden messages in Cyber Physical Systems (CPS). For an empirical evaluation we design and implement a covert channel that makes use of different embedding strategies to perform an adaptive approach in regards to the noise in sensor measurements, resulting in dynamic capacity and bandwidth selection to reduce detection probability. The results of this evaluation show that, using such methods, the exfiltration of sensible information in long-term scaled attacks would indeed be possible. Additionally, we present two detection approaches for the introduced hidden channel and carry out an extensive evaluation of our detectors with multiple test data sets and different parameters. We determine a detection accuracy of up to 87.8% on test data at a false positive rate (FPR) of 0%.",
        "link": "https://dl.acm.org/doi/10.1145/3437880.3460413",
        "category": "Databases"
    },
    {
        "title": "TreeToaster: Towards an IVM-Optimized Compiler",
        "authors": "['Darshana Balakrishnan', 'Carl Nuessle', 'Oliver Kennedy', 'Lukasz Ziarek']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "A compiler's optimizer operates over abstract syntax trees (ASTs), continuously applying rewrite rules to replace subtrees of the AST with more efficient ones. Especially on large source repositories, even simply finding opportunities for a rewrite can be expensive, as optimizer traverses the AST naively. In this paper, we leverage the need to repeatedly find rewrites, and explore options for making the search faster through indexing and incremental view maintenance (IVM). Concretely, we consider bolt-on approaches that make use of embedded IVM systems like DBToaster, as well as two new approaches: Label-indexing and TreeToaster, an AST-specialized form of IVM. We integrate these approaches into an existing just-in-time data structure compiler and show experimentally that TreeToaster can significantly improve performance with minimal memory overheads.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3459244",
        "category": "Databases"
    },
    {
        "title": "SparkCruise: workload optimization in managed spark clusters at Microsoft",
        "authors": "['Abhishek Roy', 'Alekh Jindal', 'Priyanka Gomatam', 'Xiating Ouyang', 'Ashit Gosalia', 'Nishkam Ravi', 'Swinky Mann', 'Prakhar Jain']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Today cloud companies offer fully managed Spark services. This has made it easy to onboard new customers but has also increased the volume of users and their workload sizes. However, both cloud providers and users lack the tools and time to optimize these massive workloads. To solve this problem, we designed SparkCruise that can help understand and optimize workload instances by adding a workload-driven feedback loop to the Spark query optimizer. In this paper, we present our approach to collecting and representing Spark query workloads and use it to improve the overall performance on the workload, all without requiring any access to user data. These methods scale with the number of workloads and apply learned feedback in an online fashion. We explain one specific workload optimization developed for computation reuse. We also share the detailed analysis of production Spark workloads and contrast them with the corresponding analysis of TPC-DS benchmark. To the best of our knowledge, this is the first study to share the analysis of large-scale production Spark SQL workloads.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476388",
        "category": "Databases"
    },
    {
        "title": "Mapping Organizational Tensions Using KIPO in Federated Information Systems: A Case Study in a Brazilian Bank",
        "authors": "['Nadja Antonio', 'Roberto Dias', 'Paulo Malcher', 'Flávio Horita', 'Rodrigo Santos']",
        "date": "June 2021",
        "source": "SBSI '21: Proceedings of the XVII Brazilian Symposium on Information Systems",
        "abstract": "The management of several information systems (IS) faces challenges with the number of existing, autonomous, distributed, and heterogeneous databases. By the need for an integrated access, these arrangements have been explored as federated IS (FIS). FIS are affected by several situations that generate tensions in their management. In order to discover these organizational tensions, a qualitative case study in a real context - bank credit - based on interviews was conducted with stakeholders of a real FIS. Next, we performed the scenario analysis with the data from the interviews and used the GUT Matrix tool to analyze and identify the tensions. Finally, a decision map to analyze and identify the tensions and an ontology, based on the Knowledge Intensive Process Ontology (KIPO), were used to map the tensions in the management of bank credit SIF. The results show that both the decision map and the developed ontology can be used as a practical tool to support practitioners in industry in the FIS management. As a contribution to the FIS area, this work presents another approach to support IS management focused on conceptual modeling and knowledge intensive process decisions. In addition, the developed ontology also allows the description of tacit knowledge to support FIS managers in the investigated context.",
        "link": "https://dl.acm.org/doi/10.1145/3466933.3466957",
        "category": "Databases"
    },
    {
        "title": "An experimental evaluation and investigation of waves of misery in r-trees",
        "authors": "['Lu Xing', 'Eric Lee', 'Tong An', 'Bo-Cheng Chu', 'Ahmed Mahmood', 'Ahmed M. Aly', 'Jianguo Wang', 'Walid G. Aref']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Waves of misery is a phenomenon where spikes of many node splits occur over short periods of time in tree indexes. Waves of misery negatively affect the performance of tree indexes in insertion-heavy workloads. Waves of misery have been first observed in the context of the B-tree, where these waves cause unpredictable index performance. In particular, the performance of search and index-update operations deteriorate when a wave of misery takes place, but is more predictable between the waves. This paper investigates the presence or lack of waves of misery in several R-tree variants, and studies the extent of which these waves impact the performance of each variant. Interestingly, although having poorer query performance, the Linear and Quadratic R-trees are found to be more resilient to waves of misery than both the Hilbert and R*-trees. This paper presents several techniques to reduce the impact in performance of the waves of misery for the Hilbert and R*-trees. One way to eliminate waves of misery is to force node splits to take place at regular times before nodes become full to achieve deterministic performance. The other way is that upon splitting a node, do not split it evenly but rather at different node utilization factors. This allows leaf nodes not to fill at the same pace. We study the impact of two new techniques to mitigate waves of misery after the tree index has been constructed, namely Regular Elective Splits (RES, for short) and Unequal Random Splits (URS, for short). Our experimental investigation highlights the trade-offs in performance of the introduced techniques and the pros and cons of each technique.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494132",
        "category": "Databases"
    },
    {
        "title": "Parallel discrepancy detection and incremental detection",
        "authors": "['Wenfei Fan', 'Chao Tian', 'Yanghao Wang', 'Qiang Yin']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "This paper studies how to catch duplicates, mismatches and conflicts in the same process. We adopt a class of entity enhancing rules that embed machine learning predicates, unify entity resolution and conflict resolution, and are collectively defined across multiple relations. We detect discrepancies as violations of such rules. We establish the complexity of discrepancy detection and incremental detection problems with the rules; they are both NP-complete and W[1]-hard. To cope with the intractability and scale with large datasets, we develop parallel algorithms and parallel incremental algorithms for discrepancy detection. We show that both algorithms are parallelly scalable, i.e., they guarantee to reduce runtime when more processors are used. Moreover, the parallel incremental algorithm is relatively bounded. The complexity bounds and algorithms carry over to denial constraints, a special case of the entity enhancing rules. Using real-life and synthetic datasets, we experimentally verify the effectiveness, scalability and efficiency of the algorithms.",
        "link": "https://dl.acm.org/doi/10.14778/3457390.3457400",
        "category": "Databases"
    },
    {
        "title": "IceClave: A Trusted Execution Environment for In-Storage Computing",
        "authors": "['Luyi Kang', 'Yuqi Xue', 'Weiwei Jia', 'Xiaohao Wang', 'Jongryool Kim', 'Changhwan Youn', 'Myeong Joon Kang', 'Hyung Jin Lim', 'Bruce Jacob', 'Jian Huang']",
        "date": "October 2021",
        "source": "MICRO '21: MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture",
        "abstract": "In-storage computing with modern solid-state drives (SSDs) enables developers to offload programs from the host to the SSD. It has been proven to be an effective approach to alleviate the I/O bottleneck. To facilitate in-storage computing, many frameworks have been proposed. However, few of them treat the in-storage security as the first citizen. Specifically, since modern SSD controllers do not have a trusted execution environment, an offloaded (malicious) program could steal, modify, and even destroy the data stored in the SSD.  In this paper, we first investigate the attacks that could be conducted by offloaded in-storage programs. To defend against these attacks, we build a lightweight trusted execution environment, named IceClave for in-storage computing. IceClave enables security isolation between in-storage programs and flash management functions that include flash address translation, data access control, and garbage collection, with TrustZone extensions. IceClave also achieves security isolation between in-storage programs by enforcing memory integrity verification of in-storage DRAM with low overhead. To protect data loaded from flash chips, IceClave develops a lightweight data encryption/decryption mechanism in flash controllers. We develop IceClave with a full system simulator. We evaluate IceClave with a variety of data-intensive applications such as databases. Compared to state-of-the-art in-storage computing approaches, IceClave introduces only 7.6% performance overhead, while enforcing security isolation in the SSD controller with minimal hardware cost. IceClave still keeps the performance benefit of in-storage computing by delivering up to 2.31 × better performance than the conventional host-based trusted computing approach.",
        "link": "https://dl.acm.org/doi/10.1145/3466752.3480109",
        "category": "Databases"
    },
    {
        "title": "The origins and prospects of the digitalization of the economy in the Eurasian space",
        "authors": "['A. R. Tumashev', 'M. V. Tumasheva', 'I. S. Mavlyautdinov']",
        "date": "March 2021",
        "source": "DEFIN-2021: IV International Scientific and Practical Conference",
        "abstract": "The article examines the conditions for the formation and development of the digital economy of Russia as the basis of the Eurasian economic space. Digitalization is defined as the automation of the process control system, the functioning of which is provided by digital methods of transmitting, processing and storing large databases, the addition of the human mind with artificial (machine) intelligence systems. It opens up new opportunities to improve the efficiency of production, rational use of resources, and ensure the sustainable development of society. In a broader sense, the digital economy integrates all areas of social life in which digital technologies are used, which makes it possible to form new tools and methods of social control, supplement the objective reality of virtual reality, and form new areas of profit extraction that are not directly related to material production. The article shows that the conditions for the development of digitalization in Russia as the central link of a relatively independent Eurasian economic space are effective demand, reflecting the needs for digital services, the availability of domestic software and hardware, and the element base that can create a safe mode of functioning of the national digital management system for society. The assessment of the current state of digitalization in Russia is given, it is noted that it has more affected distribution relations, education, commerce, finance and public administration, and has significant opportunities for implementation in the production sector.",
        "link": "https://dl.acm.org/doi/10.1145/3487757.3490917",
        "category": "Databases"
    },
    {
        "title": "RAMP-TAO: layering atomic transactions on Facebook's online TAO data store",
        "authors": "['Audrey Cheng', 'Xiao Shi', 'Lu Pan', 'Anthony Simpson', 'Neil Wheaton', 'Shilpa Lawande', 'Nathan Bronson', 'Peter Bailis', 'Natacha Crooks', 'Ion Stoica']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Facebook's graph store TAO, like many other distributed data stores, traditionally prioritizes availability, efficiency, and scalability over strong consistency or isolation guarantees to serve its large, read-dominant workloads. As product developers build diverse applications on top of this system, they increasingly seek transactional semantics. However, providing advanced features for select applications while preserving the system's overall reliability and performance is a continual challenge. In this paper, we first characterize developer desires for transactions that have emerged over the years and describe the current failure-atomic (i.e., write) transactions offered by TAO. We then explore how to introduce an intuitive read transaction API. We highlight the need for atomic visibility guarantees in this API with a measurement study on potential anomalies that occur without stronger isolation for reads. Our analysis shows that 1 in 1,500 batched reads reflects partial transactional updates, which complicate the developer experience and lead to unexpected results. In response to our findings, we present the RAMP-TAO protocol, a variation based on the Read Atomic Multi-Partition (RAMP) protocols that can be feasibly deployed in production with minimal overhead while ensuring atomic visibility for a read-optimized workload at scale.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476379",
        "category": "Databases"
    },
    {
        "title": "Catch a blowfish alive: a demonstration of policy-aware differential privacy for interactive data exploration",
        "authors": "['Jiaxiang Liu', 'Karl Knopf', 'Yiqing Tan', 'Bolin Ding', 'Xi He']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Policy-aware differential privacy (DP) frameworks such as Blowfish privacy enable more accurate query answers than standard DP. In this work, we build the first policy-aware DP system for interactive data exploration, BlowfishDB, that aims to (i) provide bounded and flexible privacy guarantees to the data curators of sensitive data and (ii) support accurate and efficient data exploration by data analysts. However, the specification and processing of customized privacy policies incur additional performance cost, especially for datasets with a large domain. To address this challenge, we propose dynamic Blowfish privacy which allows for the dynamic generation of smaller privacy policies and their data representations at query time. BlowfishDB ensures same levels of accuracy and privacy as one would get working on the static privacy policy. In this demonstration of BlowfishDB, we show how a data curator can fine-tune privacy policies for a sensitive dataset and how a data analyst can retrieve accuracy-bounded query answers efficiently without being a privacy expert.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476363",
        "category": "Databases"
    },
    {
        "title": "Flexible rule-based decomposition and metadata independence in modin: a parallel dataframe system",
        "authors": "['Devin Petersohn', 'Dixin Tang', 'Rehan Durrani', 'Areg Melik-Adamyan', 'Joseph E. Gonzalez', 'Anthony D. Joseph', 'Aditya G. Parameswaran']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Dataframes have become universally popular as a means to represent data in various stages of structure, and manipulate it using a rich set of operators---thereby becoming an essential tool in the data scientists' toolbox. However, dataframe systems, such as pandas, scale poorly---and are non-interactive on moderate to large datasets. We discuss our experiences developing Modin, our first cut at a parallel dataframe system, which already has users across several industries and over 1M downloads. Modin translates pandas functions into a core set of operators that are individually parallelized via columnar, row-wise, or cell-wise decomposition rules that we formalize in this paper. We also introduce metadata independence to allow metadata---such as order and type---to be decoupled from the physical representation and maintained lazily. Using rule-based decomposition and metadata independence, along with careful engineering, Modin is able to support pandas operations across both rows and columns on very large dataframes---unlike Koalas and Dask DataFrames that either break down or are unable to support such operations, while also being much faster than pandas.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494152",
        "category": "Databases"
    },
    {
        "title": "Valentine in action: matching tabular data at scale",
        "authors": "['Christos Koutras', 'Kyriakos Psarakis', 'George Siachamis', 'Andra Ionescu', 'Marios Fragkoulis', 'Angela Bonifati', 'Asterios Katsifodimos']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Capturing relationships among heterogeneous datasets in large data lakes - traditionally termed schema matching - is one of the most challenging problems that corporations and institutions face nowadays. Discovering and integrating datasets heavily relies on the effectiveness of the schema matching methods in use. However, despite the wealth of research, evaluation of schema matching methods is still a daunting task: there is a lack of openly-available datasets with ground truth, reference method implementations, and comprehensible GUIs that would facilitate development of both novel state-of-the-art schema matching techniques and novel data discovery methods.Our recently proposed Valentine is the first system to offer an open-source experiment suite to organize, execute and orchestrate large-scale matching experiments. In this demonstration we present its functionalities and enhancements: i) a scalable system, with a user-centric GUI, that enables the fabrication of datasets and the evaluation of matching methods on schema matching scenarios tailored to the scope of tabular dataset discovery, ii) a scalable holistic matching system that can receive tabular datasets from heterogeneous sources and provide with similarity scores among their columns, in order to facilitate modern procedures in data lakes, such as dataset discovery.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476366",
        "category": "Databases"
    },
    {
        "title": "FACE: a normalizing flow based cardinality estimator",
        "authors": "['Jiayi Wang', 'Chengliang Chai', 'Jiabin Liu', 'Guoliang Li']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Cardinality estimation is one of the most important problems in query optimization. Recently, machine learning based techniques have been proposed to effectively estimate cardinality, which can be broadly classified into query-driven and data-driven approaches. Query-driven approaches learn a regression model from a query to its cardinality; while data-driven approaches learn a distribution of tuples, select some samples that satisfy a SQL query, and use the data distributions of these selected tuples to estimate the cardinality of the SQL query. As query-driven methods rely on training queries, the estimation quality is not reliable when there are no high-quality training queries; while data-driven methods have no such limitation and have high adaptivity.In this work, we focus on data-driven methods. A good data-driven model should achieve three optimization goals. First, the model needs to capture data dependencies between columns and support large domain sizes (achieving high accuracy). Second, the model should achieve high inference efficiency, because many data samples are needed to estimate the cardinality (achieving low inference latency). Third, the model should not be too large (achieving a small model size). However, existing data-driven methods cannot simultaneously optimize the three goals. To address the limitations, we propose a novel cardinality estimator FACE, which leverages the Normalizing Flow based model to learn a continuous joint distribution for relational data. FACE can transform a complex distribution over continuous random variables into a simple distribution (e.g., multivariate normal distribution), and use the probability density to estimate the cardinality. First, we design a dequantization method to make data more \"continuous\". Second, we propose encoding and indexing techniques to handle Like predicates for string data. Third, we propose a Monte Carlo method to efficiently estimate the cardinality. Experimental results show that our method significantly outperforms existing approaches in terms of estimation accuracy while keeping similar latency and model size.",
        "link": "https://dl.acm.org/doi/10.14778/3485450.3485458",
        "category": "Databases"
    },
    {
        "title": "Hyperspace: the indexing subsystem of azure synapse",
        "authors": "['Rahul Potharaju', 'Terry Kim', 'Eunjin Song', 'Wentao Wu', 'Lev Novik', 'Apoorve Dave', 'Andrew Fogarty', 'Pouria Pirzadeh', 'Vidip Acharya', 'Gurleen Dhody', 'Jiying Li', 'Sinduja Ramanujam', 'Nicolas Bruno', 'César A. Galindo-Legaria', 'Vivek Narasayya', 'Surajit Chaudhuri', 'Anil K. Nori', 'Tomas Talius', 'Raghu Ramakrishnan']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Microsoft recently introduced Azure Synapse Analytics, which offers an integrated experience across data ingestion, storage, and querying in Apache Spark and T-SQL over data in the lake, including files and warehouse tables. In this paper, we present our experiences with designing and implementing Hyperspace, the indexing subsystem underlying Synapse. Hyperspace enables users to build multiple types of secondary indexes on their data, maintain them through a multi-user concurrency model, and leverage them automatically---without any change to their application code---for query/workload acceleration. Many requirements of Hyperspace are based on feedback from several enterprise customers. We present the details of Hyperspace's underlying design, the user-facing APIs, its concurrency control protocol for index access, its index-aware query processing techniques, and its maintenance mechanisms for handling index updates. Evaluations over standard industry benchmarks and real customer workloads show that Hyperspace can accelerate query execution by up to 10x and in certain real-world workloads, even up to two orders of magnitude.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476382",
        "category": "Databases"
    },
    {
        "title": "Stackless Processing of Streamed Trees",
        "authors": "['Corentin Barloy', 'Filip Murlak', 'Charles Paperman']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Processing tree-structured data in the streaming model is a challenge: capturing regular properties of streamed trees by means of a stack is costly in memory, but falling back to finite-state automata drastically limits the computational power. We propose an intermediate stackless model based on register automata equipped with a single counter, used to maintain the current depth in the tree. We explore the power of this model to validate and query streamed trees. Our main result is an effective characterization of regular path queries (RPQs) that can be evaluated stacklessly---with and without registers. In particular, we confirm the conjectured characterization of tree languages defined by DTDs that are recognizable without registers, by Segoufin and Vianu (2002), in the special case of tree languages defined by means of an RPQ.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458320",
        "category": "Databases"
    },
    {
        "title": "Privacy Preservation Techniques for Sequential Data Releasing",
        "authors": "['Surapon Riyana', 'Noppamas Riyana', 'Srikul Nanthachumphu']",
        "date": "June 2021",
        "source": "IAIT '21: Proceedings of the 12th International Conference on Advances in Information Technology",
        "abstract": "Privacy violation is a serious issue that must be considered when datasets are released for public use. To address this issue, a well-known privacy preservation model, l-Diversity, is proposed. Unfortunately, l-Diversity is generally proposed to address privacy violation issues in datasets that are focused on performing one-time data releasing. For this reason, l-Diversity could be inadequate to preserve the privacy data if datasets are dynamic and released at all times. To rid this vulnerability of l-Diversity, a new privacy preservation model for sequential data releasing to be proposed in this work, so called as ε-Error and l-Diversity. Aside from privacy preservation constraints, the complexity and the data utility are also maintained in the privacy preservation constraint of the proposed model.",
        "link": "https://dl.acm.org/doi/10.1145/3468784.3470468",
        "category": "Databases"
    },
    {
        "title": "Domain Named Entity Recognition and Applications in Test and Evaluation",
        "authors": "['Di Zhao', 'Shuai Jiang', 'Wei Wang', 'Jing Zhang', 'Rui-Peng Luan']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "A great amount of information in Test and Evaluation (T&E) is presented in the form of multi-source heterogeneous data such as performance test, combat trial and during-service assessment. Despite the existence of numerous and well-versed Domain Named Entity Recognition (DNER) methods in the general field, it still remains scarcely resourced. In this paper we survey novel methods that have recently been introduced for such DNER tasks. In addition, we construct the dataset for further NER tasks in the field of Test and Evaluation. Finally, our work lays the cornerstone for the development of subsequent NER in this field.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501595",
        "category": "Databases"
    },
    {
        "title": "Totally-Ordered Prefix Parallel Snapshot Isolation",
        "authors": "['Nuno Faria', 'José Pereira']",
        "date": "April 2021",
        "source": "PaPoC '21: Proceedings of the 8th Workshop on Principles and Practice of Consistency for Distributed Data",
        "abstract": "Distributed data management systems have increasingly been using variants of Snapshot Isolation (SI) as their transactional isolation criteria as it combines strong ACID guarantees with non-blocking reads and scalability. However, most existing proposals are limited by the performance of update propagation and stability detection, in particular, when execution and storage are disaggregated. In this paper, we propose TOPSI, an approach providing a restricted form of Parallel Snapshot Isolation (PSI) that allows partially ordering recent transactions to avoid waiting for remote updates or using a stale snapshot. Moreover, it has the interesting property of making a prefix of history in all sites converge to a common total order. This allows versions to be represented by a single scalar timestamp for certification and storage in a shared store. We demonstrate the impact on throughput and abort rate with a proof-of-concept implementation and the industry-standard TPC-C benchmark.",
        "link": "https://dl.acm.org/doi/10.1145/3447865.3457966",
        "category": "Databases"
    },
    {
        "title": "Cardinality estimation in DBMS: a comprehensive benchmark evaluation",
        "authors": "['Yuxing Han', 'Ziniu Wu', 'Peizhi Wu', 'Rong Zhu', 'Jingyi Yang', 'Liang Wei Tan', 'Kai Zeng', 'Gao Cong', 'Yanzhao Qin', 'Andreas Pfadler', 'Zhengping Qian', 'Jingren Zhou', 'Jiangneng Li', 'Bin Cui']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Cardinality estimation (CardEst) plays a significant role in generating high-quality query plans for a query optimizer in DBMS. In the last decade, an increasing number of advanced CardEst methods (especially ML-based) have been proposed with outstanding estimation accuracy and inference latency. However, there exists no study that systematically evaluates the quality of these methods and answer the fundamental problem: to what extent can these methods improve the performance of query optimizer in real-world settings, which is the ultimate goal of a CardEst method.In this paper, we comprehensively and systematically compare the effectiveness of CardEst methods in a real DBMS. We establish a new benchmark for CardEst, which contains a new complex real-world dataset STATS and a diverse query workload STATS-CEB. We integrate multiple most representative CardEst methods into an open-source DBMS PostgreSQL, and comprehensively evaluate their true effectiveness in improving query plan quality, and other important aspects affecting their applicability. We obtain a number of key findings under different data and query settings. Furthermore, we find that the widely used estimation accuracy metric (Q-Error) cannot distinguish the importance of different sub-plan queries during query optimization and thus cannot truly reflect the generated query plan quality. Therefore, we propose a new metric P-Error to evaluate the performance of CardEst methods, which overcomes the limitation of Q-Error and is able to reflect the overall end-to-end performance of CardEst methods. It could serve as a better optimization objective for future CardEst methods.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503586",
        "category": "Databases"
    },
    {
        "title": "Power-optimized Deployment of Key-value Stores Using Storage Class Memory",
        "authors": "['Hiwot Tadese Kassa', 'Jason Akers', 'Mrinmoy Ghosh', 'Zhichao Cao', 'Vaibhav Gogte', 'Ronald Dreslinski']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "High-performance flash-based key-value stores in data-centers utilize large amounts of DRAM to cache hot data. However, motivated by the high cost and power consumption of DRAM, server designs with lower DRAM-per-compute ratio are becoming popular. These low-cost servers enable scale-out services by reducing server workload densities. This results in improvements to overall service reliability, leading to a decrease in the total cost of ownership (TCO) for scalable workloads. Nevertheless, for key-value stores with large memory footprints, these reduced DRAM servers degrade performance due to an increase in both IO utilization and data access latency. In this scenario, a standard practice to improve performance for sharded databases is to reduce the number of shards per machine, which degrades the TCO benefits of reduced DRAM low-cost servers. In this work, we explore a practical solution to improve performance and reduce the costs and power consumption of key-value stores running on DRAM-constrained servers by using Storage Class Memories (SCM).SCMs in a DIMM form factor, although slower than DRAM, are sufficiently faster than flash when serving as a large extension to DRAM. With new technologies like Compute Express Link, we can expand the memory capacity of servers with high bandwidth and low latency connectivity with SCM. In this article, we use Intel Optane PMem 100 Series SCMs (DCPMM) in AppDirect mode to extend the available memory of our existing single-socket platform deployment of RocksDB (one of the largest key-value stores at Meta). We first designed a hybrid cache in RocksDB to harness both DRAM and SCM hierarchically. We then characterized the performance of the hybrid cache for three of the largest RocksDB use cases at Meta (ChatApp, BLOB Metadata, and Hive Cache). Our results demonstrate that we can achieve up to 80% improvement in throughput and 20% improvement in P95 latency over the existing small DRAM single-socket platform, while maintaining a 43–48% cost improvement over our large DRAM dual-socket platform. To the best of our knowledge, this is the first study of the DCPMM platform in a commercial data center.",
        "link": "https://dl.acm.org/doi/10.1145/3511905",
        "category": "Databases"
    },
    {
        "title": "Enabling SQL-based training data debugging for federated learning",
        "authors": "['Yejia Liu', 'Weiyuan Wu', 'Lampros Flokas', 'Jiannan Wang', 'Eugene Wu']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "How can we debug a logistic regression model in a federated learning setting when seeing the model behave unexpectedly (e.g., the model rejects all high-income customers' loan applications)? The SQL-based training data debugging framework has proved effective to fix this kind of issue in a non-federated learning setting. Given an unexpected query result over model predictions, this framework automatically removes the label errors from training data such that the unexpected behavior disappears in the retrained model. In this paper, we enable this powerful framework for federated learning. The key challenge is how to develop a security protocol for federated debugging which is proved to be secure, efficient, and accurate. Achieving this goal requires us to investigate how to seamlessly integrate the techniques from multiple fields (Databases, Machine Learning, and Cybersecurity). We first propose FedRain, which extends Rain, the state-of-the-art SQL-based training data debugging framework, to our federated learning setting. We address several technical challenges to make FedRain work and analyze its security guarantee and time complexity. The analysis results show that FedRain falls short in terms of both efficiency and security. To overcome these limitations, we redesign our security protocol and propose Frog, a novel SQL-based training data debugging framework tailored for federated learning. Our theoretical analysis shows that Frog is more secure, more accurate, and more efficient than FedRain. We conduct extensive experiments using several real-world datasets and a case study. The experimental results are consistent with our theoretical analysis and validate the effectiveness of Frog in practice.",
        "link": "https://dl.acm.org/doi/10.14778/3494124.3494125",
        "category": "Databases"
    },
    {
        "title": "Measuring the Digital Economy and Society: A Study on the Application of the Digital Economy and Society Index in the Western Balkans",
        "authors": "['Zoran Jordanoski', 'Morten Meyerhoff Nielsen']",
        "date": "October 2021",
        "source": "ICEGOV '21: Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance",
        "abstract": "Measuring the digital socio-economic transformation is an essential part of good, appropriate, knowledge-based decision making. Multiple national and international indexes measure the various aspects of the digital transformation, such as the telecommunication infrastructure development, digital skills, eGovernment, eCommerce, eBusiness, etc. Focusing on the integration of the indicators that affect the digital economy and society, the European Union (EU) launched the Digital Economy and Society Index (DESI) in 2014 aimed to measure the progress made by the EU Member States in the progress in digital performance and competitiveness. Composed of five dimensions, DESI measures both, the impact of the digital transformation on the Digital Economy and Digital Society. As the DESI and its indicators are part of the EU Acquis Communautaire, all candidates and potential candidates for EU membership must be prepared to provide complete datasets for measuring the indicators introduced and used by the EU. Currently, all Western Balkan (WB) economies are either candidates or potential candidates for EU membership. With the digital agenda ranked high on the government agendas, this raises the questions of the level of preparedness of the WB economies in providing the complete high-quality datasets required to calculate the DESI. Specifically, how is the WB economy data methodologically aligned with the DESI and other relevant EU methodologies (e.g. Survey on ICT usage in households and by individuals, Study on Broadband Coverage in Europe, eGovernment Benchmark Study, etc.)? This paper aims to assess the readiness of each WB economy to provide the complete datasets for the DESI indicators. Key findings are that WB economies are generally ready to provide methodologically align data for the DESI indicators, especially for the indicators that use data from the national databases. Key challenges in the existing domestic methodologies are generally linked to the indicators that are extracted from the Commission ad hoc studies, especially for those where the WB economies (or some of them) are not included.",
        "link": "https://dl.acm.org/doi/10.1145/3494193.3494220",
        "category": "Databases"
    },
    {
        "title": "MultiCategory: multi-model query processing meets category theory and functional programming",
        "authors": "['Valter Uotila', 'Jiaheng Lu', 'Dieter Gawlick', 'Zhen Hua Liu', 'Souripriya Das', 'Gregory Pogossiants']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The variety of data is one of the important issues in the era of Big Data. The data are naturally organized in different formats and models, including structured data, semi-structured data, and unstructured data. Prior research has envisioned an approach to abstract multi-model data with a schema category and an instance category by using category theory. In this paper, we demonstrate a system, called MultiCategory, which processes multi-model queries based on category theory and functional programming. This demo is centered around four main scenarios to show a tangible system. First, we show how to build a schema category and an instance category by loading different models of data, including relational, XML, key-value, and graph data. Second, we show a few examples of query processing by using the functional programming language Haskell. Third, we demo the flexible outputs with different models of data for the same input query. Fourth, to better understand the category theoretical structure behind the queries, we offer a variety of graphical hooks to explore and visualize queries as graphs with respect to the schema category, as well as the query processing procedure with Haskell.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476314",
        "category": "Databases"
    },
    {
        "title": "HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings",
        "authors": "['Wolfgang Fischl', 'Georg Gottlob', 'Davide Mario Longo', 'Reinhard Pichler']",
        "date": "None",
        "source": "ACM Journal of Experimental Algorithmics",
        "abstract": "To cope with the intractability of answering Conjunctive Queries (CQs) and solving Constraint Satisfaction Problems (CSPs), several notions of hypergraph decompositions have been proposed—giving rise to different notions of width, noticeably, plain, generalized, and fractional hypertree width (hw, ghw, and fhw). Given the increasing interest in using such decomposition methods in practice, a publicly accessible repository of decomposition software, as well as a large set of benchmarks, and a web-accessible workbench for inserting, analyzing, and retrieving hypergraphs are called for.We address this need by providing (i) concrete implementations of hypergraph decompositions (including new practical algorithms), (ii) a new, comprehensive benchmark of hypergraphs stemming from disparate CQ and CSP collections, and (iii) HyperBench, our new web-interface for accessing the benchmark and the results of our analyses. In addition, we describe a number of actual experiments we carried out with this new infrastructure.",
        "link": "https://dl.acm.org/doi/10.1145/3440015",
        "category": "Databases"
    },
    {
        "title": "The Interaction of Ebola Virus with the Immune System Ebola and Immunity",
        "authors": "['Jamie Tian']",
        "date": "May 2021",
        "source": "ICBEA '21: 4th International Conference on Biometric Engineering and Applications",
        "abstract": "Ebola Virus Disease is one of the deadliest diseases known with a fatality rate of 90% caused by certain species. Ebola virus belongs to the family of Filoviridae viruses. Since its discovery in 1976, multiple outbreaks were reported around the world which had a great impact on global economy and public health system. Ebola virus is made of multiple components which can be targeted by different drugs as well as antibodies, one of them is its glycoprotein on the surface of the viral particle. The virus first replicates in the spleen and lymph nodes after entering the body and interferes with the host's type I Interferon response. Systemic inflammation is another characteristic feature of the disease. The incubation period of the disease is normally 5 to 7 days, and symptoms include fever, diarrhoea, myalgia, malaise, abdominal pain, nausea, and vomiting. As the disease progresses, haemorrhagic fever might also occur in the gastrointestinal tract. This paper aims to review recent advances on Ebola treatment and immune system by providing an overview of the latest findings. To this end, articles published in peer-reviewed journals in English between 1990 to 2019 were searched via publicly available databases such as PubMed, information were extracted and summarized. The results show that Ebola causes multi-organ failure, fulminant shock and eventually death. Recent advances on monoclonal antibodies can be used to treat the disease which are normally given as cocktails, and vaccines such as the recombinant vesicular stomatitis virus-vectored vaccine are also available. In conclusion, studying how Ebola virus works and developing effective treatment methods are very urgent and important.",
        "link": "https://dl.acm.org/doi/10.1145/3476779.3476794",
        "category": "Databases"
    },
    {
        "title": "Tesseract: distributed, general graph pattern mining on evolving graphs",
        "authors": "['Laurent Bindschaedler', 'Jasmina Malicevic', 'Baptiste Lepers', 'Ashvin Goel', 'Willy Zwaenepoel']",
        "date": "April 2021",
        "source": "EuroSys '21: Proceedings of the Sixteenth European Conference on Computer Systems",
        "abstract": "Tesseract is the first distributed system for executing general graph mining algorithms on evolving graphs. Tesseract scales out by decomposing a stream of graph updates into per-update mining tasks and dynamically assigning these tasks to a set of distributed workers. We present a novel approach to change detection that efficiently determines the exact modifications to the algorithm's output for each update to the input graph. We use a disaggregated, multiversioned graph store to allow workers to process updates independently, without producing duplicates. Moreover, Tesseract provides interactive mining insights for complex applications using an incremental aggregation API. Finally, we implement and evaluate Tesseract and demonstrate that it achieves orders-of-magnitude improvements over state-of-the-art systems.",
        "link": "https://dl.acm.org/doi/10.1145/3447786.3456253",
        "category": "Databases"
    },
    {
        "title": "Efficient Exploration of Interesting Aggregates in RDF Graphs",
        "authors": "['Yanlei Diao', 'Paweł Guzewicz', 'Ioana Manolescu', 'Mirjana Mazuran']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "As large Open Data are increasingly shared as RDF graphs today, there is a growing demand to help users discover the most interesting facets of a graph, which are often hard to grasp without automatic tools. We consider the problem of automatically identifying the k most interesting aggregate queries that can be evaluated on an RDF graph, given an integer k and a user-specified interestingness function. Our problem departs from analytics in relational data warehouses in that (i) in an RDF graph we are not given but we must identify the facts, dimensions, and measures of candidate aggregates; (ii) the classical approach to efficiently evaluating multiple aggregates breaks in the face of multi-valued dimensions in RDF data. In this work, we propose an extensible end-to-end framework that enables the identification and evaluation of interesting aggregates based on a new RDF-compatible one-pass algorithm for efficiently evaluating a lattice of aggregates and a novel early-stop technique (with probabilistic guarantees) that can prune uninteresting aggregates. Experiments using both real and synthetic graphs demonstrate the ability of our framework to find interesting aggregates in a large search space, the efficiency of our algorithms (with up to 2.9x speedup over a similar pipeline based on existing algorithms), and scalability as the data size and complexity grow.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457307",
        "category": "Databases"
    },
    {
        "title": "GLOVE: Towards Privacy-Preserving Publishing of Record-Level-Truthful Mobile Phone Trajectories",
        "authors": "['Marco Gramaglia', 'Marco Fiore', 'Angelo Furno', 'Razvan Stanica']",
        "date": "None",
        "source": "ACM/IMS Transactions on Data Science",
        "abstract": "Datasets of mobile phone trajectories collected by network operators offer an unprecedented opportunity to discover new knowledge from the activity of large populations of millions. However, publishing such trajectories also raises significant privacy concerns, as they contain personal data in the form of individual movement patterns. Privacy risks induce network operators to enforce restrictive confidential agreements in the rare occasions when they grant access to collected trajectories, whereas a less involved circulation of these data would fuel research and enable reproducibility in many disciplines. In this work, we contribute a building block toward the design of privacy-preserving datasets of mobile phone trajectories that are truthful at the record level. We present GLOVE, an algorithm that implements k-anonymity, hence solving the crucial unicity problem that affects this type of data while ensuring that the anonymized trajectories correspond to real-life users. GLOVE builds on original insights about the root causes behind the undesirable unicity of mobile phone trajectories, and leverages generalization and suppression to remove them. Proof-of-concept validations with large-scale real-world datasets demonstrate that the approach adopted by GLOVE allows preserving a substantial level of accuracy in the data, higher than that granted by previous methodologies.",
        "link": "https://dl.acm.org/doi/10.1145/3451178",
        "category": "Databases"
    },
    {
        "title": "Making Graphs Compact by Lossless Contraction",
        "authors": "['Wenfei Fan', 'Yuanhao Li', 'Muyang Liu', 'Can Lu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "This paper proposes a scheme to reduce big graphs to small graphs. It contracts obsolete parts, stars, cliques and paths into supernodes. The supernodes carry a synopsis S_Q for each query class Q to abstract key features of the contracted parts for answering queries of Q. The contraction scheme provides a compact graph representation and prioritizes up-to-date data. Better still, it is generic and lossless. We show that the same contracted graph is able to support multiple query classes at the same time, no matter whether their queries are label-based or not, local or non-local. Moreover, existing algorithms for these queries can be readily adapted to compute exact answers by using the synopses when possible, and decontracting the supernodes only when necessary. As a proof of concept, we show how to adapt existing algorithms for subgraph isomorphism, triangle counting and shortest distance to contracted graphs. We also provide an incremental contraction algorithm in response to updates. We experimentally verify that on average, the contraction scheme reduces graphs by 71.2%, and improves the evaluation of these queries by 1.53, 1.42 and 2.14 times, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452797",
        "category": "Databases"
    },
    {
        "title": "Making Graphs Compact by Lossless Contraction",
        "authors": "['Wenfei Fan', 'Yuanhao Li', 'Muyang Liu', 'Can Lu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "This paper proposes a scheme to reduce big graphs to small graphs. It contracts obsolete parts, stars, cliques and paths into supernodes. The supernodes carry a synopsis S_Q for each query class Q to abstract key features of the contracted parts for answering queries of Q. The contraction scheme provides a compact graph representation and prioritizes up-to-date data. Better still, it is generic and lossless. We show that the same contracted graph is able to support multiple query classes at the same time, no matter whether their queries are label-based or not, local or non-local. Moreover, existing algorithms for these queries can be readily adapted to compute exact answers by using the synopses when possible, and decontracting the supernodes only when necessary. As a proof of concept, we show how to adapt existing algorithms for subgraph isomorphism, triangle counting and shortest distance to contracted graphs. We also provide an incremental contraction algorithm in response to updates. We experimentally verify that on average, the contraction scheme reduces graphs by 71.2%, and improves the evaluation of these queries by 1.53, 1.42 and 2.14 times, respectively.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452797",
        "category": "Databases"
    },
    {
        "title": "Differential Privacy for Directional Data",
        "authors": "['Benjamin Weggenmann', 'Florian Kerschbaum']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Directional data is an important class of data where the magnitudes of the data points are negligible. It naturally occurs in many real-world scenarios: For instance, geographic locations (approximately) lie on a sphere, and periodic data such as time of day, or day of week can be interpreted as points on a circle. Massive amounts of directional data are collected by location-based service platforms such as Google Maps or Foursquare, who depend on mobility data from users' smartphones or wearable devices to enable their analytics and marketing businesses. However, such data is often highly privacy-sensitive and hence demands measures to protect the privacy of the individuals whose data is collected and processed. Starting with the von Mises-Fisher distribution, we therefore propose and analyze two novel privacy mechanisms for directional data by combining directional statistics with differential privacy, which presents the current state-of-the-art for quantifying and limiting information disclosure about individuals. As we will see, our specialized privacy mechanisms achieve a better privacy-utility trade-off than ex post adaptions of established mechanisms to directional data.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484734",
        "category": "Databases"
    },
    {
        "title": "Towards Elastic Incrementalization for Datalog",
        "authors": "['David Zhao', 'Pavle Subotic', 'Mukund Raghothaman', 'Bernhard Scholz']",
        "date": "September 2021",
        "source": "PPDP '21: Proceedings of the 23rd International Symposium on Principles and Practice of Declarative Programming",
        "abstract": "Various incremental evaluation strategies for Datalog have been developed that reuse computations for small input changes. These methods assume that incrementalization is always a better strategy than recomputation. However, in real-world applications such as static program analysis, recomputation can be cheaper than incrementalization for large updates.  This work introduces an elastic incremental approach with two strategies that can be selected based on the impact of the input change. The first strategy is a Bootstrap strategy that recomputes the entire result for high-impact changes. The second is an Update strategy that performs an incremental update for low-impact changes. Our approach allows for a lightweight Bootstrap strategy suitable for high-impact changes, with the trade-off that Update may require more work for small changes. We demonstrate our approach using real-world applications and compare our elastic incremental approach to existing methods.",
        "link": "https://dl.acm.org/doi/10.1145/3479394.3479415",
        "category": "Databases"
    },
    {
        "title": "Stable Model Semantics for Guarded Existential Rules and Description Logics: Decidability and Complexity",
        "authors": "['Georg Gottlob', 'André Hernich', 'Clemens Kupke', 'Thomas Lukasiewicz']",
        "date": "None",
        "source": "Journal of the ACM",
        "abstract": "This work investigates the decidability and complexity of database query answering under guarded existential rules with nonmonotonic negation according to the classical stable model semantics. In this setting, existential quantification is interpreted via Skolem functions, and the unique name assumption is adopted. As a first result, we show the decidability of answering first-order queries based on such rules by a translation into the satisfiability problem for guarded second-order formulas having the tree-model property. To obtain precise complexity results for unions of conjunctive queries, we transform the original problem in polynomial time into an intermediate problem that is easier to analyze: query answering for guarded disjunctive existential rules with stratified negation. We obtain precise bounds for the general setting and for various restricted settings. We also consider extensions of the original formalism with negative constraints, keys, and the possibility of negated atoms in queries. Finally, we show how the above results can be used to provide decidability and complexity results for a natural adaptation of the stable model semantics to description logics such as ELHI and the DL-Lite family.",
        "link": "https://dl.acm.org/doi/10.1145/3447508",
        "category": "Databases"
    },
    {
        "title": "WiseKG: Balanced Access to Web Knowledge Graphs",
        "authors": "['Amr Azzam', 'Christian Aebeloe', 'Gabriela Montoya', 'Ilkcan Keles', 'Axel Polleres', 'Katja Hose']",
        "date": "April 2021",
        "source": "WWW '21: Proceedings of the Web Conference 2021",
        "abstract": "SPARQL query services that balance processing between clients and servers become more and more essential to handle the increasing load for open and decentralized knowledge graphs on the Web. To this end, Linked Data Fragments (LDF) have introduced a foundational framework that has sparked research exploring a spectrum of potential Web querying interfaces in between server-side query processing via SPARQL endpoints and client-side query processing of data dumps. Current proposals in between typically suffer from imbalanced load on either the client or the server. In this paper, to the best of our knowledge, we present the first work that combines both client-side and server-side query optimization techniques in a truly dynamic fashion: we introduce WiseKG, a system that employs a cost model that dynamically delegates the load between servers and clients by combining client-side processing of shipped partitions with efficient server-side processing of star-shaped sub-queries, based on current server workload and client capabilities. Our experiments show that WiseKG significantly outperforms state-of-the-art solutions in terms of average total query execution time per client, while at the same time decreasing network traffic and increasing server-side availability.",
        "link": "https://dl.acm.org/doi/10.1145/3442381.3449911",
        "category": "Databases"
    },
    {
        "title": "The Wind in Our Sails: Developing a Reusable and Maintainable Dutch Maritime History Knowledge Graph",
        "authors": "['Stijn Schouten', 'Victor de Boer', 'Lodewijk Petram', 'Marieke van Erp']",
        "date": "December 2021",
        "source": "K-CAP '21: Proceedings of the 11th Knowledge Capture Conference",
        "abstract": "Digital sources are more prevalent than ever but effectively using them can be challenging. One core challenge is that digitized sources are often distributed, thus forcing researchers to spend time collecting, interpreting, and aligning different sources. A knowledge graph can accelerate research by providing a single connected source of truth that humans and machines can query. During two design-test cycles, we convert four data sets from the historical maritime domain into a knowledge graph. The focus during these cycles is on creating a sustainable and usable approach that can be adopted in other linked data conversion efforts. Furthermore, our knowledge graph is available for maritime historians and other interested users to investigate the daily business of the Dutch East India Company through a unified portal.",
        "link": "https://dl.acm.org/doi/10.1145/3460210.3493548",
        "category": "Databases"
    },
    {
        "title": "RML2SHACL: RDF Generation Taking Shape",
        "authors": "['Thomas Delva', 'Birte De Smedt', 'Sitt Min Oo', 'Dylan Van Assche', 'Sven Lieber', 'Anastasia Dimou']",
        "date": "December 2021",
        "source": "K-CAP '21: Proceedings of the 11th Knowledge Capture Conference",
        "abstract": "RDF graphs are often generated by mapping data in other (semi-)structured data formats to RDF. Such mapped graphs have a repetitive structure defined by (i) the mapping rules and (ii) the schema of the input sources. However, this information is not exploited beyond its original scope. SHACL was recently introduced to model constraints that RDF graphs should validate. SHACL shapes and their constraints are either manually defined or derived from ontologies or RDF graphs. We investigate a method to derive the shapes and their constraints from mapping rules, allowing the generation of the RDF graph and the corresponding shapes in one step. In this paper, we present RML2SHACL: an approach to generate SHACL shapes that validate RDF graphs defined by RML mapping rules. RML2SHACL relies on our proposed set of correspondences between RML and SHACL constructs. RML2SHACL covers a large variety of RML constructs, as proven by generating shapes for the RML test cases. A comparative analysis shows that shapes generated by RML2SHACL are similar to shapes generated by ontology-based tools, with a larger focus on data value-based constraints instead of schema-based constraints. We also found that RML2SHACL has a faster execution time than data-graph based approaches for data sizes of 90MB and higher.",
        "link": "https://dl.acm.org/doi/10.1145/3460210.3493562",
        "category": "Databases"
    },
    {
        "title": "Real-time Data Infrastructure at Uber",
        "authors": "['Yupeng Fu', 'Chinmay Soman']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457552",
        "category": "Databases"
    },
    {
        "title": "SIMD-MIMD cocktail in a hybrid memory glass: shaken, not stirred",
        "authors": "['Mikhail Zarubin', 'Patrick Damme', 'Alexander Krause', 'Dirk Habich', 'Wolfgang Lehner']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "Hybrid memory systems consisting of DRAM and NVRAM offer a great opportunity for column-oriented data systems to persistently store and to efficiently process columnar data completely in main memory. While vectorization (SIMD) of query operators is state-of-the-art to increase the single-thread performance, it has to be combined with thread-level parallelism (MIMD) to satisfy growing needs for higher performance and scalability. However, it is not well investigated how such a SIMD-MIMD interplay could be leveraged efficiently in hybrid memory systems. On the one hand, we deliver an extensive experimental evaluation of typical workloads on columnar data in this paper. We reveal that the choice of the most performant SIMD version differs greatly for both memory types. Moreover, we show that the throughput of concurrent queries can be boosted (up to 2x) when combining various SIMD flavors in a multi-threaded execution. On the other hand, to enable that optimization, we propose an adaptive SIMD-MIMD cocktail approach incurring only a negligible runtime overhead.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463782",
        "category": "Databases"
    },
    {
        "title": "WarVictimSampo 1914–1922: A National War Memorial on the Semantic Web for Digital Humanities Research and Applications",
        "authors": "['Heikki Rantala', 'Ilkka Jokipii', 'Esko Ikkala', 'Eero Hyvönen']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "This article presents the semantic portal and Linked Open Data service WarVictimSampo 1914–1922 about the war victims, battles, and prisoner camps in the Finnish Civil and other wars in 1914–1922. The system is based on a database of the National Archives of Finland and additional related data created, compiled, and linked during the project. The system contains detailed information about some 40,000 deaths extracted from several data sources and data about over 1,000 battles of the Civil War. A key novelty of WarVictimSampo 1914–1922 is the integration of ready-to-use Digital Humanities visualizations and data analysis tooling with semantic faceted search and data exploration, which allows, e.g., studying data about wider prosopographical groups in addition to individual war victims. The article focuses on demonstrating how the tools of the portal, as well as the underlying SPARQL endpoint openly available on the Web, can be used to explore and analyze war history in flexible and visual ways. WarVictimSampo 1914–1922 is a new member in the series of “Sampo” model-based semantic portals. The portal is in use and has had 23,000 users, including both war historians and the general public seeking information about their deceased relatives.",
        "link": "https://dl.acm.org/doi/10.1145/3477606",
        "category": "Databases"
    },
    {
        "title": "Disrupção do visível: ações participativas com as imagens na era dos bigdata",
        "authors": "['Fernanda Oliveira']",
        "date": "October 2021",
        "source": "ARTECH 2021: 10th International Conference on Digital and Interactive Arts",
        "abstract": "Vivemos em um ambiente contaminado pela presença algorítmica em nosso cotidiano, resultado de um hábito impulsionado pelo rápido avanço tecnológico ao longo da história. Ininterruptamente, banco de dados com capacidade de processamento gigantescos, os big data, são alimentados através de excessivos compartilhamentos de imagens e dados pessoais. Pensando a questão pela perspectiva pós-antropocêntrica das ruínas criadas pelas táticas do capital, onde grandes empresas, baseadas na leitura de presença por dados, passam a introduzir as pessoas num programa de captação monetizada das informações. Busco com esse texto relacionar projetos de autoria própria e coletiva, experiências artísticas construídas no exercício de romper com essa lógica automatizada da informação. Exploro nesses trabalhos aspectos de ações participativas, por intermédio de obras desenvolvidas através de práticas colaborativas, como uma maneira disruptiva de se relacionar com as imagens nessa era dos big data. We live in an environment contaminated by the presence of algorithms in our daily lives, the result of a habit driven by rapid technological advances throughout history. Uninterruptedly, databases with large processing power, the big data, are fed through excessive sharing of images and personal data. Thinking about the issue from the post-anthropocentric perspective of the ruins created by the tactics of capital, where large companies, based on the reading of presence through data, start to introduce people to a monetized information capture program. With this text, I seek to relate projects of my own and collective authorship, artistic experiences built in the exercise of breaking with this automated informational logic. In these works, I explore aspects of participatory actions, through works developed through collaborative practices, as a disruptive way of relating to images in this era of big data.",
        "link": "https://dl.acm.org/doi/10.1145/3483529.3483681",
        "category": "Databases"
    },
    {
        "title": "SQLRepair: identifying and repairing mistakes in student-authored SQL queries",
        "authors": "['Kai Presler-Marshall', 'Sarah Heckman', 'Kathryn T. Stolee']",
        "date": "May 2021",
        "source": "ICSE-JSEET '21: Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training",
        "abstract": "Computer science educators seek to understand the types of mistakes that students make when learning a new (programming) language so that they can help students avoid those mistakes in the future. While educators know what mistakes students regularly make in languages such as C and Python, students struggle with SQL and regularly make mistakes when working with it. We present an analysis of mistakes that students made when first working with SQL, classify the types of errors introduced, and provide suggestions on how to avoid them going forward. In addition, we present an automated tool, SQLRepair, that is capable of repairing errors introduced by undergraduate programmers when writing SQL queries. Our results show that students find repairs produced by our tool comparable in understandability to queries written by themselves or by other students, suggesting that SQL repair tools may be useful in an educational context. We also provide to the community a benchmark of SQL queries written by the students in our study that we used for evaluation of SQLRepair.",
        "link": "https://dl.acm.org/doi/10.1109/ICSE-SEET52601.2021.00030",
        "category": "Databases"
    },
    {
        "title": "Connected Histories of the BBC: Opening up the BBC Oral History Archive to the Digital Domain",
        "authors": "['Anna-Maria Sichani', 'David Hendy']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "This article describes the computational and data-related challenges of the “Connected Histories of the BBC” project, an interdisciplinary project aiming to bring into the public realm some of the hidden treasures of the BBC's own Oral History Archive through the creation of an openly accessible, fully searchable and interconnected digital catalogue of this archive. This project stands as an interesting case study on the tensions between “computational” and “archival”, by critically designing and employing computational approaches for an historical, complex Oral History collection of scattered analogue records of various forms with an archival pre-history. From data acquisition, modeling, structuring and enhancement, metadata, data analysis procedures, to web design and legal issues, this paper discusses the various computational challenges, processes and decisions made during this project, while showcasing the principles of (re)usability, accessibility, and collaboration throughout its course.",
        "link": "https://dl.acm.org/doi/10.1145/3480954",
        "category": "Databases"
    },
    {
        "title": "Davos: a system for interactive data-driven decision making",
        "authors": "['Zeyuan Shang', 'Emanuel Zgraggen', 'Benedetto Buratti', 'Philipp Eichmann', 'Navid Karimeddiny', 'Charlie Meyer', 'Wesley Runnels', 'Tim Kraska']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Recently, a new horizon in data analytics, prescriptive analytics, is becoming more and more important to make data-driven decisions. As opposed to the progress of democratizing data acquisition and access, making data-driven decisions remains a significant challenge for people without technical expertise. In this regard, existing tools for data analytics which were designed decades ago still present a high bar for domain experts, and removing this bar requires a fundamental rethinking of both interface and backend.At Einblick, an MIT/Brown spin-off based on the Northstar project, we have been building the next generation analytics tool in the last few years. To overcome the shortcomings of existing processing engines, we propose Davos, Einblick's novel backend. Davos combines aspects of progressive computation, approximate query processing and sampling, with a specific focus on supporting user-defined operations. Moreover, Davos optimizes multi-tenant scenarios to promote collaboration. Both empirical evaluation and user study verify that Davos can greatly empower data analytics for new needs.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476370",
        "category": "Databases"
    },
    {
        "title": "Not black-box anymore!: enabling analytics-aware optimizations in teradata vantage",
        "authors": "['Mohamed Eltabakh', 'Anantha Subramanian', 'Awny Al-Omari', 'Mohammed Al-Kateb', 'Sanjay Nair', 'Mahbub Hasan', 'Wellington Cabrera', 'Charles Zhang', 'Amit Kishore', 'Snigdha Prasad']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Teradata Vantage is a platform for integrating a broad range of analytical functions and capabilities with the Teradata's SQL engine. One of the main challenges in optimizing the execution of these analytical functions is that many of them are not only black boxes, but also have polymorphic nature, i.e., their behavior and properties may change depending on the invocation context. In this paper, we first demonstrate the inherent complexity in optimizing polymorphic functions, and then present the Vantage's Collaborative Optimizer, which is a cross-platform optimizer designed for optimizing the analytical functions invoked from within the SQL engine. The Collaborative Optimizer is the industry-first effort towards enabling analytics-aware optimizations over polymorphic analytical functions. We present a novel markup language-based approach for expressing the functions' polymorphic properties via a set of well-defined instructions. The Collaborative Optimizer uses these instructions at query time to infer the corresponding properties, and then decide on the applicable optimizations. From several possible optimizations, we showcase two core optimizations, namely \"projection push\" and \"predicate push\", which aim at optimizing the data movement to and from the analytical functions. The experiments using the Teradata-MLE analytical system demonstrate the expressiveness power and flexibility of the proposed markup language. Moreover, benchmark and real-world customer queries show the significant performance gain that the Collaborative Optimizer brings to the Vantage system.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476375",
        "category": "Databases"
    },
    {
        "title": "Cover or Pack: New Upper and Lower Bounds for Massively Parallel Joins",
        "authors": "['Xiao Hu']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "This paper considers the worst-case complexity of multi-round join evaluation in the Massively Parallel Computation (MPC) model. Unlike the sequential RAM model, in which there is a unified optimal algorithm based on the AGM bound for all join queries, worst-case optimal algorithms have been achieved on a very restrictive class of joins in the MPC model. The only known lower bound is still derived from the AGM bound, in terms of the optimal fractional edge covering number of the query. In this work, we make efforts towards bridging this gap. We design an instance-dependent algorithm for the class of α-acyclic join queries. In particular, when the maximum size of input relations is bounded, this complexity has a closed form in terms of the optimal fractional edge covering number of the query, which is worst-case optimal. Beyond acyclic joins, we surprisingly find that the optimal fractional edge covering number does not lead to a tight lower bound. More specifically, we prove for a class of cyclic joins a higher lower bound in terms of the optimal fractional edge packing number of the query, which is matched by existing algorithms, thus optimal. This new result displays a significant distinction for join evaluation, not only between acyclic and cyclic joins, but also between the fine-grained RAM and coarse-grained MPC model.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458319",
        "category": "Databases"
    },
    {
        "title": "Conditional Cuckoo Filters",
        "authors": "['Daniel Ting', 'Rick Cole']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Bloom filters, cuckoo filters, and other approximate set membership sketches have a wide range of applications. Oftentimes, expensive operations can be skipped if an item is not in a data set. These filters provide an inexpensive, memory efficient way to test if an item is in a set and avoid unnecessary operations. Existing sketches only allow membership testing for a single set. However, in some applications such as join processing, the relevant set is not fixed and is determined by a set of predicates. We propose the Conditional Cuckoo Filter, a simple modification of the cuckoo filter that allows for set membership testing given predicates on a pre-computed sketch. This filter also introduces a novel chaining technique that enables cuckoo filters to handle insertion of duplicate keys. We evaluate our methods on a join processing application and show that they significantly reduce the number of tuples that a join must process.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452811",
        "category": "Databases"
    },
    {
        "title": "Verifiable Query Processing Over Outsourced Social Graph",
        "authors": "['Xin Yao', 'Rui Zhang', 'Dingquan Huang', 'Yanchao Zhang']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Social data outsourcing is an emerging paradigm for effective and efficient access to the social data. In such a system, a third-party Social Data Provider (SDP) purchases social network datasets from Online Social Network (OSN) operators and then resells them to data consumers who can be any individuals or entities desiring social data through query interfaces. The SDP cannot be fully trusted and may return forged or incomplete query results to data consumers for various reasons, e.g., in favor of the businesses willing to pay. In this paper, we initiate the study on verifiable query processing over outsourced social graph whereby a data consumer can verify both the integrity and completeness of any query result returned by an untrusted SDP. We propose three schemes for single-attribute queries and another scheme for multi-attribute queries over outsourced social data. The four schemes all require the OSN provider to generate some cryptographic auxiliary information, based on which the SDP can construct a verification object to allow the data consumer to verify the integrity and completeness of the query result. They, however, differ in how the auxiliary information is generated and how the verification object is constructed and verified. Detailed analysis and extensive experiments using a real Twitter dataset confirm the efficacy and efficiency of the proposed schemes.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3085574",
        "category": "Databases"
    },
    {
        "title": "FW-KV: improving read guarantees in PSI",
        "authors": "['Masoomeh Javidi Kishi', 'Roberto Palmieri']",
        "date": "December 2021",
        "source": "Middleware '21: Proceedings of the 22nd International Middleware Conference",
        "abstract": "We present FW-KV, a novel distributed transactional in-memory key-value store that guarantees the Parallel Snapshot Isolation (PSI) correctness level. FW-KV's primary goal is to allow its read-only transactions to access more up-to-date (fresher) versions of objects than Walter, the state-of-the-art implementation of PSI. FW-KV achieves that without assuming synchrony or a synchronized clock service. The improved level of freshness comes at no significant performance degradation, especially in low contention workloads, as assessed by our evaluation study including two standard OLTP benchmarks, YCSB and TPC-C. The performance gap between FW-KV and Walter is less than 5% in low contention scenarios, and less than 28% in high contention.",
        "link": "https://dl.acm.org/doi/10.1145/3464298.3476131",
        "category": "Databases"
    },
    {
        "title": "LifeStream: a high-performance stream processing engine for periodic streams",
        "authors": "['Anand Jayarajan', 'Kimberly Hau', 'Andrew Goodwin', 'Gennady Pekhimenko']",
        "date": "April 2021",
        "source": "ASPLOS '21: Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Hospitals around the world collect massive amounts of physiological data from their patients every day. Recently, there has been an increase in research interest to subject this data to statistical analysis to gain more insights and provide improved medical diagnoses. Such analyses require complex computations on large volumes of data, demanding efficient data processing systems. This paper shows that currently available data processing solutions either fail to meet the performance requirements or lack simple and flexible programming interfaces. To address this problem, we propose LifeStream, a high-performance stream processing engine for physiological data. LifeStream hits the sweet spot between ease of programming by providing a rich temporal query language support and performance by employing optimizations that exploit the periodic nature of physiological data. We demonstrate that LifeStream achieves end-to-end performance up to 7.5× higher than state-of-the-art streaming engines and 3.2× than hand-optimized numerical libraries on real-world datasets and workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3445814.3446725",
        "category": "Databases"
    },
    {
        "title": "The Raptor Join Operator for Processing Big Raster + Vector Data",
        "authors": "['Samriddhi Singla', 'Ahmed Eldawy', 'Tina Diao', 'Ayan Mukhopadhyay', 'Elia Scudiero']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "Pre-processing spatial data for machine learning applications often includes combining different datasets into a form usable by the machine learning algorithms. Spatial data is generally available in two representations, raster and vector. The best data science and machine learning applications need to combine multiple datasets of both representations which is a data and compute intensive problem. This paper proposes a formal raster-vector join operator, Raptor Join, that can bridge the gap between raster and vector data. It is modeled as a relational join operator in Spark that can be easily combined with other operators, while also offering the advantage of in-situ processing. To implement the Raptor join operator efficiently, we propose a novel Flash index that has a low memory requirement and can process the entire operation with one data scan. We run an extensive experimental evaluation on large scale satellite data with up-to a trillion pixels, and big vector data with up-to hundreds of millions of segments and billions of points, and show that the proposed method can scale to big data with up-to three orders of magnitude performance gain over baselines.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3483971",
        "category": "Databases"
    },
    {
        "title": "Predicting SPARQL Query Dynamics",
        "authors": "['Alberto Moya Loustaunau', 'Aidan Hogan']",
        "date": "December 2021",
        "source": "K-CAP '21: Proceedings of the 11th Knowledge Capture Conference",
        "abstract": "Given historical versions of an RDF graph, we propose and compare several methods to predict whether or not the results of a SPARQL query will change for the next version. Unsurprisingly, we find that the best results for this task are achievable by considering the full history of results for the query over previous versions of the graph. However, given a previously unseen query, producing historical results requires costly offline maintenance of previous versions of the data, and costly online computation of the query results over these previous versions. This prompts us to explore more lightweight alternatives that rely on features computed from the query and statistical summaries of historical versions of the graph. We evaluate the quality of the predictions produced over weekly snapshots of Wikidata and daily snapshots of DBpedia. Our results provide insights into the trade-offs for predicting SPARQL query dynamics, where we find that a detailed history of changes for a query's results enables much more accurate predictions, but has higher overhead versus more lightweight alternatives.",
        "link": "https://dl.acm.org/doi/10.1145/3460210.3493565",
        "category": "Databases"
    },
    {
        "title": "NASCENT2: Generic Near-Storage Sort Accelerator for Data Analytics on SmartSSD",
        "authors": "['Sahand Salamat', 'Hui Zhang', 'Yang Seok Ki', 'Tajana Rosing']",
        "date": "None",
        "source": "ACM Transactions on Reconfigurable Technology and Systems",
        "abstract": "As the size of data generated every day grows dramatically, the computational bottleneck of computer systems has shifted toward storage devices. The interface between the storage and the computational platforms has become the main limitation due to its limited bandwidth, which does not scale when the number of storage devices increases. Interconnect networks do not provide simultaneous access to all storage devices and thus limit the performance of the system when executing independent operations on different storage devices. Offloading the computations to the storage devices eliminates the burden of data transfer from the interconnects. Near-storage computing offloads a portion of computations to the storage devices to accelerate big data applications. In this article, we propose a generic near-storage sort accelerator for data analytics, NASCENT2, which utilizes Samsung SmartSSD, an NVMe flash drive with an on-board FPGA chip that processes data in situ.NASCENT2 consists of dictionary decoder, sort, and shuffle FPGA-based accelerators to support sorting database tables based on a key column with any arbitrary data type. It exploits data partitioning applied by data processing management systems, such as SparkSQL, to breakdown the sort operations on colossal tables to multiple sort operations on smaller tables. NASCENT2 generic sort provides 2 × speedup and 15.2 × energy efficiency improvement as compared to the CPU baseline. It moreover considers the specifications of the SmartSSD (e.g., the FPGA resources, interconnect network, and solid-state drive bandwidth) to increase the scalability of computer systems as the number of storage devices increases. With 12 SmartSSDs, NASCENT2 is 9.9× (137.2 ×) faster and 7.3 × (119.2 ×) more energy efficient in sorting the largest tables of TPCC and TPCH benchmarks than the FPGA (CPU) baseline.",
        "link": "https://dl.acm.org/doi/10.1145/3472769",
        "category": "Databases"
    },
    {
        "title": "Model Counting meets F0 Estimation",
        "authors": "['A. Pavan', 'N. V. Vinodchandran', 'Arnab Bhattacharya', 'Kuldeep S. Meel']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Constraint satisfaction problems (CSP's) and data stream models are two powerful abstractions to capture a wide variety of problems arising in different domains of computer science. Developments in the two communities have mostly occurred independently and with little interaction between them. In this work, we seek to investigate whether bridging the seeming communication gap between the two communities may pave the way to richer fundamental insights. To this end, we focus on two foundational problems: model counting for CSP's and computation of zeroth frequency moments F0 for data streams. Our investigations lead us to observe striking similarity in the core techniques employed in the algorithmic frameworks that have evolved separately for model counting and F0 computation. We design a recipe for translation of algorithms developed for F0 estimation to that of model counting, resulting in new algorithms for model counting. We then observe that algorithms in the context of distributed streaming can be transformed to distributed algorithms for model counting. We next turn our attention to viewing streaming from the lens of counting and show that framing F0 estimation as a special case of #DNF counting allows us to obtain a general recipe for a rich class of streaming problems, which had been subjected to case-specific analysis in prior works. In particular, our view yields a state-of-the art algorithm for multidimensional range efficient F0 estimation with a simpler analysis.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458311",
        "category": "Databases"
    },
    {
        "title": "On the Root of Trust Identification Problem",
        "authors": "['Ivan De Oliveira Nunes', 'Xuhua Ding', 'Gene Tsudik']",
        "date": "May 2021",
        "source": "IPSN '21: Proceedings of the 20th International Conference on Information Processing in Sensor Networks (co-located with CPS-IoT Week 2021)",
        "abstract": "Trusted Execution Environments (TEEs) are becoming ubiquitous and are currently used in many security applications: from personal IoT gadgets to banking and databases. Prominent examples of such architectures are Intel SGX, ARM TrustZone, and Trusted Platform Modules (TPMs). A typical TEE relies on a dynamic Root of Trust (RoT) to provide security services such as code/data confidentiality and integrity, isolated secure software execution, remote attestation, and sensor auditing. Despite their usefulness, there is currently no secure means to determine whether a given security service or task is being performed by the particular RoT within a specific physical device. We refer to this as the Root of Trust Identification (RTI) problem and discuss how it inhibits security for applications such as sensing and actuation. We formalize the RTI problem and argue that security of RTI protocols is especially challenging due to local adversaries, cuckoo adversaries, and the combination thereof. To cope with this problem we propose a simple and effective protocol based on biometrics. Unlike biometric-based user authentication, our approach is not concerned with verifying user identity, and requires neither pre-enrollment nor persistent storage for biometric templates. Instead, it takes advantage of the difficulty of cloning a biometric in realtime to securely identify the RoT of a given physical device, by using the biometric as a challenge. Security of the proposed protocol is analyzed in the combined Local and Cuckoo adversarial model. Also, a prototype implementation is used to demonstrate the protocol's feasibility and practicality. We further propose a Proxy RTI protocol, wherein a previously identified RoT assists a remote verifier in identifying new RoTs.",
        "link": "https://dl.acm.org/doi/10.1145/3412382.3458274",
        "category": "Databases"
    },
    {
        "title": "Cache-Efficient Fork-Processing Patterns on Large Graphs",
        "authors": "['Shengliang Lu', 'Shixuan Sun', 'Johns Paul', 'Yuchen Li', 'Bingsheng He']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "As large graph processing emerges, we observe a costly fork-processing pattern (FPP) that is common in many graph algorithms. The unique feature of the FPP is that it launches many independent queries from different source vertices on the same graph. For example, an algorithm in analyzing the network community profile can execute Personalized PageRanks that start from tens of thousands of source vertices at the same time. We study the efficiency of handling FPPs in state-of-the-art graph processing systems on multi-core architectures, including Ligra, Gemini, and GraphIt. We find that those systems suffer from severe cache miss penalty because of the irregular and uncoordinated memory accesses in processing FPPs. In this paper, we propose ForkGraph, a cache-efficient FPP processing system on multi-core architectures. In order to improve the cache reuse, we divide the graph into partitions each sized of LLC (last-level cache) capacity, and the queries in an FPP are buffered and executed on the partition basis. We further develop efficient intra- and inter-partition execution strategies for efficiency. For intra-partition processing, since the graph partition fits into LLC, we propose to execute each graph query with efficient sequential algorithms (in contrast with parallel algorithms in existing parallel graph processing systems) and present an atomic-free query processing method by consolidating contending operations to cache-resident graph partition. For inter-partition processing, we propose two designs, yielding and priority-based scheduling, to reduce redundant work in processing. Besides, we theoretically prove that ForkGraph performs the same amount of work, to within a constant factor, as the fastest known sequential algorithms in FPP queries processing, which is work efficient. Our evaluations on real-world graphs show that ForkGraph significantly outperforms state-of-the-art graph processing systems (including Ligra, Gemini, and GraphIt) with two orders of magnitude speedups.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457253",
        "category": "Databases"
    },
    {
        "title": "To Not Miss the Forest for the Trees - A Holistic Approach for Explaining Missing Answers over Nested Data",
        "authors": "['Ralf Diestelkämper', 'Seokki Lee', 'Melanie Herschel', 'Boris Glavic']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Query-based explanations for missing answers identify which operators of a query are responsible for the failure to return a missing answer of interest. This type of explanations has proven useful, e.g., to debug complex analytical queries. Such queries are frequent in big data systems such as Apache Spark. We present a novel approach to produce query-based explanations. It is the first to support nested data and to consider operators that modify the schema and structure of the data (e.g., nesting, projections) as potential causes of missing answers. To efficiently compute explanations, we propose a heuristic algorithm that applies two novel techniques: (i) reasoning about multiple schema alternatives for a query and (ii) re-validating at each step whether an intermediate result can contribute to the missing answer. Using an implementation on Spark, we demonstrate that our approach is the first to scale to large datasets while often finding explanations that existing techniques fail to identify.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457249",
        "category": "Databases"
    },
    {
        "title": "OWSP-Miner: Self-adaptive One-off Weak-gap Strong Pattern Mining",
        "authors": "['Youxi Wu', 'Xiaohui Wang', 'Yan Li', 'Lei Guo', 'Zhao Li', 'Ji Zhang', 'Xindong Wu']",
        "date": "None",
        "source": "ACM Transactions on Management Information Systems",
        "abstract": "Gap constraint sequential pattern mining (SPM), as a kind of repetitive SPM, can avoid mining too many useless patterns. However, this method is difficult for users to set a suitable gap without prior knowledge and each character is considered to have the same effects. To tackle these issues, this article addresses a self-adaptive One-off Weak-gap Strong Pattern (OWSP) mining, which has three characteristics. First, it determines the gap constraint adaptively according to the sequence. Second, all characters are divided into two groups: strong and weak characters, and the pattern is composed of strong characters, while weak characters are allowed in the gaps. Third, each character can be used at most once in the process of support (the frequency of pattern) calculation. To handle this problem, this article presents OWSP-Miner, which equips with two key steps: support calculation and candidate pattern generation. A reverse-order filling strategy is employed to calculate the support of a candidate pattern, which reduces the time complexity. OWSP-Miner generates candidate patterns using pattern join strategy, which effectively reduces the candidate patterns. For clarification, time series is employed in the experiments and the results show that OWSP-Miner is not only more efficient but also is easier to mine valuable patterns. In the experiment of stock application, we also employ OWSP-Miner to mine OWSPs and the results show that OWSPs mining is more meaningful in real life. The algorithms and data can be downloaded at https://github.com/wuc567/Pattern-Mining/tree/master/OWSP-Miner.",
        "link": "https://dl.acm.org/doi/10.1145/3476247",
        "category": "Databases"
    },
    {
        "title": "InFeMo: Flexible Big Data Management Through a Federated Cloud System",
        "authors": "['Christos L. Stergiou', 'Konstantinos E. Psannis', 'Brij B. Gupta']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "This paper introduces and describes a novel architecture scenario based on Cloud Computing and counts on the innovative model of Federated Learning. The proposed model is named Integrated Federated Model, with the acronym InFeMo. InFeMo incorporates all the existing Cloud models with a federated learning scenario, as well as other related technologies that may have integrated use with each other, offering a novel integrated scenario. In addition to this, the proposed model is motivated to deliver a more energy efficient system architecture and environment for the users, which aims to the scope of data management. Also, by applying the InFeMo the user would have less waiting time in every procedure queue. The proposed system was built on the resources made available by Cloud Service Providers (CSPs) and by using the PaaS (Platform as a Service) model, in order to be able to handle user requests better and faster. This research tries to fill a scientific gap in the field of federated Cloud systems. Thus, taking advantage of the existing scenarios of FedAvg and CO-OP, we were keen to end up with a new federated scenario that merges these two algorithms, and aiming for a more efficient model that is able to select, depending on the occasion, if it “trains” the model locally in client or globally in server.",
        "link": "https://dl.acm.org/doi/10.1145/3426972",
        "category": "Databases"
    },
    {
        "title": "Computing and Maintaining Provenance of Query Result Probabilities in Uncertain Knowledge Graphs",
        "authors": "['Garima Gaur', 'Abhishek Dang', 'Arnab Bhattacharya', 'Srikanta Bedathur']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Knowledge graphs (KG) model relationships between entities as labeled edges (or facts). They are mostly constructed using a suite of automated extractors, thereby inherently leading to uncertainty in the extracted facts. Modeling the uncertainty as probabilistic confidence scores results in a probabilistic knowledge graph. Graph queries over such probabilistic KGs require answer computation along with the computation of result probabilities, i.e., probabilistic inference. We propose a system, HAPPI (How Provenance of Probabilistic Inference), to handle such query processing and inference. Complying with the standard provenance semiring model, we propose a novel commutative semiring to symbolically compute the probability of the result of a query. These provenance-polynomial-like symbolic expressions encode fine-grained information about the probability computation process. We leverage this encoding to efficiently compute as well as maintain probabilities of results even as the underlying KG changes. Focusing on conjunctive basic graph pattern queries, we observe that HAPPI is more efficient than knowledge compilation for answering commonly occurring queries with lower range of probability derivation complexity. We propose an adaptive system that leverages the strengths of both HAPPI and compilation based techniques, for not only to perform efficient probabilistic inference and compute their provenance, but also to incrementally maintain them.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3482330",
        "category": "Databases"
    },
    {
        "title": "Tuplex: Data Science in Python at Native Code Speed",
        "authors": "['Leonhard Spiegelberg', 'Rahul Yesantharao', 'Malte Schwarzkopf', 'Tim Kraska']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Today's data science pipelines often rely on user-defined functions (UDFs) written in Python. But interpreted Python code is slow, and Python UDFs cannot be compiled to machine code easily. We present Tuplex, a new data analytics framework that just in-time compiles developers' natural Python UDFs into efficient, end-to-end optimized native code. Tuplex introduces a novel dual-mode execution model that compiles an optimized fast path for the common case, and falls back on slower exception code paths for data that fail to match the fast path's assumptions. Dual-mode execution is crucial to making end-to-end optimizing compilation tractable: by focusing on the common case, Tuplex keeps the code simple enough to apply aggressive optimizations. Thanks to dual-mode execution, Tuplex pipelines always complete even if exceptions occur, and Tuplex's post-facto exception handling simplifies debugging. We evaluate Tuplex with data science pipelines over real-world datasets. Compared to Spark and Dask, Tuplex improves end-to-end pipeline runtime by 5-91x and comes within 1.1-1.7x of a hand-optimized C++ baseline. Tuplex outperforms other Python compilers by 6x and competes with prior, more limited query compilers. Optimizations enabled by dual-mode processing improve runtime by up to 3x, and Tuplex performs well in a distributed setting on serverless functions.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457244",
        "category": "Databases"
    },
    {
        "title": "Bypassing Push-based Second Factor and Passwordless Authentication with Human-Indistinguishable Notifications",
        "authors": "['Mohammed Jubur', 'Prakash Shrestha', 'Nitesh Saxena', 'Jay Prakash']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Second factor (2FA) or passwordless authentication based on notifications pushed to a user's personal device (e.g., a phone) that the user can simply approve (or deny) has become widely popular due to its convenience. In this paper, we show that the effortlessness of this approach gives rise to a fundamental design vulnerability. The vulnerability stems from the fact that the notification, as shown to the user, is not uniquely bound to the user's login session running through the browser, and thus if two notifications are sent around the same time (one for the user's session and one for an attacker's session), the user may not be able to distinguish between the two, likely ending up accepting the notification of the attacker's session. Exploiting this vulnerability, we present HIENA, a simple yet devastating attack against such \"one-push\" 2FA or passwordless schemes, which can allow a malicious actor to login soon after the victim user attempts to login triggering multiple near-concurrent notifications that seem indistinguishable to the user. To further deceive the user into accepting the attacker-triggered notification, HIENA can optionally spoof/mimic the victim's client machine information (e.g., the city from which the victim logs in, by being in the same city) and even issue other third-party notifications (e.g., email or social media) for obfuscation purposes. In case of 2FA schemes, we assume that the attacker knows the victim's password (e.g., obtained via breached password databases), a standard methodology to evaluate the security of any 2FA scheme. To evaluate the effectiveness of HIENA, we carefully designed and ran a human factors lab study where we tested benign and adversarial settings mimicking the user interface designs of well-known one-push 2FA and passwordless schemes. Our results show that users are prone to accepting attacker's notification in HIENA with high rates, about 83% overall and about 99% upon using spoofed information, which is almost similar to the rates of acceptance of benign login sessions. Even for the non-spoofed sessions (our primary attack), the attack success rates are about 68%, which go up to about 90-97% if the attack attempt is repeated 2-3 times. While we did not see a statistically significant effect of using third-party notifications on attack success rate, in real-life, the use of such obfuscation can be quite effective as users may only see one single 2FA notification (corresponding to attacker's session) on top of the notifications list which is most likely to be accepted. We have verified that many widely deployed one-push 2FA schemes (e.g., Duo Push, Authy OneTouch, LastPass, Facebook's and OpenOTP) seem directly vulnerable to our attack.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453084",
        "category": "Databases"
    },
    {
        "title": "Minimum Coresets for Maxima Representation of Multidimensional Data",
        "authors": "['Yanhao Wang', 'Michael Mathioudakis', 'Yuchen Li', 'Kian-Lee Tan']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Coresets are succinct summaries of large datasets such that, for a given problem, the solution obtained from a coreset is provably competitive with the solution obtained from the full dataset. As such, coreset-based data summarization techniques have been successfully applied to various problems, e.g., geometric optimization, clustering, and approximate query processing, for scaling them up to massive data. In this paper, we study coresets for the maxima representation of multidimensional data: Given a set P of points in $ \\mathbbR ^d $, where d is a small constant, and an error parameter $ \\varepsilon \\in (0,1) $, a subset $ Q \\subseteq P $ is an $ \\varepsilon $-coreset for the maxima representation of P iff the maximum of Q is an $ \\varepsilon $-approximation of the maximum of P for any vector $ u \\in \\mathbbR ^d $, where the maximum is taken over the inner products between the set of points (P or Q) and u. We define a novel minimum $\\varepsilon$-coreset problem that asks for an $\\varepsilon$-coreset of the smallest size for the maxima representation of a point set. For the two-dimensional case, we develop an optimal polynomial-time algorithm for the minimum $ \\varepsilon $-coreset problem by transforming it into the shortest-cycle problem in a directed graph. Then, we prove that this problem is NP-hard in three or higher dimensions and present polynomial-time approximation algorithms in an arbitrary fixed dimension. Finally, we provide extensive experimental results on both real and synthetic datasets to demonstrate the superior performance of our proposed algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458322",
        "category": "Databases"
    },
    {
        "title": "High-Dimensional Bayesian Optimization with Multi-Task Learning for RocksDB",
        "authors": "['Sami Alabed', 'Eiko Yoneki']",
        "date": "April 2021",
        "source": "EuroMLSys '21: Proceedings of the 1st Workshop on Machine Learning and Systems",
        "abstract": "RocksDB is a general-purpose embedded key-value store used in multiple different settings. Its versatility comes at the cost of complex tuning configurations. This paper investigates maximizing the throughput of RocksDB 10 operations by auto-tuning ten parameters of varying ranges. Off-the-shelf optimizers struggle with high-dimensional problem spaces and require a large number of training samples. We propose two techniques to tackle this problem: multitask modeling and dimensionality reduction through clustering. By incorporating adjacent optimization in the model, the model converged faster and found complicated settings that other tuners could not find. This approach had an additional computational complexity overhead, which we mitigated by manually assigning parameters to each sub-goal through our knowledge of RocksDB. The model is then incorporated in a standard Bayesian Optimization loop to find parameters that maximize RocksDB's 10 throughput. Our method achieved x1.3 improvement when bench-marked against a simulation of Facebook's social graph traffic, and converged in ten optimization steps compared to other state-of-the-art methods that required fifty steps.",
        "link": "https://dl.acm.org/doi/10.1145/3437984.3458841",
        "category": "Databases"
    },
    {
        "title": "System-aware dynamic partitioning for batch and streaming workloads",
        "authors": "['Zoltán Zvara', 'Péter G. N. Szabó', 'Balázs Barnabás Lóránt', 'András A. Benczúr']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "When processing data streams with highly skewed and nonstationary key distributions, we often observe overloaded partitions when the hash partitioning fails to balance data correctly. To avoid slow tasks that delay the completion of the whole stage of computation, it is necessary to apply adaptive, on-the-fly partitioning that continuously recomputes an optimal partitioner, given the observed key distribution. While such solutions exist for batch processing of static data sets and stateless stream processing, the task is difficult for long-running stateful streaming jobs where key distribution changes over time. Careful checkpointing and operator state migration is necessary to change the partitioning while the operation is running. Our key result is a lightweight on-the-fly Dynamic Repartitioning (DR) module for distributed data processing systems (DDPS), including Apache Spark and Flink, which improves the performance with negligible overhead. DR can adaptively repartition data during execution using our Key Isolator Partitioner (KIP). In our experiments with real workloads and power-law distributions, we reach a speedup of 1.5-6 for a variety of Spark and Flink jobs.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494087",
        "category": "Databases"
    },
    {
        "title": "Finding data compatibility bugs with JSON subschema checking",
        "authors": "['Andrew Habib', 'Avraham Shinnar', 'Martin Hirzel', 'Michael Pradel']",
        "date": "July 2021",
        "source": "ISSTA 2021: Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "abstract": "JSON is a data format used pervasively in web APIs, cloud computing, NoSQL databases, and increasingly also machine learning. To ensure that JSON data is compatible with an application, one can define a JSON schema and use a validator to check data against the schema. However, because validation can happen only once concrete data occurs during an execution, it may detect data compatibility bugs too late or not at all. Examples include evolving the schema for a web API, which may unexpectedly break client applications, or accidentally running a machine learning pipeline on incorrect data. This paper presents a novel way of detecting a class of data compatibility bugs via JSON subschema checking. Subschema checks find bugs before concrete JSON data is available and across all possible data specified by a schema. For example, one can check if evolving a schema would break API clients or if two components of a machine learning pipeline have incompatible expectations about data. Deciding whether one JSON schema is a subschema of another is non-trivial because the JSON Schema specification language is rich. Our key insight to address this challenge is to first reduce the richness of schemas by canonicalizing and simplifying them, and to then reason about the subschema question on simpler schema fragments using type-specific checkers. We apply our subschema checker to thousands of real-world schemas from different domains. In all experiments, the approach is correct whenever it gives an answer (100% precision and correctness), which is the case for most schema pairs (93.5% recall), clearly outperforming the state-of-the-art tool. Moreover, the approach reveals 43 previously unknown bugs in popular software, most of which have already been fixed, showing that JSON subschema checking helps finding data compatibility bugs early.",
        "link": "https://dl.acm.org/doi/10.1145/3460319.3464796",
        "category": "Databases"
    },
    {
        "title": "On the limits of machine knowledge: completeness, recall and negation in web-scale knowledge bases",
        "authors": "['Simon Razniewski', 'Hiba Arnaout', 'Shrestha Ghosh', 'Fabian Suchanek']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "General-purpose knowledge bases (KBs) are an important component of several data-driven applications. Pragmatically constructed from available web sources, these KBs are far from complete, which poses a set of challenges in curation as well as consumption.In this tutorial we discuss how completeness, recall and negation in DBs and KBs can be represented, extracted, and inferred. We proceed in 5 parts: (i) We introduce the logical foundations of knowledge representation and querying under partial closed-world semantics. (ii) We show how information about recall can be identified in KBs and in text, and (iii) how it can be estimated via statistical patterns. (iv) We show how interesting negative statements can be identified, and (v) how recall can be targeted in a comparative notion.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476401",
        "category": "Databases"
    },
    {
        "title": "Multi-relation Graph Summarization",
        "authors": "['Xiangyu Ke', 'Arijit Khan', 'Francesco Bonchi']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Graph summarization is beneficial in a wide range of applications, such as visualization, interactive and exploratory analysis, approximate query processing, reducing the on-disk storage footprint, and graph processing in modern hardware. However, the bulk of the literature on graph summarization surprisingly overlooks the possibility of having edges of different types. In this article, we study the novel problem of producing summaries of multi-relation networks, i.e., graphs where multiple edges of different types may exist between any pair of nodes. Multi-relation graphs are an expressive model of real-world activities, in which a relation can be a topic in social networks, an interaction type in genetic networks, or a snapshot in temporal graphs.The first approach that we consider for multi-relation graph summarization is a two-step method based on summarizing each relation in isolation, and then aggregating the resulting summaries in some clever way to produce a final unique summary. In doing this, as a side contribution, we provide the first polynomial-time approximation algorithm based on the k-Median clustering for the classic problem of lossless single-relation graph summarization.Then, we demonstrate the shortcomings of these two-step methods, and propose holistic approaches, both approximate and heuristic algorithms, to compute a summary directly for multi-relation graphs. In particular, we prove that the approximation bound of k-Median clustering for the single relation solution can be maintained in a multi-relation graph with proper aggregation operation over adjacency matrices corresponding to its multiple relations. Experimental results and case studies (on co-authorship networks and brain networks) validate the effectiveness and efficiency of the proposed algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3494561",
        "category": "Databases"
    },
    {
        "title": "SetSketch: filling the gap between MinHash and HyperLogLog",
        "authors": "['Otmar Ertl']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "MinHash and HyperLogLog are sketching algorithms that have become indispensable for set summaries in big data applications. While HyperLogLog allows counting different elements with very little space, MinHash is suitable for the fast comparison of sets as it allows estimating the Jaccard similarity and other joint quantities. This work presents a new data structure called SetSketch that is able to continuously fill the gap between both use cases. Its commutative and idempotent insert operation and its mergeable state make it suitable for distributed environments. Fast, robust, and easy-to-implement estimators for cardinality and joint quantities, as well as the ability to use SetSketch for similarity search, enable versatile applications. The presented joint estimator can also be applied to other data structures such as MinHash, HyperLogLog, or Hyper-MinHash, where it even performs better than the corresponding state-of-the-art estimators in many cases.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476276",
        "category": "Databases"
    },
    {
        "title": "Distributed Stream KNN Join",
        "authors": "['Amirhesam Shahvarani', 'Hans-Arno Jacobsen']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "kNN join over data streams is an important operation for location-aware systems, which correlates events from different sources based on their occurrence locations. Combining the complexity of kNN join and the dynamicity of data streams, kNN join in streaming environments is a computationally intensive operator, and its performance can be greatly improved by utilizing the computational capabilities of modern non-uniform memory access (NUMA) computing platforms. However, the conventional approaches to kNN join for prestored datasets do not work efficiently with the kind of highly dynamic data found in streaming environments. Therefore, in this paper, we introduce an adaptive scalable stream kNN join, named ADS-kNN, to address the challenges of performing the kNN join operation on highly dynamic data. We propose a multistage kNN execution plan that enables high-performance kNN queries in distributed settings by overlapping the computation and communication stages. Moreover, we propose an adaptive data partitioning scheme that dynamically adjusts the load among the operators according to the changes in the input values. Combining these two techniques, ADS-kNN provides a scalable and adaptive kNN join operator for data streams. Our experiments using a 56-core system show that ADS-kNN achieves a maximum throughput that is 21 times higher than that of a single-threaded approach.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457269",
        "category": "Databases"
    },
    {
        "title": "Nova-LSM: A Distributed, Component-based LSM-tree Key-value Store",
        "authors": "['Haoyu Huang', 'Shahram Ghandeharizadeh']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The cloud infrastructure motivates disaggregation of monolithic data stores into components that are assembled together based on an application's workload. This study investigates disaggregation of an LSM-tree key-value store into components that communicate using RDMA. These components separate storage from processing, enabling processing components to share storage bandwidth and space. The processing components scatter blocks of a file (SSTable) across an arbitrary number of storage components and balance load across them using power-of-d. They construct ranges dynamically at runtime to parallelize compaction and enhance performance. Each component has configuration knobs that control its scalability. The resulting component-based system, Nova-LSM, is elastic. It outperforms its monolithic counterparts, both LevelDB and RocksDB, by several orders of magnitude with workloads that exhibit a skewed pattern of access to data.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457297",
        "category": "Databases"
    },
    {
        "title": "Cquirrel: continuous query processing over acyclic relational schemas",
        "authors": "['Qichen Wang', 'Chaoqi Zhang', 'Danish Alsayed', 'Ke Yi', 'Bin Wu', 'Feifei Li', 'Chaoqun Zhan']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We will demonstrate Cquirrel, a continuous query processing engine built on top of Flink. Cquirrel assumes a relational schema where the foreign-key constraints form a directed acyclic graph, and supports any selection-projection-join-aggregation query where all join conditions are between a primary key and a foreign key. It allows arbitrary updates to any of the relations, and outputs the deltas in the query answers in real-time. It provides much better support for multi-way joins than the native join operator in Flink. Meanwhile, it offers better performance, scalability, and fault tolerance than other continuous query processing engines.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476315",
        "category": "Databases"
    },
    {
        "title": "Babelfish: efficient execution of polyglot queries",
        "authors": "['Philipp Marian Grulich', 'Steffen Zeuch', 'Volker Markl']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Today's users of data processing systems come from different domains, have different levels of expertise, and prefer different programming languages. As a result, analytical workload requirements shifted from relational to polyglot queries involving user-defined functions (UDFs). Although some data processing systems support polyglot queries, they often embed third-party language runtimes. This embedding induces a high performance overhead, as it causes additional data materialization between execution engines.In this paper, we present Babelfish, a novel data processing engine designed for polyglot queries. Babelfish introduces an intermediate representation that unifies queries from different implementation languages. This enables new, holistic optimizations across operator and language boundaries, e.g., operator fusion and workload specialization. As a result, Babelfish avoids data transfers and enables efficient utilization of hardware resources. Our evaluation shows that Babelfish outperforms state-of-the-art data processing systems by up to one order of magnitude and reaches the performance of handwritten code. With Babelfish, we bridge the performance gap between relational and multi-language UDFs and lay the foundation for the efficient execution of future polyglot workloads.",
        "link": "https://dl.acm.org/doi/10.14778/3489496.3489501",
        "category": "Databases"
    },
    {
        "title": "UDF to SQL translation through compositional lazy inductive synthesis",
        "authors": "['Guoqiang Zhang', 'Yuanchao Xu', 'Xipeng Shen', 'Işıl Dillig']",
        "date": "None",
        "source": "Proceedings of the ACM on Programming Languages",
        "abstract": "Many data processing systems allow SQL queries that call user-defined functions (UDFs) written in conventional programming languages. While such SQL extensions provide convenience and flexibility to users, queries involving UDFs are not as efficient as their pure SQL counterparts that invoke SQL’s highly-optimized built-in functions. Motivated by this problem, we propose a new technique for translating SQL queries with UDFs to pure SQL expressions. Unlike prior work in this space, our method is not based on syntactic rewrite rules and can handle a much more general class of UDFs. At a high-level, our method is based on counterexample-guided inductive synthesis (CEGIS) but employs a novel compositional strategy that decomposes the synthesis task into simpler sub-problems. However, because there is no universal decomposition strategy that works for all UDFs, we propose a novel lazy inductive synthesis approach that generates a sequence of decompositions that correspond to increasingly harder inductive synthesis problems. Because most realistic UDF-to-SQL translation tasks are amenable to a fine-grained decomposition strategy, our lazy inductive synthesis method scales significantly better than traditional CEGIS. We have implemented our proposed technique in a tool called CLIS for optimizing Spark SQL programs containing Scala UDFs. To evaluate CLIS, we manually study 100 randomly selected UDFs and find that 63 of them can be expressed in pure SQL. Our evaluation on these 63 UDFs shows that CLIS can automatically synthesize equivalent SQL expressions in 92% of the cases and that it can solve 2.4× more benchmarks compared to a baseline that does not use our compositional approach. We also show that CLIS yields an average speed-up of 3.5× for individual UDFs and 1.3× to 3.1× in terms of end-to-end application performance.",
        "link": "https://dl.acm.org/doi/10.1145/3485489",
        "category": "Databases"
    },
    {
        "title": "PoBery: Possibly-complete Big Data Queries with Probabilistic Data Placement and Scanning",
        "authors": "['Jie Song', 'Qiang He', 'Feifei Chen', 'Ye Yuan', 'Ge Yu']",
        "date": "None",
        "source": "ACM/IMS Transactions on Data Science",
        "abstract": "In big data query processing, there is a trade-off between query accuracy and query efficiency, for example, sampling query approaches trade-off query completeness for efficiency. In this article, we argue that query performance can be significantly improved by slightly losing the possibility of query completeness, that is, the chance that a query is complete. To quantify the possibility, we define a new concept, Probability of query Completeness (hereinafter referred to as PC). For example, If a query is executed 100 times, PC = 0.95 guarantees that there are no more than 5 incomplete results among 100 results. Leveraging the probabilistic data placement and scanning, we trade off PC for query performance. In the article, we propose PoBery (POssibly-complete Big data quERY), a method that supports neither complete queries nor incomplete queries, but possibly-complete queries. The experimental results conducted on HiBench prove that PoBery can significantly accelerate queries while ensuring the PC. Specifically, it is guaranteed that the percentage of complete queries is larger than the given PC confidence. Through comparison with state-of-the-art key-value stores, we show that while Drill-based PoBery performs as fast as Drill on complete queries, it is 1.7 ×, 1.1 ×, and 1.5 × faster on average than Drill, Impala, and Hive, respectively, on possibly-complete queries.",
        "link": "https://dl.acm.org/doi/10.1145/3465375",
        "category": "Databases"
    },
    {
        "title": "Research on EHR Storage and Sharing Scheme Based on Consortium Blockchain",
        "authors": "['Li Haohao', 'Liu Jianhua']",
        "date": "September 2021",
        "source": "AIPR '21: Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition",
        "abstract": "In order to realize the safe storage and sharing of medical data, this paper proposes a medical data storage model based on blockchain. Firstly, all medical institutions are divided by regions, and multiple medical institutions alliances are generated, and the consortium blockchain is formed respectively. The medical metadata is stored on the blockchain, and the medical data block is stored in InterPlanetary File System (IPFS). In each consortium blockchain, a small number of authoritative organizations are selected to form a main chain, which is responsible for the cross chain query function among the consortium blockchains. In the query process, the proxy re encryption technology is used to realize the user's access to the patient's medical information. The experimental results show that the model ensures the tamper proof and security of medical data, It realizes the safe sharing of medical data and improves the query efficiency of medical data, When 78.5% of the total number of queries in the same alliance, the cross chain scheme is better than the ordinary scheme.",
        "link": "https://dl.acm.org/doi/10.1145/3488933.3489014",
        "category": "Databases"
    },
    {
        "title": "Development of a Mobile App for Sleep Monitoring Using Internet of Things Hub",
        "authors": "['Qiting Li', 'Xiaohui Fan', 'Rongchao Peng']",
        "date": "November 2021",
        "source": "ICBBE '21: Proceedings of the 2021 8th International Conference on Biomedical and Bioinformatics Engineering",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3502871.3502890",
        "category": "Databases"
    },
    {
        "title": "Thermique: An Integrated AI-Based Temperature Sensing and Management System to Hold Back COVID-19 Contamination",
        "authors": "['Md. Rokonuzzaman Reza', 'Shaqran Bin Saleh', 'Tashfia Fatema', 'Ishraq Hasan', 'Muhaimin Bin Munir', 'Md Fazle Rabbi']",
        "date": "March 2022",
        "source": "ICCA '22: Proceedings of the 2nd International Conference on Computing Advancements",
        "abstract": "In the advent of a global pandemic, the necessity for early COVID-19 suspect detection and quarantine is of paramount importance. Medical research indicates that a high fever provides a general litmus of whether or not a person is infected with Coronavirus. Among several available solutions, thermal imaging has proven to be a better contactless screening procedure. It enables fast and easy detection of fever from a reasonable distance. In this research, a solution named Thermique is proposed. It is a cheap, easy to mass-produce, and automated AI-enabled thermal screening platform that combines facial detection, instant contactless temperature scanning, and RFID logging, while also providing an integrated defense against the spread of COVID-19 in a particular facility. Consisting of only off-the-shelf electronic components, this solution can be implemented with a significantly minimized cost, compared to its similar-function providing alternatives available on the market. To design and implement Thermique, a system architecture was developed for the platform, the details of which are highlighted within this paper. After the development of the prototype, several analytical evaluations of the system have been conducted, including the system’s performance, and overall usability.",
        "link": "https://dl.acm.org/doi/10.1145/3542954.3542978",
        "category": "Databases"
    },
    {
        "title": "A practical approach to groupjoin and nested aggregates",
        "authors": "['Philipp Fent', 'Thomas Neumann']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Groupjoins, the combined execution of a join and a subsequent group by, are common in analytical queries, and occur in about 1/8 of the queries in TPC-H and TPC-DS. While they were originally invented to improve performance, efficient parallel execution of groupjoins can be limited by contention, which limits their usefulness in a many-core system. Having an efficient implementation of groupjoins is highly desirable, as groupjoins are not only used to fuse group by and join but are also introduced by the unnesting component of the query optimizer to avoid nested-loops evaluation of aggregates. Furthermore, the query optimizer needs be able to reason over the result of aggregation in order to schedule it correctly. Traditional selectivity and cardinality estimations quickly reach their limits when faced with computed columns from nested aggregates, which leads to poor cost estimations and thus, suboptimal query plans.In this paper, we present techniques to efficiently estimate, plan, and execute groupjoins and nested aggregates. We propose two novel techniques, aggregate estimates to predict the result distribution of aggregates, and parallel groupjoin execution for a scalable execution of groupjoins. The resulting system has significantly better estimates and a contention-free evaluation of groupjoins, which can speed up some TPC-H queries up to a factor of 2.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476288",
        "category": "Databases"
    },
    {
        "title": "Design, Development, and Evaluation of a Physical Exercise Monitoring and Managing System for Athletes",
        "authors": "['Noor Nafiz Islam', 'Nafiz Imtiaz Khan', 'Md Abdur Razzak', 'Muhammad Nazrul Islam']",
        "date": "November 2021",
        "source": "iiWAS2021: The 23rd International Conference on Information Integration and Web Intelligence",
        "abstract": "The worldwide sports industry is booming through the usage of information and communication technologies. The collection, analysis, and presentation of athlete data is common practice for professional individual and team sports to assess individuals (athletes)/teams capability, fatigue, and subsequent adaptation responses; examine potential improvement areas, and minimize the risk of injury. Nutrition and exercise plans are also blended to meet specific training requirements and build strategic programs to maximize athletes’ ability to perform. An effective and efficient system is required for the athletes and their mentors to monitor and manage the athlete’s physical exercise. Therefore, the purpose of this article is to reveal the user requirements for creating an athlete monitoring system and to propose a wearable system based on the revealed requirements. To achieve these objectives, a Design Science Research (DSR) approach was adopted. As such, an empirical study (through semi-structured interviews) was conducted with 41 participants to reveal the system requirements; then a wearable athlete monitoring application was developed considering the revealed requirements. Finally, the proposed system was evaluated with 21 participants through the System Usability Scale (SUS) method. The study found that the proposed system is reliable, user-friendly, and useful for monitoring and managing physical exercise for the athletes and their mentors. The study also showed that the proposed system is useful and usable regardless of the athlete’s age or gender.",
        "link": "https://dl.acm.org/doi/10.1145/3487664.3487725",
        "category": "Databases"
    },
    {
        "title": "Thermique: An Integrated AI-Based Temperature Sensing and Management System to Hold Back COVID-19 Contamination",
        "authors": "['Md. Rokonuzzaman Reza', 'Shaqran Bin Saleh', 'Tashfia Fatema', 'Ishraq Hasan', 'Muhaimin Bin Munir', 'Md Fazle Rabbi']",
        "date": "March 2022",
        "source": "ICCA '22: Proceedings of the 2nd International Conference on Computing Advancements",
        "abstract": "In the advent of a global pandemic, the necessity for early COVID-19 suspect detection and quarantine is of paramount importance. Medical research indicates that a high fever provides a general litmus of whether or not a person is infected with Coronavirus. Among several available solutions, thermal imaging has proven to be a better contactless screening procedure. It enables fast and easy detection of fever from a reasonable distance. In this research, a solution named Thermique is proposed. It is a cheap, easy to mass-produce, and automated AI-enabled thermal screening platform that combines facial detection, instant contactless temperature scanning, and RFID logging, while also providing an integrated defense against the spread of COVID-19 in a particular facility. Consisting of only off-the-shelf electronic components, this solution can be implemented with a significantly minimized cost, compared to its similar-function providing alternatives available on the market. To design and implement Thermique, a system architecture was developed for the platform, the details of which are highlighted within this paper. After the development of the prototype, several analytical evaluations of the system have been conducted, including the system’s performance, and overall usability.",
        "link": "https://dl.acm.org/doi/10.1145/3542954.3542978",
        "category": "Databases"
    },
    {
        "title": "Research on Project Cooperation Prediction based on Metapath2vec",
        "authors": "['Xiaojun Zhou', 'Yulin Yang']",
        "date": "November 2021",
        "source": "ICISE '21: Proceedings of the 6th International Conference on Information Systems Engineering",
        "abstract": "Scientific research cooperation is an important way to carry out scientific projects and solve research problems, cooperative relationship prediction is one of the research hotspots at home and abroad in recent years. This paper focus on the academia of science, using Neo4j to build the network diagram of researchers and projects. Optimize on the basis of metapath2vec algorithm to predict cooperative relationship between researchers in different fields. Firstly, this paper studies and compares the cooperative prediction algorithms. Secondly, combined with the data set of project module of dataset, this paper optimizes and experiments the metapath2vec algorithm. Meanwhile, we design the weight coefficient under the influence of time factor, member contribution degree and project level, using heterogeneous knowledge graph to improve the computational efficiency and enhance the interpretability of prediction at the same time. Finally, the experiment and verification are carried out by using the project data of the National Fund Committee. The experimental result shows that the optimized cooperative prediction algorithm can take into account many factors such as time, member contribution, project level and so on, which helps predicting the potential cooperative relationship quickly, and the accuracy is improved by about 7% compared with the baseline.",
        "link": "https://dl.acm.org/doi/10.1145/3503928.3503940",
        "category": "Databases"
    },
    {
        "title": "Building operators in the wild - going beyond just install and upgrade",
        "authors": "['Henry Nash', 'Martin Hickey']",
        "date": "November 2021",
        "source": "CASCON '21: Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering",
        "abstract": "Building scalable, distributed cloud-native solutions results in many moving parts. While cloud-native development leads to more performant (and upgradeable-by-component) solutions, there are unique challenges related to deploying and maintaining such applications. For many years, the Kubernetes community focused on solving how to deploy a cloud-native application and upgrade it. A number of solutions exist, the most popular being Helm. At their core, solutions like Helm provide a common method for describing how to configure Kubernetes components for an application. Helm simplifies the install, upgrade, or delete operations of an applications. It is however not designed for managing the application instances running in the cluster. Operations outside of the basic scope might be application-specific, requiring specific application knowledge in order to be carried out reliably and safely. For example, how do you ensure caches are written through before maintenance operations happen? How might you circulate or synchronize security tokens when they change? Typically, a human operator needs to control application-specific operations (often called Day 2 operations), implementing a series of steps to perform an overall task. Enter the Kubernetes operator. A Kubernetes operator provides a standardized structure which incorporates the human operator's knowledge in an automated way that minimizes the need for real-time human support. It's often said that operators \"automate activities for a human operator.\" In other words, minimize the \"3 o'clock in the morning call\" for the site reliability engineer! However, there are other potential ways of providing the same functionality. The first solution you might try is to script a series of steps that you can externally apply to your cluster. You could, indeed, a achieve some if not all of the tasks needed with such an approach. The downside to this is that many of the cloud-native patterns and approaches would end up being re-used and have to be re-coded in those external solutions. Plus, externally applied actions are harder to monitor. Operators offer a better option, baking in human knowledge that runs inside the cluster with access to all the internal cloud-native concepts and support that Kubernetes provides. Operators are application-specific extensions to Kubernetes, giving those applications access to concepts and support that is usually added by human operators. This operator concept was first launched by CoreOS for stateful applications like databases, caches, and monitoring systems. Kubernetes deals with the deployment of both stateful and stateless distributed applications. However, handling full-fledged scaling, upgrading, and reconfiguration of operations without losing data or availability is a bigger challenge. This requires application domain knowledge that is driven by the human operator. As with many things in life, there is more than one way to do something. This also applies to the automation and management of Kubernetes applications. The best way to understand how useful operators are is to see an example in action, and examine the pros and cons of the various ways the Day 2 functionality can be achieved.",
        "link": "https://dl.acm.org/doi/10.5555/3507788.3507843",
        "category": "Databases"
    },
    {
        "title": "Tanium reveal: a federated search engine for querying unstructured file data on large enterprise networks",
        "authors": "['Josh Stoddard', 'Adam Mustafa', 'Naveen Goela']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Tanium Reveal is a federated search engine deployed on large-scale enterprise networks that is capable of executing data queries across billions of private data files within 60 seconds. Data resides at the edge of networks, potentially distributed on hundreds of thousands of endpoints. The anatomy of the search engine consists of local inverse indexes on each endpoint and a global communication platform called Tanium for issuing search queries to all endpoints. Reveal enables asynchronous parsing and indexing on endpoints without noticeable impact to the endpoints' primary functionality. The engine harnesses the Tanium platform, which is based on a self-organizing, fault-tolerant, scalable, linear chain communication scheme. We demonstrate a multi-tier workflow for executing search queries across a network and for viewing matching snippets of text on any endpoint. We analyze metrics for federated indexing and searching in multiple environments including a production network with 1.05 billion searchable files distributed across 4236 endpoints. While primarily focusing on Boolean, phrase, and similarity query types, Reveal is compatible with further automation (e.g., semantic classification based on machine learning). Lastly, we discuss safeguards for sensitive information within Reveal including cryptographic hashing of private text and role-based access control (RBAC).",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476386",
        "category": "Databases"
    },
    {
        "title": "TaintStream: fine-grained taint tracking for big data platforms through dynamic code translation",
        "authors": "['Chengxu Yang', 'Yuanchun Li', 'Mengwei Xu', 'Zhenpeng Chen', 'Yunxin Liu', 'Gang Huang', 'Xuanzhe Liu']",
        "date": "August 2021",
        "source": "ESEC/FSE 2021: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "abstract": "Big data has become valuable property for enterprises and enabled various intelligent applications. Today, it is common to host data in big data platforms (e.g., Spark), where developers can submit scripts to process the original and intermediate data tables. Meanwhile, it is highly desirable to manage the data to comply with various privacy requirements. To enable flexible and automated privacy policy enforcement, we propose TaintStream, a fine-grained taint tracking framework for Spark-like big data platforms. TaintStream works by automatically injecting taint tracking logic into the data processing scripts, and the injected scripts are dynamically translated to maintain a taint tag for each cell during execution. The dynamic translation rules are carefully designed to guarantee non-interference in the original data operation. By defining different semantics of taint tags, TaintStream can enable various data management applications such as access control, data retention, and user data erasure. Our experiments on a self-crafted benchmarksuite show that TaintStream is able to achieve accurate cell-level taint tracking with a precision of 93.0% and less than 15% overhead. We also demonstrate the usefulness of TaintStream through several real-world use cases of privacy policy enforcement.",
        "link": "https://dl.acm.org/doi/10.1145/3468264.3468532",
        "category": "Databases"
    },
    {
        "title": "Evolution of a compiling query engine",
        "authors": "['Thomas Neumann']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In 2011 we showed how to use dynamic code generation to process queries in a data-centric manner. This execution model can produce compact and efficient code and was successfully used by both our own systems and systems of other groups. As the systems become used in practice, additional techniques were developed for shortcomings that did arrive, including low-latency compilation, multi-threading support, and others. This paper gives an overview of the evolution of our query engine within in the last ten years, and points out which problem have to be tackled to bring a compiling system into production usage.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476410",
        "category": "Databases"
    },
    {
        "title": "Asynchronous Prefix Recoverability for Fast Distributed Stores",
        "authors": "['Tianyu Li', 'Badrish Chandramouli', 'Jose M. Faleiro', 'Samuel Madden', 'Donald Kossmann']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Accessing and updating data sharded across distributed machines safely and speedily in the face of failures remains a challenging problem. Most prominently, applications that share state across different nodes want their writes to quickly become visible to others, without giving up recoverability guarantees in case a failure occurs. Current solutions of a fast cache backed by storage cannot support this use case easily. In this work, we design a distributed protocol, called Distributed Prefix Recovery (DPR) that builds on top of a sharded cache-store architecture with single-key operations, to provide cross-shard recoverability guarantees. With DPR, many clients can read and update shared state at sub-millisecond latency, while receiving periodic prefix durability guarantees. On failure, DPR quickly restores the system to a prefix-consistent state with a novel non-blocking rollback scheme. We added DPR to a key-value store (FASTER) and cache (Redis) and show that we can get high throughput and low latency similar to in-memory systems, while lazily providing durability guarantees similar to persistent stores.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3458454",
        "category": "Databases"
    },
    {
        "title": "Scaling Biodiversity Monitoring for the Data Age",
        "authors": "['Sara Beery']",
        "date": "Summer 2021",
        "source": "XRDS: Crossroads, The ACM Magazine for Students",
        "abstract": "Technological advances have made it possible to collect massive amounts of biodiversity data. How can analysis efforts keep up?",
        "link": "https://dl.acm.org/doi/10.1145/3466857",
        "category": "Databases"
    },
    {
        "title": "AutoExecutor: predictive parallelism for spark SQL queries",
        "authors": "['Rathijit Sen', 'Abhishek Roy', 'Alekh Jindal', 'Rui Fang', 'Jeff Zheng', 'Xiaolei Liu', 'Ruiping Li']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Right-sizing resources for query execution is important for cost-efficient performance, but estimating how performance is affected by resource allocations, upfront, before query execution is difficult. We demonstrate AutoExecutor, a predictive system that uses machine learning models to predict query run times as a function of the number of allocated executors, that limits the maximum allowed parallelism, for Spark SQL queries running on Azure Synapse.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476362",
        "category": "Databases"
    },
    {
        "title": "Would you like a quick peek? providing logging support to monitor data processing in big data applications",
        "authors": "['Zehao Wang', 'Haoxiang Zhang', 'Tse-Hsun (Peter) Chen', 'Shaowei Wang']",
        "date": "August 2021",
        "source": "ESEC/FSE 2021: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "abstract": "To analyze large-scale data efficiently, developers have created various big data processing frameworks (e.g., Apache Spark). These big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic. In traditional software systems, developers leverage logging to monitor applications and record intermediate states to assist workload understanding and issue diagnosis. However, due to the abstraction and the peculiarity of big data frameworks, there is currently no effective monitoring approach for big data applications. In this paper, we first manually study 1,000 randomly sampled Spark-related questions on Stack Overflow to study their root causes and the type of information, if recorded, that can assist developers with motioning and diagnosis. Then, we design an approach, DPLOG, which assists developers with monitoring Spark applications. DPLOG leverages statistical sampling to minimize performance overhead and provides intermediate information and hint/warning messages for each data processing step of a chained method pipeline. We evaluate DPLOG on six benchmarking programs and find that DPLOG has a relatively small overhead (i.e., less than 10% increase in response time when processing 5GB data) compared to without using DPLOG, and reduce the overhead by over 500% compared to the baseline. Our user study with 20 developers shows that DPLOG can reduce the needed time to debug big data applications by 63% and the participants give DPLOG an average of 4.85/5 for its usefulness. The idea of DPLOG may be applied to other big data processing frameworks, and our study sheds light on future research opportunities in assisting developers with monitoring big data applications.",
        "link": "https://dl.acm.org/doi/10.1145/3468264.3468613",
        "category": "Databases"
    },
    {
        "title": "Handling Fuzzy Spatial Data in R Using the fsr Package",
        "authors": "['Anderson Chaves Carniel', 'Felippe Galdino', 'Juliana Strieder Philippsen', 'Markus Schneider']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "GIS and spatial data science (SDS) tools have been recently approaching each other by establishing bridge technologies between them. R as one of the most prominent programming languages used in SDS projects has been granted access to GIS infrastructure, while R scripts can be integrated and executed in GIS functions. Unfortunately, the treatment of spatial fuzziness has so far not been considered in SDS projects and bridge technologies due to a lack of software packages that can handle fuzzy spatial objects. This paper introduces an R package named fsr as an implementation of the fuzzy spatial data types, operations, and predicates of the Spatial Plateau Algebra that is based on the abstract Fuzzy Spatial Algebra. This R package solves the problem of constructing fuzzy spatial objects as spatial plateau objects from real datasets and describes how to conduct exploratory spatial data analysis by issuing geometric operations and topological predicates on fuzzy spatial objects. Further, fsr provides the possibility of designing fuzzy spatial inference models to discover new findings from fuzzy spatial objects. It optimizes the inference process by deploying the particle swarm optimization to obtain the point locations with the maximum or minimum inferred values that answer a specific user request.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3484255",
        "category": "Databases"
    },
    {
        "title": "Can Applications Recover from fsync Failures?",
        "authors": "['Anthony Rebello', 'Yuvraj Patel', 'Ramnatthan Alagappan', 'Andrea C. Arpaci-Dusseau', 'Remzi H. Arpaci-Dusseau']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "We analyze how file systems and modern data-intensive applications react to fsync failures. First, we characterize how three Linux file systems (ext4, XFS, Btrfs) behave in the presence of failures. We find commonalities across file systems (pages are always marked clean, certain block writes always lead to unavailability) as well as differences (page content and failure reporting is varied). Next, we study how five widely used applications (PostgreSQL, LMDB, LevelDB, SQLite, Redis) handle fsync failures. Our findings show that although applications use many failure-handling strategies, none are sufficient: fsync failures can cause catastrophic outcomes such as data loss and corruption. Our findings have strong implications for the design of file systems and applications that intend to provide strong durability guarantees.",
        "link": "https://dl.acm.org/doi/10.1145/3450338",
        "category": "Databases"
    },
    {
        "title": "SeDaSOMA: A Framework for Supporting Serendipitous, Data-As-A-Service-Oriented, Open Big Data Management and Analytics",
        "authors": "['Alfredo Cuzzocrea', 'Paolo Ciancarini']",
        "date": "August 2021",
        "source": "ICCBDC '21: Proceedings of the 2021 5th International Conference on Cloud and Big Data Computing",
        "abstract": "This paper describes the anatomy of SeDaSOMA, a reference framework for supporting serendipitous, data-as-a-service-oriented, open big data management and analytics. The proposed framework aims at supporting advanced big data management and analytics by relying on innovative research findings and next-generation big data tools. The paper also depicts some Cloud-aware big data vertical applications of SeDaSOMA in specific scenarios that are currently of great interest.",
        "link": "https://dl.acm.org/doi/10.1145/3481646.3481647",
        "category": "Databases"
    },
    {
        "title": "Combining Sampling and Synopses with Worst-Case Optimal Runtime and Quality Guarantees for Graph Pattern Cardinality Estimation",
        "authors": "['Kyoungmin Kim', 'Hyeonji Kim', 'George Fletcher', 'Wook-Shin Han']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Graph pattern cardinality estimation is the problem of estimating the number of embeddings of a query graph in a data graph. This fundamental problem arises, for example, during query planning in subgraph matching algorithms. There are two major approaches to solving the problem: sampling and synopsis. Synopsis (or summary)-based methods are fast and accurate if synopses capture information of graphs well. However, these methods suffer from large errors due to loss of information during summarization and inherent assumptions. Sampling-based methods are unbiased but suffer from large estimation variance due to large sample space. To address these limitations, we propose Alley, a hybrid method that combines both sampling and synopses. Alley employs 1) a novel sampling strategy, random walk with intersection, which effectively reduces the sample space, 2) branching to further reduce variance, and 3) a novel mining approach that extracts and indexes tangled patterns as synopses which are inherently difficult to estimate by sampling. By using them in the online estimation phase, we can effectively reduce the sample space while still ensuring unbiasedness. We establish that Alley has worst-case optimal runtime and approximation quality guarantees for any given error bound ε and required confidence μ. In addition to the theoretical aspect of Alley, our extensive experiments show that Alley outperforms the state-of-the-art methods by up to orders of magnitude higher accuracy with similar efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457246",
        "category": "Databases"
    },
    {
        "title": "Estimating the Size of Union of Sets in Streaming Models",
        "authors": "['Kuldeep S. Meel', 'N.V. Vinodchandran', 'Sourav Chakraborty']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "In this paper we study the problem of estimating the size of the union of sets $S_1, \\dots, S_M$ where each set $S_i \\subseteq Ømega$ (for some discrete universe $Ømega$) is implicitly presented and comes in a streaming fashion. We define the notion of Delphic sets to capture class of streaming problems where membership, sampling, and counting calls to the sets are efficient. In particular, we show our notion of Delphic sets capture three well known problems: Klee's measure problem (discrete version), test coverage estimation, and model counting of DNF formulas. The Klee's measure problem corresponds to computation of volume of multi-dimension axis aligned rectangles, i.e., every d-dimension axis-aligned rectangle can be defined as $[a_1,b_1] \\times [a_2,b_2] \\times łdots \\times [a_d, b_d]$. The problem of test coverage estimation focuses on the computation of coverage measure for a given testing array in the context of combinatorial testing, which is a fundamental technique in the context of hardware and software testing. Finally, given a DNF formula $\\varphi = T_1 \\vee T_2 \\vee łdots \\vee T_M$, the problem of model counting seeks to compute the number of satisfying assignments of $\\varphi$. The primary contribution of our work is a simple and efficient sampling-based algorithm, called \\hybrid, for estimating the of union of sets in streaming setting. Our algorithm has the space complexity of $O(Rłog |Ømega|)$ and update time is $O(Rłog R \\cdot łog(M/δ) \\cdot łog|Ømega|)$ where, $R = Ołeft(łog (M/δ)\\cdot \\varepsilon^2 \\right).$ Consequently, our algorithm provides the first algorithm with linear dependence on d for Klee's measure problem in streaming setting for $d>1$, thereby settling the open problem of Tirthpura and Woodruff (PODS-12). Furthermore, a straightforward application of our algorithm lends to an efficient algorithm for coverage estimation problem in streaming setting. We then investigate whether the space complexity for coverage estimation can be further improved, and in this context, we present another streaming algorithm that uses near-optimal $O(tłog n/\\varepsilon^2)$ space complexity but uses an update algorithm that is in $\\rm P ^\\rm NP $, thereby showcasing an interesting time vs space trade-off in the streaming setting. Finally, we demonstrate the generality of our Delphic sets by obtaining a streaming algorithm for model counting of DNF formulas. It is worth remarking that we view a key strength of our work is the simplicity of both the algorithm and its theoretical analysis, which makes it amenable to practical implementation and easy adoption.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458333",
        "category": "Databases"
    },
    {
        "title": "A Formal Framework for Complex Event Recognition",
        "authors": "['Alejandro Grez', 'Cristian Riveros', 'Martín Ugarte', 'Stijn Vansummeren']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "Complex event recognition (CER) has emerged as the unifying field for technologies that require processing and correlating distributed data sources in real time. CER finds applications in diverse domains, which has resulted in a large number of proposals for expressing and processing complex events. Existing CER languages lack a clear semantics, however, which makes them hard to understand and generalize. Moreover, there are no general techniques for evaluating CER query languages with clear performance guarantees.In this article, we embark on the task of giving a rigorous and efficient framework to CER. We propose a formal language for specifying complex events, called complex event logic (CEL), that contains the main features used in the literature and has a denotational and compositional semantics. We also formalize the so-called selection strategies, which had only been presented as by-design extensions to existing frameworks. We give insight into the language design trade-offs regarding the strict sequencing operators of CEL and selection strategies.With a well-defined semantics at hand, we discuss how to efficiently process complex events by evaluating CEL formulas with unary filters. We start by introducing a formal computational model for CER, called complex event automata (CEA), and study how to compile CEL formulas with unary filters into CEA. Furthermore, we provide efficient algorithms for evaluating CEA over event streams using constant time per event followed by output-linear delay enumeration of the results.",
        "link": "https://dl.acm.org/doi/10.1145/3485463",
        "category": "Databases"
    },
    {
        "title": "Scotty: General and Efficient Open-source Window Aggregation for Stream Processing Systems",
        "authors": "['Jonas Traub', 'Philipp Marian Grulich', 'Alejandro Rodríguez Cuéllar', 'Sebastian Breß', 'Asterios Katsifodimos', 'Tilmann Rabl', 'Volker Markl']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "Window aggregation is a core operation in data stream processing. Existing aggregation techniques focus on reducing latency, eliminating redundant computations, or minimizing memory usage. However, each technique operates under different assumptions with respect to workload characteristics, such as properties of aggregation functions (e.g., invertible, associative), window types (e.g., sliding, sessions), windowing measures (e.g., time- or count-based), and stream (dis)order. In this article, we present Scotty, an efficient and general open-source operator for sliding-window aggregation in stream processing systems, such as Apache Flink, Apache Beam, Apache Samza, Apache Kafka, Apache Spark, and Apache Storm. One can easily extend Scotty with user-defined aggregation functions and window types. Scotty implements the concept of general stream slicing and derives workload characteristics from aggregation queries to improve performance without sacrificing its general applicability. We provide an in-depth view on the algorithms of the general stream slicing approach. Our experiments show that Scotty outperforms alternative solutions.",
        "link": "https://dl.acm.org/doi/10.1145/3433675",
        "category": "Databases"
    },
    {
        "title": "Expand your Training Limits! Generating Training Data for ML-based Data Management",
        "authors": "['Francesco Ventura', 'Zoi Kaoudi', 'Jorge Arnulfo Quiané-Ruiz', 'Volker Markl']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Machine Learning (ML) is quickly becoming a prominent method in many data management components, including query optimizers which have recently shown very promising results. However, the low availability of training data (i.e., large query workloads with execution time or output cardinality as labels) widely limits further advancement in research and compromises the technology transfer from research to industry. Collecting a labeled query workload has a very high cost in terms of time and money due to the development and execution of thousands of realistic queries/jobs. In this work, we face the problem of generating training data for data management components tailored to users' needs. We present DataFarm, an innovative framework for efficiently generating and labeling large query workloads. We follow a data-driven white-box approach to learn from pre-existing small workload patterns, input data, and computational resources. Our framework allows users to produce a large heterogeneous set of realistic jobs with their labels, which can be used by any ML-based data management component. We show that our framework outperforms the current state-of-the-art both in query generation and label estimation using synthetic and real datasets. It has up to 9x better labeling performance, in terms of R2 score. More importantly, it allows users to reduce the cost of getting labeled query workloads by 54x (and up to an estimated factor of 104x) compared to standard approaches.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457286",
        "category": "Databases"
    },
    {
        "title": "ESPBench: The Enterprise Stream Processing Benchmark",
        "authors": "['Guenter Hesse', 'Christoph Matthies', 'Michael Perscheid', 'Matthias Uflacker', 'Hasso Plattner']",
        "date": "April 2021",
        "source": "ICPE '21: Proceedings of the ACM/SPEC International Conference on Performance Engineering",
        "abstract": "Growing data volumes and velocities in fields such as Industry 4.0 or the Internet of Things have led to the increased popularity of data stream processing systems. Enterprises can leverage these developments by enriching their core business data and analyses with up-to-date streaming data. Comparing streaming architectures for these complex use cases is challenging, as existing benchmarks do not cover them. ESPBench is a new enterprise stream processing benchmark that fills this gap. We present its architecture, the benchmarking process, and the query workload. We employ ESPBench on three state-of-the-art stream processing systems, Apache Spark, Apache Flink, and Hazelcast Jet, using provided query implementations developed with Apache Beam. Our results highlight the need for the provided ESPBench toolkit that supports benchmark execution, as it enables query result validation and objective latency measures.",
        "link": "https://dl.acm.org/doi/10.1145/3427921.3450242",
        "category": "Databases"
    },
    {
        "title": "The Construction Plan Of The Chinese and Korean Colloquial Corpus",
        "authors": "['JinQiu Kong', 'Lin Liu', 'HanLei Liu']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "The construction of the Korean corpus has always been an effective way to analyze and explore the Korean language. Due to cultural differences and translation errors, the study and learning of the Korean language, especially the colloquial category, is in the stage of blind reading, which has caused a lot of understanding and research on its original meaning. In view of the impact of Korean colloquialism, this article proposes a corpus based on Korean colloquialism and a corpus model suitable for Korean colloquialism analysis. And the corpus model is verified and analyzed. The experimental results show that the core of the colloquialism can be clearly analyzed based on the Korean colloquialism corpus model, which provides an ideal solution for the learning of Korean colloquialism.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501699",
        "category": "Databases"
    },
    {
        "title": "Entity resolution on-demand",
        "authors": "['Giovanni Simonini', 'Luca Zecchini', 'Sonia Bergamaschi', 'Felix Naumann']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Entity Resolution (ER) aims to identify and merge records that refer to the same real-world entity. ER is typically employed as an expensive cleaning step on the entire data before consuming it. Yet, determining which entities are useful once cleaned depends solely on the user's application, which may need only a fraction of them. For instance, when dealing with Web data, we would like to be able to filter the entities of interest gathered from multiple sources without cleaning the entire, continuously-growing data. Similarly, when querying data lakes, we want to transform data on-demand and return the results in a timely manner---a fundamental requirement of ELT (Extract-Load-Transform) pipelines.We propose BrewER, a framework to evaluate SQL SP queries on dirty data while progressively returning results as if they were issued on cleaned data. BrewER tries to focus the cleaning effort on one entity at a time, following an ORDER BY predicate. Thus, it inherently supports top-k and stop-and-resume execution. For a wide range of applications, a significant amount of resources can be saved. We exhaustively evaluate and show the efficacy of BrewER on four real-world datasets.",
        "link": "https://dl.acm.org/doi/10.14778/3523210.3523226",
        "category": "Databases"
    },
    {
        "title": "État de l'art sur les applications dédiées à la rééducation de personnes aphasiques: State of The Art Review On Applications Dedicated To The Rehabilitation Of People With Aphasia",
        "authors": "['Oussama Ben Aziza', 'Isabelle Pecci', 'Benoît Martin']",
        "date": "April 2021",
        "source": "IHM '21: Proceedings of the 32nd Conference on l&apos;Interaction Homme-Machine",
        "abstract": "Aphasia is a language disorder that often occurs following a stroke. The objective of our study is to give an overview of the use of desktop technologies, oriented towards augmented, mixed or virtual reality. An exploratory search of medical and computer article databases (Google Scholar, ResearchGate, Taylor and Francis, PubMed and ACM digital library) found 1728 articles on the subject. A multi-criteria selection method identified 44 relevant articles. Based on these articles, this paper presents the interaction techniques used for rehabilitation. It also presents the methods used with a description of the user tests carried out: protocol, participants and results. Our study shows that the use of computer programs can help rehabilitate aphasic people. But little research has been done on the use of innovative technologies based on augmented, mixed or virtual reality. We conclude with recommendations and suggestions to further the research in this subject. L'aphasie est un trouble du langage qui apparaît souvent à la suite d'un Accident Vasculaire Cérébral (AVC). L'objectif de notre étude est de donner un aperçu sur l'usage des technologies de type bureau, orientées réalité augmentée, mixte ou virtuelle. Une recherche exploratoire dans des bases de données d'articles médicaux et informatiques (Google Scholar, Researchgate, Taylor and Francis, PubMed et ACM digital library) a permis de trouver 1728 articles sur le sujet. Une méthode de sélection multicritères a dégagé 44 articles pertinents. À partir de ces articles, ce papier présente les techniques d'interaction utilisées pour la rééducation. Il présente également les méthodes utilisées avec une description des tests utilisateurs réalisés : protocole, participants et résultats. Notre étude montre que l'utilisation de programmes informatiques peut aider à rééduquer des personnes aphasiques. Mais peu de recherches portent sur l'utilisation de technologies innovantes à base de réalité augmentée, mixte ou virtuelle. Nous concluons par des recommandations et suggestions pour approfondir la recherche sur ce sujet.",
        "link": "https://dl.acm.org/doi/10.1145/3450522.3451327",
        "category": "Databases"
    },
    {
        "title": "Social Signals of Cohesion in Multi-party Interactions",
        "authors": "['Reshmashree B. Kantharaju', 'Catherine Pelachaud']",
        "date": "September 2021",
        "source": "IVA '21: Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents",
        "abstract": "Group conversation is a frequently used form of communication for exchanging ideas and making decisions. Cohesion is an emergent phenomenon that describes the members' attraction towards the group and towards working together. In this paper, we present the cohesion labels assigned to segments from [redacted], a multimodal dataset of simulated medical consultations. Then, we present the analysis performed to identify social cues that characterize cohesion and report the accuracy for classifying cohesion. Results show that non-verbal social cues like gaze, facial AUs, laughter etc., indeed convey information regarding the level of cohesion. Finally we present a preliminary evaluation conducted using the prominent cues to simulate a cohesive group of agents.",
        "link": "https://dl.acm.org/doi/10.1145/3472306.3478362",
        "category": "Databases"
    },
    {
        "title": "Expressive Power of Linear Algebra Query Languages",
        "authors": "['Floris Geerts', 'Thomas Muñoz', 'Cristian Riveros', 'Domagoj Vrgoč']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Linear algebra algorithms often require some sort of iteration or recursion as is illustrated by standard algorithms for Gaussian elimination, matrix inversion, and transitive closure. A key characteristic shared by these algorithms is that they allow looping for a number of steps that is bounded by the matrix dimension. In this paper we extend the matrix query language MATLANG with this type of recursion, and show that this suffices to express classical linear algebra algorithms. We study the expressive power of this language and show that it naturally corresponds to arithmetic circuit families, which are often said to capture linear algebra. Furthermore, we analyze several sub-fragments of our language, and show that their expressive power is closely tied to logical formalisms on semiring-annotated relations.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458314",
        "category": "Databases"
    },
    {
        "title": "Designing Technologies to Support Parent-Child Relationships: A Review of Current Findings and Suggestions for Future Directions",
        "authors": "['Ji Youn Shin', 'Minjin Rheu', 'Jina Huh-Yoo', 'Wei Peng']",
        "date": "None",
        "source": "Proceedings of the ACM on Human-Computer Interaction",
        "abstract": "Diverse fields, including CSCW, Communication, and Human Development studies, have investigated how technologies can better support parent-child relationships. While these studies are scattered across literature, little effort has been made to synthesize the findings. We conducted a review of studies that examined the factors associated with parent-child relationships that are mediated by technologies. Specifically, we synthesized previous studies based on children's age groups and different family contexts, including cohabitation. From a total of 12,942 search results from two databases, and 32 results from the hand-searching process, we conducted a full-text review of 190 articles and identified 19 suitable studies. An additional search during the revision cycle resulted in 6 more full-text reviews and 1 additional study being included in the data analysis. We analyzed challenges and facilitators in designing CSCW systems supporting parent-child relationships for families living together or apart and families with children of different developmental stages. Findings showed two common challenges, which should be addressed in technology designed to support parent-child relationships: discrepancies in expected communication between parents and child(ren) and the complex emotions of parents toward parenting caused by their busy schedules. Challenges specific to families who are living apart included consequences from being physically distant and having limited access to communication resources. The following factors commonly helped facilitate parent-child relationships: (1) reciprocity norms of the family (2) reinforcement of transparency, affection, and trust, (3) a physical proxy of each other through an object or interface design, (4) accessibility, the sophistication level of technology, and communication resources, (5) enjoyable, age-appropriate shared content among parents and children, and (6) situational awareness and routine as ways to increase parent-child relationships. Media richness and synchronicity in system design and privacy preservation without interruption facilitated parent-child relationships of families living apart. Based on the findings, we discuss opportunities for technological innovation for physically co-located families and the importance of considering children's age and developmental stages in designing technology for parent-child relationships.",
        "link": "https://dl.acm.org/doi/10.1145/3479585",
        "category": "Databases"
    },
    {
        "title": "Fast detection of denial constraint violations",
        "authors": "['Eduardo H. M. Pena', 'Eduardo C. de Almeida', 'Felix Naumann']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The detection of constraint-based errors is a critical task in many data cleaning solutions. Previous works perform the task either using traditional data management systems or using specialized systems that speed up error detection. Unfortunately, both approaches may fail to execute in a reasonable time or even exhaust the available memory in the attempt. To address the main drawbacks of previous approaches, we present the FAst Constraint-based Error DeTector (FACET) to detect violations of denial constraints (DCs). FACET uses column sketch information to organize a pipeline of special operators for DC predicates and it implements these operators using a set of efficient algorithms and data structures that adapt to different data characteristics and predicate structures. We evaluate our system on a diverse array of datasets and constraints, showing its robustness and performance gains compared to different types of DBMSs and to a specialized system.",
        "link": "https://dl.acm.org/doi/10.14778/3503585.3503595",
        "category": "Databases"
    },
    {
        "title": "Collective shortest paths for minimizing congestion on temporal load-aware road networks",
        "authors": "['Chris Conlan', 'Teddy Cunningham', 'Gunduz Vehbi Demirci', 'Hakan Ferhatosmanoglu']",
        "date": "November 2021",
        "source": "IWCTS '21: Proceedings of the 14th ACM SIGSPATIAL International Workshop on Computational Transportation Science",
        "abstract": "Shortest path queries over graphs are usually considered as isolated tasks, where the goal is to return the shortest path for each individual query. In practice, however, such queries are typically part of a system (e.g., a road network) and their execution dynamically affects other queries and network parameters, such as the loads on edges, which in turn affects the shortest paths. We study the problem of collectively processing shortest path queries, where the objective is to optimize a collective objective, such as minimizing the overall cost. We define a temporal load-aware network that dynamically tracks expected loads while satisfying the desirable 'first in, first out' property. We develop temporal load-aware extensions of widely used shortest path algorithms, and a scalable collective routing solution that seeks to reduce system-wide congestion through dynamic path reassignment. Experiments illustrate that our collective approach to this NP-hard problem achieves improvements in a variety of performance measures, such as, i) reducing average travel times by up to 63%, ii) producing fairer suggestions across queries, and iii) distributing load across up to 97% of a city's road network capacity. The proposed approach is generalizable, which allows it to be adapted for other concurrent query processing tasks over networks.",
        "link": "https://dl.acm.org/doi/10.1145/3486629.3490691",
        "category": "Databases"
    },
    {
        "title": "Accelerating approximate aggregation queries with expensive predicates",
        "authors": "['Daniel Kang', 'John Guibas', 'Peter Bailis', 'Tatsunori Hashimoto', 'Yi Sun', 'Matei Zaharia']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Researchers and industry analysts are increasingly interested in computing aggregation queries over large, unstructured datasets with selective predicates that are computed using expensive deep neural networks (DNNs). As these DNNs are expensive and because many applications can tolerate approximate answers, analysts are interested in accelerating these queries via approximations. Unfortunately, standard approximate query processing techniques to accelerate such queries are not applicable because they assume the result of the predicates are available ahead of time. Furthermore, recent work using cheap approximations (i.e., proxies) do not support aggregation queries with predicates.To accelerate aggregation queries with expensive predicates, we develop and analyze a query processing algorithm that leverages proxies (ABAE). ABAE must account for the key challenge that it may sample records that do not satisfy the predicate. To address this challenge, we first use the proxy to group records into strata so that records satisfying the predicate are ideally grouped into few strata. Given these strata, ABAE uses pilot sampling and plugin estimates to sample according to the optimal allocation. We show that ABAE converges at an optimal rate in a novel analysis of stratified sampling with draws that may not satisfy the predicate. We further show that ABAE outperforms on baselines on six real-world datasets, reducing labeling costs by up to 2.3X.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476285",
        "category": "Databases"
    },
    {
        "title": "HADAD: A Lightweight Approach for Optimizing Hybrid Complex Analytics Queries",
        "authors": "['Rana Alotaibi', 'Bogdan Cautis', 'Alin Deutsch', 'Ioana Manolescu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Hybrid complex analytics workloads typically include (i) data management tasks (joins, selections, etc. ), easily expressed using relational algebra (RA)-based languages, and (ii) complex analytics tasks (regressions, matrix decompositions, etc.), mostly expressed in linear algebra (LA) expressions. Such workloads are common in many application areas, including scientific computing, web analytics, and business recommendation. Existing solutions for evaluating hybrid analytical tasks - ranging from LA-oriented systems, to relational systems (extended to handle LA operations), to hybrid systems - either optimize data management and complex tasks separately, exploit RA properties only while leaving LA-specific optimization opportunities unexploited, or focus heavily on physical optimization, leaving semantic query optimization opportunities unexplored. Additionally, they are not able to exploit precomputed (materialized) results to avoid recomputing (part of) a given mixed (RA and/or LA) computation. In this paper, we take a major step towards filling this gap by proposing HADAD, an extensible lightweight approach for optimizing hybrid complex analytics queries, based on a common abstraction that facilitates unified reasoning: a relational model endowed with integrity constraints. Our solution can be naturally and portably applied on top of pure LA and hybrid RA-LA platforms without modifying their internals. An extensive empirical evaluation shows that HADAD yields significant performance gains on diverse workloads, ranging from LA-centered to hybrid.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457311",
        "category": "Databases"
    },
    {
        "title": "Two-Attribute Skew Free, Isolated CP Theorem, and Massively Parallel Joins",
        "authors": "['Miao Qiao', 'Yufei Tao']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "This paper presents an algorithm to process a multi-way join with load $\\tO(n/p^2/(α φ) )$ under the MPC model, where n is the number of tuples in the input relations, α the maximum arity of those relations, p the number of machines, and φ a newly introduced parameter called the \\em generalized vertex packing number. The algorithm owes to two new findings. The first is a \\em two-attribute skew free technique to partition the join result for parallel computation. The second is an \\em isolated cartesian product theorem, which provides fresh graph-theoretic insights on joins with α \\ge 3$ and generalizes an existing theorem on α = 2$.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458321",
        "category": "Databases"
    },
    {
        "title": "iTurboGraph: Scaling and Automating Incremental Graph Analytics",
        "authors": "['Seongyun Ko', 'Taesung Lee', 'Kijae Hong', 'Wonseok Lee', 'In Seo', 'Jiwon Seo', 'Wook-Shin Han']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "With the rise of streaming data for dynamic graphs, large-scale graph analytics meets a new requirement of Incremental Computation because the larger the graph, the higher the cost for updating the analytics results by re-execution. A dynamic graph consists of an initial graph G and graph mutation updates Δ G$ of edge insertions or deletions. Given a query Q, its results $Q(G)$, and updates for Δ G$ to G, incremental graph analytics computes updates Δ Q$ such that Q($G \\cup Δ G)$ = $Q(G)$ $\\cup$ Δ Q$ where $\\cup$ is a union operator. In this paper, we consider the problem of large-scale incremental neighbor-centric graph analytics (\\NGA ). We solve the limitations of previous systems: lack of usability due to the difficulties in programming incremental algorithms for \\NGA and limited scalability and efficiency due to the overheads in maintaining intermediate results for graph traversals in \\NGA. First, we propose a domain-specific language, ŁNGA, and develop its compiler for intuitive programming of \\NGA, automatic query incrementalization, and query optimizations. Second, we define Graph Streaming Algebra as a theoretical foundation for scalable processing of incremental \\NGA. We introduce a concept of Nested Graph Windows and model graph traversals as the generation of walk streams. Lastly, we present a system \\SystemName, which efficiently processes incremental \\NGA for large graphs. Comprehensive experiments show that it effectively avoids costly re-executions and efficiently updates the analytics results with reduced IO and computations.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457243",
        "category": "Databases"
    },
    {
        "title": "Spanner Evaluation over SLP-Compressed Documents",
        "authors": "['Markus L. Schmid', 'Nicole Schweikardt']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "We consider the problem of evaluating regular spanners over compressed documents, i.e., we wish to solve evaluation tasks directly on the compressed data, without decompression. As compressed forms of the documents we use straight-line programs (SLPs) --- a lossless compression scheme for textual data widely used in different areas of theoretical computer science and particularly well-suited for algorithmics on compressed data. In data complexity, our results are as follows. For a regular spanner M and an SLP $\\mathcalS $ of size $\\mathbfs $ that represents a document D, we can solve the tasks of model checking and of checking non-emptiness in time $O(\\mathbfs )$. Computing the set $łlbracket M \\rrbracket(D)$ of all span-tuples extracted from D can be done in time $Ø(\\mathbfs |łlbracket M \\rrbracket(D)|)$, and enumeration of $łlbracket M \\rrbracket(D)$ can be done with linear preprocessing $O(\\mathbfs )$ and a delay of $O(depth\\mathcalS )$, where $depth\\mathcalS $ is the depth of $\\mathcalS $'s derivation tree. Note that $\\mathbfs $ can be exponentially smaller than the document's size $|D|$; and, due to known balancing results for SLPs, we can always assume that $depth\\mathcalS = O(log(|D|))$ independent of D's compressibility. Hence, our enumeration algorithm has a delay logarithmic in the size of the non-compressed data and a preprocessing time that is at best (i.e., in the case of highly compressible documents) also logarithmic, but at worst still linear. Therefore, in a big-data perspective, our enumeration algorithm for SLP-compressed documents may nevertheless beat the known linear preprocessing and constant delay algorithms for non-compressed documents.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458325",
        "category": "Databases"
    },
    {
        "title": "Algorithms for a Topology-aware Massively Parallel Computation Model",
        "authors": "['Xiao Hu', 'Paraschos Koutris', 'Spyros Blanas']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Most of the prior work in massively parallel data processing assumes homogeneity, i.e., every computing unit has the same computational capability and can communicate with every other unit with the same latency and bandwidth. However, this strong assumption of a uniform topology rarely holds in practical settings, where computing units are connected through complex networks. To address this issue, Blanas et al. \\citeblanas2020topology recently proposed a topology-aware massively parallel computation model that integrates the network structure and heterogeneity in the modeling cost. The network is modeled as a directed graph, where each edge is associated with a cost function that depends on the data transferred between the two endpoints. The computation proceeds in synchronous rounds and the cost of each round is measured as the maximum cost over all the edges in the network. In this work, we take the first step into investigating three fundamental data processing tasks in this topology-aware parallel model: set intersection, cartesian product, and sorting. We focus on network topologies that are tree topologies, and present both lower bounds as well as (asymptotically) matching upper bounds. Instead of assuming a worst-case distribution as in previous results, the optimality of our algorithms is with respect to the initial data distribution among the network nodes. Apart from the theoretical optimality of our results, our protocols are simple, use a constant number of rounds, and we believe can be implemented in practical settings as well.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458318",
        "category": "Databases"
    },
    {
        "title": "XStore: Fast RDMA-Based Ordered Key-Value Store Using Remote Learned Cache",
        "authors": "['Xingda Wei', 'Rong Chen', 'Haibo Chen', 'Binyu Zang']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "RDMA (Remote Direct Memory Access) has gained considerable interests in network-attached in-memory key-value stores. However, traversing the remote tree-based index in ordered key-value stores with RDMA becomes a critical obstacle, causing an order-of-magnitude slowdown and limited scalability due to multiple round trips. Using index cache with conventional wisdom—caching partial data and traversing them locally—usually leads to limited effect because of unavoidable capacity misses, massive random accesses, and costly cache invalidations. We argue that the machine learning (ML) model is a perfect cache structure for the tree-based index, termed learned cache. Based on it, we design and implement XStore, an RDMA-based ordered key-value store with a new hybrid architecture that retains a tree-based index at the server to perform dynamic workloads (e.g., inserts) and leverages a learned cache at the client to perform static workloads (e.g., gets and scans). The key idea is to decouple ML model retraining from index updating by maintaining a layer of indirection from logical to actual positions of key-value pairs. It allows a stale learned cache to continue predicting a correct position for a lookup key. XStore ensures correctness using a validation mechanism with a fallback path and further uses speculative execution to minimize the cost of cache misses. Evaluations with YCSB benchmarks and production workloads show that a single XStore server can achieve over 80 million read-only requests per second. This number outperforms state-of-the-art RDMA-based ordered key-value stores (namely, DrTM-Tree, Cell, and eRPC+Masstree) by up to 5.9× (from 3.7×). For workloads with inserts, XStore still provides up to 3.5× (from 2.7×) throughput speedup, achieving 53M reqs/s. The learned cache can also reduce client-side memory usage and further provides an efficient memory-performance tradeoff, e.g., saving 99% memory at the cost of 20% peak throughput.",
        "link": "https://dl.acm.org/doi/10.1145/3468520",
        "category": "Databases"
    },
    {
        "title": "ArkDB: A Key-Value Engine for Scalable Cloud Storage Services",
        "authors": "['Zhu Pang', 'Qingda Lu', 'Shuo Chen', 'Rui Wang', 'Yikang Xu', 'Jiesheng Wu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Persistent key-value stores play a crucial role in enabling internet-scale services. At Alibaba Cloud, scale-out cloud storage services including Object Storage Service, File Storage Service and Tablestore are built on distributed key-value stores. Key challenges in the design of the underlying key-value engine for these services lie in utilization of disaggregated storage, supporting write and range query-heavy workloads, and balancing of scalability, availability and resource usage. This paper presents ArkDB, a key-value engine designed to address these challenges by combining advantages of both LSM tree and Bw-tree, and leveraging advances in hardware technologies. Built on top of Pangu, an append-only distributed file system, ArkDB's innovations include shrinkable page mapping table, clear separation of system and user states for fast recovery, write amplification reduction, efficient garbage collection and lightweight partition split and merge. Experimental results demonstrate ArkDB's improvements over existing designs. Compared with Bw-tree, ArkDB efficiently stabilizes the mapping table size despite continuous write working set growth. Compared with RocksDB, an LSM tree-based key-value engine, ArkDB increases ingestion throughput by 2.16x, while reducing write amplification by 3.1x. It outperforms RocksDB by 52% and 37% respectively on a write-heavy workload and a range query-intensive workload of the Yahoo! Cloud Serving Benchmark. Experiments running in Tablestore in a cluster environment further demonstrate ArkDB's performance on Pangu and its efficient partition split/merge support.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457553",
        "category": "Databases"
    },
    {
        "title": "Efficient Graph Encryption Scheme for Shortest Path Queries",
        "authors": "['Esha Ghosh', 'Seny Kamara', 'Roberto Tamassia']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Graph encryption schemes (introduced by [Chase and Kamara, 2010]) have been receiving growing interest across various disciplines due to their attractive tradeoff between functionality, efficiency and privacy. In this paper, we advance the state of the art on encrypted graph search by providing an efficient graph encryption scheme for shortest path queries. The preprocessing time and space and the query time are proportional to those for building and querying the search structure for the unencrypted graph. Hence, the overhead of providing structured encryption is asymptotically optimal. We implement our scheme and experimentally validate its performance on real world networks. Furthermore, we extend our scheme to support verifiability. Our scheme is the first structured encryption scheme that supports a recursive algorithm, where the number of recursion steps is not known at setup time (unlike the chaining technique from [Chase and Kamara, 2010]). Recursion is an important algorithmic design paradigm. Hence, our technique may help develop other practical encrypted structures for recursive algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453099",
        "category": "Databases"
    },
    {
        "title": "The case for adding privacy-related offloading to smart storage",
        "authors": "['Claudiu Mihali', 'Anca Hangan', 'Gheorghe Sebestyen', 'Zsolt István']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "It is important to ensure that personally identifiable information (PII) is protected within large distributed systems and is used only for intended purposes. Achieving this is challenging and several techniques have been proposed for privacy-preserving analytics, but they typically focus on the end hosts only. We argue that future storage solutions should include, in addition to emerging compute offload, also privacy-related operators. Since many privacy operators, such as perturbation and anonymization, take place as the very first step before other computations, query offload to a Smart Storage device might be only feasible in the future if privacy-related operators can also be offloaded. In this work we demonstrate that privacy-preserving operators can be implemented in hardware without reducing read bandwidths. We focus on perturbations and extend an FPGA-based network-attached Smart Storage solution to show that it is possible to provide these operations at 10Gbps line-rate while using only a small amount of additional FPGA real-estate. We also discuss how future faster smart storage nodes should look like in the light of these additional requirements.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463769",
        "category": "Databases"
    },
    {
        "title": "A Learned Query Optimizer for Spatial Join",
        "authors": "['Tin Vu', 'Alberto Belussi', 'Sara Migliorini', 'Ahmed Eldawy']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "The importance and complexity of spatial join resulted in many join algorithms, some of which run on big-data platforms such as Hadoop and Spark. This paper proposes the first machine-learning-based query optimizer for spatial join operation which can accommodate the skewness of the spatial datasets and the complexity of the different algorithms. The main challenge is how to develop portable cost models that take into account the important input characteristics such as data distribution, spatial partitioning, logic of spatial join algorithms, and the relationship between the two datasets. The proposed system defines a set of features that can all be computed efficiently for the data to catch the intricate aspects of spatial join. Then, it uses these features to train three machine learning models that capture several metrics to estimate the cost of four spatial join algorithms according to user requirements. The first model can estimate the cardinality of spatial join algorithm. The second model can predict the number of rough comparisons for a specific join algorithm. Finally, the third model is a classification model that can choose the best join algorithm to run. Experiments on large scale synthetic and real data show the efficiency of the proposed models over baseline methods.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3484217",
        "category": "Databases"
    },
    {
        "title": "Automated Selection of Multiple Datasets for Extension by Integration",
        "authors": "['Yael Amsterdamer', 'Moran Cohen']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "Organizations often seek to extend their data by integration with available datasets originating from external sources. While there are many tools that recommend how to perform the integration for given datasets, the selection of what datasets to integrate is often challenging in itself. First, the relevant candidates must be efficiently identified among irrelevant ones. Next, relevant datasets need to be evaluated according to issues such as low quality or poor matching to the target data and schema. Last, jointly integrating multiple datasets may have significant benefits such as increasing completeness and information gain, but may also greatly complicate the task due to dependencies in the integration process. To assist administrators in this task, we quantify to what extent an integration of multiple datasets is valuable as an extension of an initial dataset and formalize the computational problem of finding the most valuable subset to integrate by this measure. We formally analyze the problem, showing that it is NP-hard; we nevertheless introduce heuristic efficient algorithms, which our experiments show to be near-optimal in practice and highly effective in finding the most valuable integration.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3482322",
        "category": "Databases"
    },
    {
        "title": "Multi-User Collusion-Resistant Searchable Encryption with Optimal Search Time",
        "authors": "['Yun Wang', 'Dimitrios Papadopoulos']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "The continued development of cloud computing requires technologies that protect users' data privacy even from the cloud providers themselves. Multi-user searchable encryption is one such kind of technology. It allows a data owner to selectively enable users to perform keyword searches over her encrypted documents that are stored at a cloud server. For privacy purposes, it is important to limit what an adversarial server can infer about the encrypted documents, even if it colludes with some of the users. Clearly, in this case it can learn the content of documents shared with this subset of \"corrupted\" users, however, it is important to ensure that this collusion does not reveal information about parts of the dataset that are only shared with the remaining \"uncorrupted\" users via cross-user leakage. In this work, we propose three novel multi-user searchable encryption schemes for this setting that achieve different trade-offs between performance and leakage. Compared to previous ones, our first two schemes are the first to achieve asymptotically optimal search time. Our third scheme achieves minimal user storage and forward privacy with respect to document sharing, but slightly slower search performance. We formally prove the security of our schemes under reasonable assumptions. Moreover, we implement and evaluate their performance both on a single machine and over WAN. Our experimental results are encouraging, e.g., the search computation time is in the order of a few milliseconds.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3437535",
        "category": "Databases"
    },
    {
        "title": "Bao: Making Learned Query Optimization Practical",
        "authors": "['Ryan Marcus', 'Parimarjan Negi', 'Hongzi Mao', 'Nesime Tatbul', 'Mohammad Alizadeh', 'Tim Kraska']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Recent efforts applying machine learning techniques to query optimization have shown few practical gains due to substantive training overhead, inability to adapt to changes, and poor tail performance. Motivated by these difficulties, we introduce Bao (the \\underlineBa ndit \\underlineo ptimizer). Bao takes advantage of the wisdom built into existing query optimizers by providing per-query optimization hints. Bao combines modern tree convolutional neural networks with Thompson sampling, a well-studied reinforcement learning algorithm. As a result, Bao automatically learns from its mistakes and adapts to changes in query workloads, data, and schema. Experimentally, we demonstrate that Bao can quickly learn strategies that improve end-to-end query execution performance, including tail latency, for several workloads containing long-running queries. In cloud environments, we show that Bao can offer both reduced costs and better performance compared with a commercial system.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452838",
        "category": "Databases"
    },
    {
        "title": "Constructing and analyzing the LSM compaction design space",
        "authors": "['Subhadeep Sarkar', 'Dimitris Staratzis', 'Ziehen Zhu', 'Manos Athanassoulis']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Log-structured merge (LSM) trees offer efficient ingestion by appending incoming data, and thus, are widely used as the storage layer of production NoSQL data stores. To enable competitive read performance, LSM-trees periodically re-organize data to form a tree with levels of exponentially increasing capacity, through iterative compactions. Compactions fundamentally influence the performance of an LSM-engine in terms of write amplification, write throughput, point and range lookup performance, space amplification, and delete performance. Hence, choosing the appropriate compaction strategy is crucial and, at the same time, hard as the LSM-compaction design space is vast, largely unexplored, and has not been formally defined in the literature. As a result, most LSM-based engines use a fixed compaction strategy, typically hand-picked by an engineer, which decides how and when to compact data.In this paper, we present the design space of LSM-compactions, and evaluate state-of-the-art compaction strategies with respect to key performance metrics. Toward this goal, our first contribution is to introduce a set of four design primitives that can formally define any compaction strategy: (i) the compaction trigger, (ii) the data layout, (iii) the compaction granularity, and (iv) the data movement policy. Together, these primitives can synthesize both existing and completely new compaction strategies. Our second contribution is to experimentally analyze 10 compaction strategies. We present 12 observations and 7 high-level takeaway messages, which show how LSM systems can navigate the compaction design space.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476274",
        "category": "Databases"
    },
    {
        "title": "Learned cardinality estimation: a design space exploration and a comparative evaluation",
        "authors": "['Ji Sun', 'Jintao Zhang', 'Zhaoyan Sun', 'Guoliang Li', 'Nan Tang']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Cardinality estimation is core to the query optimizers of DBMSs. Non-learned methods, especially based on histograms and samplings, have been widely used in commercial and open-source DBMSs. Nevertheless, histograms and samplings can only be used to summarize one or few columns, which fall short of capturing the joint data distribution over an arbitrary combination of columns, because of the oversimplification of histograms and samplings over the original relational table(s). Consequently, these traditional methods typically make bad predictions for hard cases such as queries over multiple columns, with multiple predicates, and joins between multiple tables. Recently, learned cardinality estimators have been widely studied. Because these learned estimators can better capture the data distribution and query characteristics, empowered by the recent advance of (deep learning) models, they outperform non-learned methods on many cases. The goals of this paper are to provide a design space exploration of learned cardinality estimators and to have a comprehensive comparison of the SOTA learned approaches so as to provide a guidance for practitioners to decide what method to use under various practical scenarios.",
        "link": "https://dl.acm.org/doi/10.14778/3485450.3485459",
        "category": "Databases"
    },
    {
        "title": "Cuckoo Trie: Exploiting Memory-Level Parallelism for Efficient DRAM Indexing",
        "authors": "['Adar Zeitak', 'Adam Morrison']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "We present the Cuckoo Trie, a fast, memory-efficient ordered index structure. The Cuckoo Trie is designed to have memory-level parallelism---which a modern out-of-order processor can exploit to execute DRAM accesses in parallel--- without sacrificing memory efficiency. The Cuckoo Trie thus breaks a fundamental performance barrier faced by current indexes, whose bottleneck is a series of dependent pointer-chasing DRAM accesses---e.g., traversing a search tree path--- which the processor cannot parallelize. Our evaluation shows that the Cuckoo Trie outperforms state-of-the-art-indexes by up to 20%-360% on a variety of datasets and workloads, typically with a smaller or comparable memory footprint.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483551",
        "category": "Databases"
    },
    {
        "title": "Weighted Distinct Sampling: Cardinality Estimation for SPJ Queries",
        "authors": "['Yuan Qiu', 'Yilei Wang', 'Ke Yi', 'Feifei Li', 'Bin Wu', 'Chaoqun Zhan']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "SPJ (select-project-join) queries form the backbone of many SQL queries used in practice. Accurate cardinality estimation of these queries is thus an important problem, with applications in query optimization, approximate query processing, and data analytics. However, this problem has not been rigorously addressed in the literature, despite the fact that cardinality estimation techniques of the three relational operators, selection, projection, and join, have each been extensively studied (but not when used in combination) in the past 30+ years. The major technical difficulty is that (distinct) projection seems to be difficult to combine with the other two operators when it comes to cardinality estimation. In this paper, we give the first formal study of cardinality estimation for SP queries. While it was studied in a prior work in 2001, there is no guarantee on its optimality. We define a class of algorithms, which we call weighted distinct sampling, for estimating SP query sizes, and show how to find a near-optimal sampling strategy that is away from the optimum only by a lower order term. We then extend it to handling SPJ queries, giving the first non-trivial solution for SPJ cardinality estimation. We have also performed an extensive experimental evaluation to complement our theoretical findings.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452821",
        "category": "Databases"
    },
    {
        "title": "Machine Learning Method Based Industrial Risk Analysis and Prediction",
        "authors": "['Salim Khan', 'Fahim Hasan', 'Mohammad Omar Faruk', 'Anayet Ullah', 'Mohammad Woli Ullah', 'Abdul Gafur']",
        "date": "March 2022",
        "source": "ICCA '22: Proceedings of the 2nd International Conference on Computing Advancements",
        "abstract": "IoT-based technologies growing all over the world. After the industrial revolution, machines and robots gradually replaced human effort. In the absence of the human brain-machine and robots makes an error. In this paper, a plan was developed to get out of this situation that works not only efficiently but also thinks like humans. In this system, the machine will learn based on the situation that has been made by any occurrence. In this work Raspberry Pi-based system helps to make a proper analysis of the machines. Voltage, current, gas value, and temperate values are taken as input parameters. Machine learning matches/compares these real-time sensor data with training data (which is used to train the system). As a result, The machine learning module provides some statistics graphs of sensor data. Machine performance can analyze by observing these graphs. Also, determine the efficiency and predict the possibility of upcoming threats or risks.",
        "link": "https://dl.acm.org/doi/10.1145/3542954.3543003",
        "category": "Databases"
    },
    {
        "title": "The Evolution of Search: Three Computing Paradigms",
        "authors": "['Xindong Wu', 'Xingquan Zhu', 'Minghui Wu']",
        "date": "None",
        "source": "ACM Transactions on Management Information Systems",
        "abstract": "Search is probably the most common activity that humans conduct all the time. A search target can be a concrete item (with a yes or no answer and location information), an abstract concept (such as the most important information on the Web about Xindong Wu), or a plan/path for a specific target with an objective function (like flight scheduling with a minimal travel time), among others. In this article, we propose a Universal Connection Theorem (UCT) to suggest that all physical objects/items in the universe are connected through explicit or implicit relationships. Search is to explore the relationships, using different computing methods, to retrieve relevant objects. Under the UCT theorem, we summarize mainstream search approaches into two categories from the user perspective, deterministic search vs. abstract search, and further distinguish them into three computing paradigms: planning based search, data driven search, and knowledge enhanced search. The planning based paradigm explores search as a planning process in a large search space, by graph traversing with heuristic principles to locate optimal solutions. The data driven paradigm seeks to find objects matching the user's query from a large data repository. Indexing, hashing, information retrieval, and recommendations are typical strategies to tackle the data volumes and select the best answers for users’ queries. The knowledge enhanced search does not aim to find matching objects, but to discover and then meet user's search requirements through knowledge mining. The evolution of these three search paradigms, from planning to data engineering and knowledge engineering, provides increasing levels of challenges and opportunities. This article elaborates the respective principles of these paradigms.",
        "link": "https://dl.acm.org/doi/10.1145/3495214",
        "category": "Databases"
    },
    {
        "title": "Combining Aggregation and Sampling (Nearly) Optimally for Approximate Query Processing",
        "authors": "['Xi Liang', 'Stavros Sintos', 'Zechao Shang', 'Sanjay Krishnan']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Sample-based approximate query processing (AQP) suffers from many pitfalls such as the inability to answer very selective queries and unreliable confidence intervals when sample sizes are small. Recent research presented an intriguing solution of combining materialized, pre-computed aggregates with sampling for accurate and more reliable AQP. We explore this solution in detail in this work and propose an AQP physical design called PASS, or Precomputation-Assisted Stratified Sampling. PASS builds a tree of partial aggregates that cover different partitions of the dataset. The leaf nodes of this tree form the strata for stratified samples. Aggregate queries whose predicates align with the partitions (or unions of partitions) are exactly answered with a depth-first search, and any partial overlaps are approximated with the stratified samples. We propose an algorithm for optimally partitioning the data into such a data structure with various practical approximation techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457277",
        "category": "Databases"
    },
    {
        "title": "EIRES: Efficient Integration of Remote Data in Event Stream Processing",
        "authors": "['Bo Zhao', 'Han van der Aa', 'Thanh Tam Nguyen', 'Quoc Viet Hung Nguyen', 'Matthias Weidlich']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "To support reactive and predictive applications, complex event processing (CEP) systems detect patterns in event streams based on predefined queries. To determine the events that constitute a query match, their payload data may need to be assessed together with data from remote sources. Such dependencies are problematic, since waiting for remote data to be fetched interrupts the processing of the stream. Yet, without event selection based on remote data, the query state to maintain may grow exponentially. In either case, the performance of the CEP system degrades drastically. To tackle these issues, we present EIRES, a framework for efficient integration of static data from remote sources in CEP. It employs a cost-model to determine when to fetch certain remote data elements and how long to keep them in a cache for future use. EIRES combines strategies for (i) prefetching that queries remote data based on anticipated use and (ii) lazy evaluation that postpones the event selection based on remote data without interrupting the stream processing. Our experiments indicate that the combination of these strategies improves the latency of query evaluation by up to 3,725x for synthetic data and 47x for real-world data.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457304",
        "category": "Databases"
    },
    {
        "title": "Distributed Decentralized Chain (DDC) and k-Queue Variable Bulk Arrival and Static Bulk Service Model",
        "authors": "['Abhilash Kancharla', 'Jongho Seol', 'Hye-Young Kim', 'Nohpill Park']",
        "date": "May 2021",
        "source": "BSCI '21: Proceedings of the 3rd ACM International Symposium on Blockchain and Secure Critical Infrastructure",
        "abstract": "This paper proposes a quantitative model for a new blockchain technology that distributes public ledger in a decentralized manner, referred to as Distributed Decentralized chain (DDC). A clique of k nodes in the P2P network participates in storing a complete copy of blockchain instead of having every node carry an entire copy. The proposed model is k-Queue Variable Bulk Arrival and Static Bulk Service queueing model (k-VBASBS), in which the state is defined by (i,k), where i is the number of slots from 0 upto n in a block on the current node, and k is the number of distributed nodes to store a complete copy of a chain of blocks. Without loss of generality and practicality, it is assumed that there are two different transaction posting rates assumed to take into account the overhead of inter-node (i.e., μ(inter-node)) control-hopping versus the one of the original intra-node posting rate (i.e., μ(intra-node)), and μ(inter-node)μ<<(intra-node). Based on the proposed k-VBASBS model, the average waiting time, space requirement, and throughput of the transactions will be simulated for the performance, and the dependability will be also modeled and simulated by the vulnerability to 51% attack.",
        "link": "https://dl.acm.org/doi/10.1145/3457337.3457840",
        "category": "Databases"
    },
    {
        "title": "STAR: A Cache-based Distributed Warehouse System for Spatial Data Streams",
        "authors": "['Zhida Chen', 'Gao Cong', 'Walid G. Aref']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "The proliferation of mobile phones and location-based services has given rise to an explosive growth in spatial data. In order to enable spatial data analytics, spatial data needs to be streamed into a data stream warehouse system that can provide real-time analytical results over the most recent and historical spatial data in the warehouse. Existing data stream warehouse systems are not tailored for spatial data. In this paper, we introduce the STAR (Spatial Data Stream Warehouse) system. STAR is a distributed in-memory data stream warehouse system that provides low-latency and up-to-date analytical results over a fast-arriving spatial data stream. STAR supports queries that are composed of aggregate functions and ad hoc query constraints over spatial, textual, and temporal data attributes. STAR implements a cache-based mechanism to facilitate the processing of queries that collectively utilizes the techniques of query-based caching (i.e., view materialization) and object-based caching. Extensive experiments over real data sets demonstrate the superior performance of STAR over existing systems.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3484265",
        "category": "Databases"
    },
    {
        "title": "Weighted Distinct Sampling: Cardinality Estimation for SPJ Queries",
        "authors": "['Yuan Qiu', 'Yilei Wang', 'Ke Yi', 'Feifei Li', 'Bin Wu', 'Chaoqun Zhan']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "SPJ (select-project-join) queries form the backbone of many SQL queries used in practice. Accurate cardinality estimation of these queries is thus an important problem, with applications in query optimization, approximate query processing, and data analytics. However, this problem has not been rigorously addressed in the literature, despite the fact that cardinality estimation techniques of the three relational operators, selection, projection, and join, have each been extensively studied (but not when used in combination) in the past 30+ years. The major technical difficulty is that (distinct) projection seems to be difficult to combine with the other two operators when it comes to cardinality estimation. In this paper, we give the first formal study of cardinality estimation for SP queries. While it was studied in a prior work in 2001, there is no guarantee on its optimality. We define a class of algorithms, which we call weighted distinct sampling, for estimating SP query sizes, and show how to find a near-optimal sampling strategy that is away from the optimum only by a lower order term. We then extend it to handling SPJ queries, giving the first non-trivial solution for SPJ cardinality estimation. We have also performed an extensive experimental evaluation to complement our theoretical findings.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452821",
        "category": "Databases"
    },
    {
        "title": "PCOR: Private Contextual Outlier Release via Differentially Private Search",
        "authors": "['Masoumeh Shafieinejad', 'Florian Kerschbaum', 'Ihab F. Ilyas']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Outlier detection plays a significant role in various real world applications such as intrusion, malfunction, and fraud detection. Traditionally, outlier detection techniques are applied to find outliers in the context of the whole dataset. However, this practice neglects the data points, namely contextual outliers, that are not outliers in the whole dataset but in some specific neighborhoods. Contextual outliers are particularly important in data exploration and targeted anomaly explanation and diagnosis. In these scenarios, the data owner computes the following information: i) The attributes that contribute to the abnormality of an outlier (metric), ii) Contextual description of the outlier's neighborhoods (context), and iii) The utility score of the context, e.g. its strength in showing the outlier's significance, or in relation to a particular explanation for the outlier. However, revealing the outlier's context leaks information about the other individuals in the population as well, violating their privacy. We address the issue of population privacy violations in this paper. There are two main challenges in defining and applying privacy in contextual outlier release. In this setting, the data owner is required to release a valid context for the queried record, i.e. a context in which the record is an outlier. Hence, the first major challenge is that the privacy technique must preserve the validity of the context for each record. We propose techniques to protect the privacy of individuals through a relaxed notion of differential privacy to satisfy this requirement. The second major challenge is applying the proposed techniques efficiently, as they impose intensive computation to the base algorithm. To overcome this challenge, we propose a graph structure to map the contexts to, and introduce differentially private graph search algorithms as efficient solutions for the computation problem caused by differential privacy techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452812",
        "category": "Databases"
    },
    {
        "title": "Elastic and Stable Compaction for LSM-tree: A FaaS-Based Approach on TerarkDB",
        "authors": "['Jianchuan Li', 'Peiquan Jin', 'Yuanjin Lin', 'Ming Zhao', 'Yi Wang', 'Kuankuan Guo']",
        "date": "October 2021",
        "source": "CIKM '21: Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "abstract": "LSM-tree is widely used as a write-optimized storage engine in many NoSQL systems. However, the periodical compaction operations in LSM-tree cost many I/O bandwidths and CPU resources of the local server, resulting in throughput drops of the system. To address this issue, this paper proposes a new compaction scheme based on the FaaS (Functions as a Service) architecture, which is called FaaS Compaction. It utilizes the elastic computing capability of FaaS and always pushes compactions to a FaaS cluster. The FaaS cluster will perform actual compaction operations, which will not affect the processing of the local server. Therefore, we can maintain stable performance even when periodical compactions are triggered. We also present a Parallel Slight Compaction method to solve the timeout problem caused by heavy compactions. We implement the FaaS Compaction based on TerarkDB and a real FaaS cluster and experimentally compare the FaaS Compaction with the RocksDB's local compaction scheme and the state-of-the-art offloading compaction policy. The results suggest the efficiency, stability, and elasticity of our proposal.",
        "link": "https://dl.acm.org/doi/10.1145/3459637.3481913",
        "category": "Databases"
    },
    {
        "title": "LES3: learning-based exact set similarity search",
        "authors": "['Yifan Li', 'Xiaohui Yu', 'Nick Koudas']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Set similarity search is a problem of central interest to a wide variety of applications such as data cleaning and web search. Past approaches on set similarity search utilize either heavy indexing structures, incurring large search costs or indexes that produce large candidate sets. In this paper, we design a learning-based exact set similarity search approach, LES3. Our approach first partitions sets into groups, and then utilizes a light-weight bitmap-like indexing structure, called token-group matrix (TGM), to organize groups and prune out candidates given a query set. In order to optimize pruning using the TGM, we analytically investigate the optimal partitioning strategy under certain distributional assumptions. Using these results, we then design a learning-based partitioning approach called L2P and an associated data representation encoding, PTR, to identify the partitions. We conduct extensive experiments on real and synthetic datasets to fully study LES3, establishing the effectiveness and superiority over other applicable approaches.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476263",
        "category": "Databases"
    },
    {
        "title": "Inference from Visible Information and Background Knowledge",
        "authors": "['Michael Benedikt', 'Pierre Bourhis', 'Balder Ten Cate', 'Gabrieled Puppis', 'Michael Vanden Boom']",
        "date": "None",
        "source": "ACM Transactions on Computational Logic",
        "abstract": "We provide a wide-ranging study of the scenario where a subset of the relations in a relational vocabulary is visible to a user—that is, their complete contents are known—while the remaining relations are invisible. We also have a background theory—invariants given by logical sentences—that may relate the visible relations to invisible ones, and also may constrain both the visible and invisible relations in isolation. We want to determine whether some other information, given as a positive existential formula, can be inferred using only the visible information and the background theory. This formula whose inference we are concerned with is denoted as the query. We consider whether positive information about the query can be inferred, and also whether negative information—the sentence does not hold—can be inferred. We further consider both the instance-level version of the problem, where both the query and the visible instance are given, and the schema-level version, where we want to know whether truth or falsity of the query can be inferred in some instance of the schema.",
        "link": "https://dl.acm.org/doi/10.1145/3452919",
        "category": "Databases"
    },
    {
        "title": "Shahin: Faster Algorithms for Generating Explanations for Multiple Predictions",
        "authors": "['Sona Hasani', 'Saravanan Thirumuruganathan', 'Nick Koudas', 'Gautam Das']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Machine learning (ML) models have achieved widespread adoption in the last few years. Generating concise and accurate explanations often increases user trust and understanding of the model prediction. Usually, the implementations of popular explanation algorithms are highly optimized for a single prediction. In practice, explanations often have to be generated in a batch for multiple predictions at a time. To the best of our knowledge, there has been no work for efficiently generating explanations for more than one prediction. While one could use multiple machines to generate explanations in parallel, this approach is sub-optimal as it does not leverage higher-level optimizations that are available in a batch setting. We propose a principled and lightweight approach for identifying redundant computations and several effective heuristics for dramatically speeding up explanation generation. Our techniques are general and could be applied to a wide variety of perturbation based explanation algorithms. We demonstrate this over a diverse set of algorithms including, LIME, Anchor, and SHAP. Our empirical experiments show that our methods impose very little overhead and require minimal modification to the explanation algorithms. They achieve significant speedup over baseline approaches that generate explanations in a sequential manner.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457332",
        "category": "Databases"
    },
    {
        "title": "Contract-based return-value commutativity: safely exploiting contract-based commutativity for faster serializable transactions",
        "authors": "['Tim Soethout', 'Tijs van der Storm', 'Jurgen J. Vinju']",
        "date": "October 2021",
        "source": "AGERE 2021: Proceedings of the 11th ACM SIGPLAN International Workshop on Programming Based on Actors, Agents, and Decentralized Control",
        "abstract": "A key challenge of designing distributed software systems is maintaining data consistency. We can define data consistency and data isolation guarantees --e.g. serializability-- in terms of schedules of atomic reads and writes, but this excludes schedules that would be semantically consistent. Others use manually provided information on \"non-conflicting operations\" to define guarantees that work for more applications allowing more parallel schedules. To be safe, an engineer might avoid marking operations as non-conflicting, with detrimental effects to efficiency. To be fast, they might mark more non-conflicting operations than is strictly safe.   Our goal is to help engineers by automatically deriving commutative operations (using their respective contracts) such that more parallel schedules with global consistency are possible. We define a new general consistency and isolation guarantee named \"Return-Value Serializability\" to check consistency claims automatically, and we present distributed event processing algorithms that make use of the same \"Contract-based Commutativity\" information. We validated both the definitions and the algorithms using model-checking with TLA+. Previous work provided evidence that local coordination avoidance such as applied here has a significant positive effect on the performance of distributed transaction systems.   Client-centric return-value commutativity promises to hit a sweet spot in design trade-offs for business applications, such as payment systems, that must scale-out while their operations are not embarrassingly parallel and consistency guarantees are of the highest priority. It can also provide design feedback, indicating that some operations will simply not scale together even before a line of code has been written.",
        "link": "https://dl.acm.org/doi/10.1145/3486601.3486707",
        "category": "Databases"
    },
    {
        "title": "Temporal Geo-Social Personalized Keyword Search Over Streaming Data",
        "authors": "['Abdulaziz Almaslukh', 'Yunfan Kang', 'Amr Magdy']",
        "date": "None",
        "source": "ACM Transactions on Spatial Algorithms and Systems",
        "abstract": "The unprecedented rise of social media platforms, combined with location-aware technologies, has led to continuously producing a significant amount of geo-social data that flows as a user-generated data stream. This data has been exploited in several important use cases in various application domains. This article supports geo-social personalized queries in streaming data environments. We define temporal geo-social queries that provide users with real-time personalized answers based on their social graph. The new queries allow incorporating keyword search to get personalized results that are relevant to certain topics. To efficiently support these queries, we propose an indexing framework that provides lightweight and effective real-time indexing to digest geo-social data in real time. The framework distinguishes highly dynamic data from relatively stable data and uses appropriate data structures and a storage tier for each. Based on this framework, we propose a novel geo-social index and adopt two baseline indexes to support the addressed queries. The query processor then employs different types of pruning to efficiently access the index content and provide a real-time query response. The extensive experimental evaluation based on real datasets has shown the superiority of our proposed techniques to index real-time data and provide low-latency queries compared to existing competitors.",
        "link": "https://dl.acm.org/doi/10.1145/3473006",
        "category": "Databases"
    },
    {
        "title": "Investigating Sharing of Cyber Threat Intelligence and Proposing A New Data Model for Enabling Automation in Knowledge Representation and Exchange",
        "authors": "['Siri Bromander', 'Morton Swimmer', 'Lilly Pijnenburg Muller', 'Audun Jøsang', 'Martin Eian', 'Geir Skjøtskift', 'Fredrik Borg']",
        "date": "None",
        "source": "Digital Threats: Research and Practice",
        "abstract": "For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.",
        "link": "https://dl.acm.org/doi/10.1145/3458027",
        "category": "Databases"
    },
    {
        "title": "Clonos: Consistent Causal Recovery for Highly-Available Streaming Dataflows",
        "authors": "['Pedro F. Silvestre', 'Marios Fragkoulis', 'Diomidis Spinellis', 'Asterios Katsifodimos']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Stream processing lies in the backbone of modern businesses, being employed for mission critical applications such as real-time fraud detection, car-trip fare calculations, traffic management, and stock trading. Large-scale applications are executed by scale-out stream processing systems on thousands of long-lived operators, which are subject to failures. Recovering from failures fast and consistently are both top priorities, yet they are only partly satisfied by existing fault tolerance methods due to the strong assumptions these make. In particular, prior solutions fail to address consistency in the presence of nondeterminism, such as calls to external services, asynchronous timers and processing-time windows. This paper describes Clonos, a fault tolerance approach that achieves fast, local operator recovery with exactly-once guarantees and high availability by instantly switching to passive standby operators. Clonos enforces causally consistent recovery, including output deduplication, by tracking nondeterminism within the system through causal logging. To implement Clonos we re-engineered many of the internal subsystems of a state of the art stream processor. We evaluate Clonos' overhead and recovery on the Nexmark benchmark against Apache Flink. Clonos achieves instant recovery with negligible overhead and, unlike previous work, does not make assumptions on the deterministic nature of operators.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457320",
        "category": "Databases"
    },
    {
        "title": "Consistency and Completeness: Rethinking Distributed Stream Processing in Apache Kafka",
        "authors": "['Guozhang Wang', 'Lei Chen', 'Ayusman Dikshit', 'Jason Gustafson', 'Boyang Chen', 'Matthias J. Sax', 'John Roesler', 'Sophie Blee-Goldman', 'Bruno Cadonna', 'Apurva Mehta', 'Varun Madan', 'Jun Rao']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "An increasingly important system requirement for distributed stream processing applications is to provide strong correctness guarantees under unexpected failures and out-of-order data so that its results can be authoritative (not needing complementary batch results). Although existing systems have put a lot of effort into addressing some specific issues, such as consistency and completeness, how to enable users to make flexible and transparent trade-off decisions among correctness, performance, and cost still remains a practical challenge. Specifically, similar mechanisms are usually applied to tackle both consistency and completeness, which can result in unnecessary performance penalties. We present Apache Kafka's core design for stream processing, which relies on its persistent log architecture as the storage and inter-processor communication layers to achieve correctness guarantees. Kafka Streams, a scalable stream processing client library in Apache Kafka, defines the processing logic as read-process-write cycles in which all processing state updates and result outputs are captured as log appends. Idempotent and transactional write protocols are utilized to guarantee exactly-once semantics. Furthermore, revision-based speculative processing is employed to emit results as soon as possible while handling out-of-order data. We also demonstrate how Kafka Streams behaves in practice with large-scale deployments and performance insights exhibiting its flexible and low-overhead trade-offs.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457556",
        "category": "Databases"
    },
    {
        "title": "Distributed Spatio-Temporal k Nearest Neighbors Join",
        "authors": "['Ruiyuan Li', 'Rubin Wang', 'Junwen Liu', 'Zisheng Yu', 'Huajun He', 'Tianfu He', 'Sijie Ruan', 'Jie Bao', 'Chao Chen', 'Fuqiang Gu', 'Liang Hong', 'Yu Zheng']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "The rapid development of positioning technology produces an extremely large volume of spatio-temporal data with various geometry types such as point, line string, polygon, or a mixed combination of them. As one of the most basic but time-consuming operations, k nearest neighbors join (kNN join) has attracted much attention. However, most existing works for kNN join either ignore temporal information or consider point data only. This paper proposes a novel and useful problem, i.e., ST-kNN join, which considers both spatial closeness and temporal concurrency. To support ST-kNN join over a huge amount of spatio-temporal data with any geometry types efficiently, we propose a novel distributed solution based on Apache Spark. Specifically, our method adopts a two-round join framework. In the first round join, we propose a new spatio-temporal partitioning method that achieves spatio-temporal locality and load balance at the same time. We also propose a lightweight index structure, i.e., Time Range Count Index (TRC-index), to enable efficient ST-kNN join. In the second round join, to reduce the data transmission among different machines, we remove duplicates based on spatio-temporal reference points before shuffling local results. Extensive experiments are conducted using three real big datasets, showing that our method is much more scalable and achieves 9X faster than baselines. A demonstration system is deployed and the source code is released.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3484209",
        "category": "Databases"
    },
    {
        "title": "Scalable Representation Learning for Dynamic Heterogeneous Information Networks via Metagraphs",
        "authors": "['Yang Fang', 'Xiang Zhao', 'Peixin Huang', 'Weidong Xiao', 'Maarten de Rijke']",
        "date": "None",
        "source": "ACM Transactions on Information Systems",
        "abstract": "Content representation is a fundamental task in information retrieval. Representation learning is aimed at capturing features of an information object in a low-dimensional space. Most research on representation learning for heterogeneous information networks (HINs) focuses on static HINs. In practice, however, networks are dynamic and subject to constant change. In this article, we propose a novel and scalable representation learning model, M-DHIN, to explore the evolution of a dynamic HIN. We regard a dynamic HIN as a series of snapshots with different time stamps. We first use a static embedding method to learn the initial embeddings of a dynamic HIN at the first time stamp. We describe the features of the initial HIN via metagraphs, which retains more structural and semantic information than traditional path-oriented static models. We also adopt a complex embedding scheme to better distinguish between symmetric and asymmetric metagraphs. Unlike traditional models that process an entire network at each time stamp, we build a so-called change dataset that only includes nodes involved in a triadic closure or opening process, as well as newly added or deleted nodes. Then, we utilize the above metagraph-based mechanism to train on the change dataset. As a result of this setup, M-DHIN is scalable to large dynamic HINs since it only needs to model the entire HIN once while only the changed parts need to be processed over time. Existing dynamic embedding models only express the existing snapshots and cannot predict the future network structure. To equip M-DHIN with this ability, we introduce an LSTM-based deep autoencoder model that processes the evolution of the graph via an LSTM encoder and outputs the predicted graph. Finally, we evaluate the proposed model, M-DHIN, on real-life datasets and demonstrate that it significantly and consistently outperforms state-of-the-art models.",
        "link": "https://dl.acm.org/doi/10.1145/3485189",
        "category": "Databases"
    },
    {
        "title": "Safety of quantified ABoxes w.r.t. singleton εL policies",
        "authors": "['Franz Baader', 'Francesco Kriegel', 'Adrian Nuradiansyah', 'Rafael Peñaloza']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "In recent work, we have shown how to compute compliant anonymizations of quantified ABoxes w.r.t. εL policies. In this setting, quantified ABoxes can be used to publish information about individuals, some of which are anonymized. The policy is given by concepts of the Description Logic (DL) εL, and compliance means that one cannot derive from the ABox that some non-anonymized individual is an instance of a policy concept. If one assumes that a possible attacker could have additional knowledge about some of the involved non-anonymized individuals, then compliance with a policy is not sufficient. One wants to ensure that the quantified ABox is safe in the sense that none of the secret instance information is revealed, even if the attacker has additional compliant knowledge. In the present paper, we show that safety can be decided in polynomial time, and that the unique optimal safe anonymization of a non-safe quantified ABox can be computed in exponential time, provided that the policy consists of a single εL concept.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3441961",
        "category": "Databases"
    },
    {
        "title": "Parallax: Hybrid Key-Value Placement in LSM-based Key-Value Stores",
        "authors": "['Giorgos Xanthakis', 'Giorgos Saloustros', 'Nikos Batsaras', 'Anastasios Papagiannis', 'Angelos Bilas']",
        "date": "November 2021",
        "source": "SoCC '21: Proceedings of the ACM Symposium on Cloud Computing",
        "abstract": "Key-value (KV) separation is a technique that introduces randomness in the I/O access patterns to reduce I/O amplification in LSM-based key-value stores. KV separation has a significant drawback that makes it less attractive: Delete and update operations in modern workloads result in frequent and expensive garbage collection (GC) in the value log. In this paper, we design and implement Parallax, which proposes hybrid KV placement to reduce GC overhead significantly and increases the benefits of using a log. We first model the benefits of KV separation for different KV pair sizes. We use this model to classify KV pairs in three categories small, medium, and large. Then, Parallax uses different approaches for each KV category: It always places large values in a log and small values in place. For medium values it uses a mixed strategy that combines the benefits of using a log and eliminates GC overhead as follows: It places medium values in a log for all but the last few (typically one or two) levels in the LSM structure, where it performs a full compaction, merges values in place, and reclaims log space without the need for GC. We evaluate Parallax against RocksDB that places all values in place and BlobDB that always performs KV separation. We find that Parallax increases throughput by up to 12.4x and 17.83x, decreases I/O amplification by up to 27.1x and 26x, and increases CPU efficiency by up to 18.7x and 28x, respectively, for all but scan-based YCSB workloads.",
        "link": "https://dl.acm.org/doi/10.1145/3472883.3487012",
        "category": "Databases"
    },
    {
        "title": "Abstractions, their algorithms, and their compilers",
        "authors": "['Alfred Aho', 'Jeffrey Ullman']",
        "date": "February 2022",
        "source": "Communications of the ACM",
        "abstract": "Jeffrey D. Ullman and Alfred V. Aho are recipients of the 2020 ACM A.M. Turing award. They were recognized for creating fundamental algorithms and theory underlying programming language implementation and for synthesizing these results and those of others in their highly influential books, which educated generations of computer scientists.",
        "link": "https://dl.acm.org/doi/10.1145/3490685",
        "category": "Databases"
    },
    {
        "title": "VLDB Panel Summary: \"The Future of Data(base) Education: Is the Cow Book Dead?\"",
        "authors": "['Zack Ives', 'Johannes Gehrke', 'Jana Giceva', 'Arun Kumar', 'Rachel Pottinger']",
        "date": "September 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Database education is at an inflection point. With the surge of interest in all things \"data\", enrollments in traditional database courses are at an all time high. At the same time, the rise of Data Science as a discipline has led to the creation of new courses whose content significantly overlaps that of an introductory database course (e.g. data preparation, cleaning, SQL). Students from all across campus aspire to take data science courses, even with limited Computer Science backgrounds. This juxtaposition of content and proliferation of audiences is causing many database educators to question what we should be teaching in our data-oriented courses, and what resources we should use to teach them.",
        "link": "https://dl.acm.org/doi/10.1145/3503780.3503786",
        "category": "Databases"
    },
    {
        "title": "Out of Many We are One: Measuring Item Batch with Clock-Sketch",
        "authors": "['Peiqing Chen', 'Dong Chen', 'Lingxiao Zheng', 'Jizhou Li', 'Tong Yang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Item batch denotes a consecutive sequence of identical items that are close in time in a data stream. It is a useful data stream pattern in cache, burst detection, APT detection, \\etc Basic item batch measurement tasks include membership, cardinality, time span and size. Currently, there is no algorithm tailored for item batch measurement. The greatest challenge lies in accurately estimating the time gap between two consecutive identical items. In this paper, we propose Clock-sketch, a framework that introduces the well-known CLOCK algorithm into item batch measurement. The methodology of Clock-sketch is to clean outdated information as much as possible, while guaranteeing that the information of all items visited within the time window $\\mathcalT $ is preserved. We conduct experiments on three real-world datasets that feature in item batch pattern. We compare the accuracy and throughput performance of our Clock-sketch against the state-of-the-art and two naive approaches without using Clock-sketch technique. Results of item batch activeness show that Clock-sketch outperforms the state-of-the-art SWAMP in generating 50 times less false positive rate when memory is small. All source codes are open-sourced and released at Github.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452784",
        "category": "Databases"
    },
    {
        "title": "A Closer Look: Evaluating Location Privacy Empirically",
        "authors": "['Liyue Fan', 'Ishan Gote']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "The breach of users' location privacy can be catastrophic. To provide users with privacy protections, numerous location privacy methods have been developed in the last two decades. While several studies surveyed existing location privacy methods, the lack of comparative, empirical evaluations imposes challenges for adopting location privacy by applications and researchers who may not be privacy experts. This study fills the gap by conducting a comparative evaluation among a range of location privacy methods with real-world datasets. To evaluate utility, we consider different types of measures, e.g., distortion and mobility metrics; to evaluate privacy protection, we design two empirical privacy risk measures via inference and re-identification attacks. Furthermore, we study the computational overheads inflicted by location privacy in CPU time and memory requirement. The results are thoroughly examined in our work and show that it is possible to strike a balance between utility and privacy when sharing location data with untrusted servers.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3484219",
        "category": "Databases"
    },
    {
        "title": "A Survey on Big Data Processing Frameworks for Mobility Analytics",
        "authors": "['Christos Doulkeridis', 'Akrivi Vlachou', 'Nikos Pelekis', 'Yannis Theodoridis']",
        "date": "June 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "In the current era of big spatial data, the vast amount of produced mobility data (by sensors, GPS-equipped devices, surveillance networks, radars, etc.) poses new challenges related to mobility analytics. A cornerstone facilitator for performing mobility analytics at scale is the availability of big data processing frameworks and techniques tailored for spatial and spatio-temporal data. Motivated by this pressing need, in this paper, we provide a survey of big data processing frameworks for mobility analytics. Particular focus is put on the underlying techniques; indexing, partitioning, query processing are essential for enabling efficient and scalable data management. In this way, this report serves as a useful guide of state-of-the-art methods and modern techniques for scalable mobility data management and analytics.",
        "link": "https://dl.acm.org/doi/10.1145/3484622.3484626",
        "category": "Databases"
    },
    {
        "title": "A Synopsis Based Approach for Itemset Frequency Estimation over Massive Multi-Transaction Stream",
        "authors": "['Guangtao Wang', 'Gao Cong', 'Ying Zhang', 'Zhen Hai', 'Jieping Ye']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "The streams where multiple transactions are associated with the same key are prevalent in practice, e.g., a customer has multiple shopping records arriving at different time. Itemset frequency estimation on such streams is very challenging since sampling based methods, such as the popularly used reservoir sampling, cannot be used. In this article, we propose a novel k-Minimum Value (KMV) synopsis based method to estimate the frequency of itemsets over multi-transaction streams. First, we extract the KMV synopses for each item from the stream. Then, we propose a novel estimator to estimate the frequency of an itemset over the KMV synopses. Comparing to the existing estimator, our method is not only more accurate and efficient to calculate but also follows the downward-closure property. These properties enable the incorporation of our new estimator with existing frequent itemset mining (FIM) algorithm (e.g., FP-Growth) to mine frequent itemsets over multi-transaction streams. To demonstrate this, we implement a KMV synopsis based FIM algorithm by integrating our estimator into existing FIM algorithms, and we prove it is capable of guaranteeing the accuracy of FIM with a bounded size of KMV synopsis. Experimental results on massive streams show our estimator can significantly improve on the accuracy for both estimating itemset frequency and FIM compared to the existing estimators.",
        "link": "https://dl.acm.org/doi/10.1145/3465238",
        "category": "Databases"
    },
    {
        "title": "In the land of data streams where synopses are missing, one framework to bring them all",
        "authors": "['Rudi Poepsel-Lemaitre', 'Martin Kiefer', 'Joscha von Hein', 'Jorge-Arnulfo Quiané-Ruiz', 'Volker Markl']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In pursuit of real-time data analysis, approximate summarization structures, i.e., synopses, have gained importance over the years. However, existing stream processing systems, such as Flink, Spark, and Storm, do not support synopses as first class citizens, i.e., as pipeline operators. Synopses' implementation is upon users. This is mainly because of the diversity of synopses, which makes a unified implementation difficult. We present Condor, a framework that supports synopses as first class citizens. Condor facilitates the specification and processing of synopsis-based streaming jobs while hiding all internal processing details. Condor's key component is its model that represents synopses as a particular case of windowed aggregate functions. An inherent divide and conquer strategy allows Condor to efficiently distribute the computation, allowing for high-performance and linear scalability. Our evaluation shows that Condor outperforms existing approaches by up to a factor of 75x and that it scales linearly with the number of cores.",
        "link": "https://dl.acm.org/doi/10.14778/3467861.3467871",
        "category": "Databases"
    },
    {
        "title": "Design of LSM-tree-based Key-value SSDs with Bounded Tails",
        "authors": "['Junsu Im', 'Jinwook Bae', 'Chanwoo Chung', 'Arvind', 'Sungjin Lee']",
        "date": "None",
        "source": "ACM Transactions on Storage",
        "abstract": "Key-value store based on a log-structured merge-tree (LSM-tree) is preferable to hash-based key-value store, because an LSM-tree can support a wider variety of operations and show better performance, especially for writes. However, LSM-tree is difficult to implement in the resource constrained environment of a key-value SSD (KV-SSD), and, consequently, KV-SSDs typically use hash-based schemes. We present PinK, a design and implementation of an LSM-tree-based KV-SSD, which compared to a hash-based KV-SSD, reduces 99th percentile tail latency by 73%, improves average read latency by 42%, and shows 37% higher throughput. The key idea in improving the performance of an LSM-tree in a resource constrained environment is to avoid the use of Bloom filters and instead, use a small amount of DRAM to keep/pin the top levels of the LSM-tree. We also find that PinK is able to provide a flexible design space for a wide range of KV workloads by leveraging the read-write tradeoff in LSM-trees.",
        "link": "https://dl.acm.org/doi/10.1145/3452846",
        "category": "Databases"
    },
    {
        "title": "Non-Equivocation in Blockchain: Double-Authentication-Preventing Signatures Gone Contractual",
        "authors": "['Yannan Li', 'Willy Susilo', 'Guomin Yang', 'Yong Yu', 'Tran Viet Xuan Phuong', 'Dongxi Liu']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Equivocation is one of the most fundamental problems that need to be solved when designing distributed protocols. Traditional methods to defeat equivocation rely on trusted hardware or particular assumptions, which may hinder their adoption in practice. The advent of blockchain and decentralized cryptocurrencies provides an auspicious breakthrough paradigm to resolve the problem above. In this paper, we propose a blockchain-based solution to address contractual equivocation, which supports user-defined fine-grained policy-based equivocation. Specifically, users will be de-incentive if the statements they made breach the predefined access rules. The core of our solution is a newly introduced primitive named Policy-Authentication-Preventing Signature (PoAPS), which combined with a deposit mechanism allows a signer to make conflict statements corresponding to a policy to be penalized. We present a generic construction of PoAPS based on Policy-Based Verifiable Secret Sharing (PBVSS) and demonstrate its practicality via a concrete implementation in the blockchain. Compared with the existing solutions that only handle specific types of equivocation, our proposed approach is more generic and can be instantiated to deal with various kinds of equivocation.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3437516",
        "category": "Databases"
    },
    {
        "title": "Learned Cardinality Estimation for Similarity Queries",
        "authors": "['Ji Sun', 'Guoliang Li', 'Nan Tang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "In this paper, we study the problem of using deep neural networks (DNNs) for estimating the cardinality of similarity queries. Intuitively, DNNs can capture the distribution of data points, and learn to predict the number of data points that are similar to one data point (a similarity search) or a set of data points (a similarity join). However, DNNs are data hungry; directly training a DNN often results in poor performance. We propose two strategies to improve the accuracy and reduce the size of training data: query segmentation and data segmentation. Query segmentation divides a query into query segments, trains a neural network for each query segment, and combines their outputs with subsequent DNNs to get the query embedding. Data segmentation groups similar data into data segments, train a local-model for each data segment, and learn a global-model to decide which local-models should be used for a given query. The estimates from selected local-models will be summed up as the final estimate.We also extend our model to support similarity joins, which trains a DNN to directly estimate the cumulative sum of objects that are similar to a set of queries. Experiments show that our methods can efficiently (i.e., with small training data) learn to estimate the cardinality of similarity searches/joins, and yield effective estimates (i.e., close to real cardinalities).",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452790",
        "category": "Databases"
    },
    {
        "title": "Relative Error Streaming Quantiles",
        "authors": "['Graham Cormode', 'Zohar Karnin', 'Edo Liberty', 'Justin Thaler', 'Pavel Veselý']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Approximating ranks, quantiles, and distributions over streaming data is a central task in data analysis and monitoring. Given a stream of n items from a data universe U equipped with a total order, the task is to compute a sketch (data structure) of size poly (log(n), 1/ε). Given the sketch and a query item y ∈ U, one should be able to approximate its rank in the stream, i.e., the number of stream elements smaller than or equal to y. Most works to date focused on additive ε n error approximation, culminating in the KLL sketch that achieved optimal asymptotic behavior. This paper investigates multiplicative (1±ε)$-error approximations to the rank. Practical motivation for multiplicative error stems from demands to understand the tails of distributions, and hence for sketches to be more accurate near extreme values. The most space-efficient algorithms due to prior work store either O(log(ε2 n)/ε2) or O(log3(ε n)/ε) universe items. This paper presents a randomized algorithm storing O(log1.5 (ε n)/ε) items, which is within an O(√log(ε n)) factor of optimal. The algorithm does not require prior knowledge of the stream length and is fully mergeable, rendering it suitable for parallel and distributed computing environments.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458323",
        "category": "Databases"
    },
    {
        "title": "Interactive Search for One of the Top-k",
        "authors": "['Weicheng Wang', 'Raymond Chi-Wing Wong', 'Min Xie']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "When a large dataset is given, it is not desirable for a user to read all tuples one-by-one in the whole dataset to find satisfied tuples. The traditional top-k query finds the best k tuples (i.e., the top-k tuples) w.r.t. the user's preference. However, in practice, it is difficult for a user to specify his/her preference explicitly. We study how to enhance the top-k query with user interaction. Specifically, we ask a user several questions, each of which consists of two tuples and asks the user to indicate which one s/he prefers. Based on the feedback, the user's preference is learned implicitly and one of the top-k tuples w.r.t. the learned preference is returned. Here, instead of directly following the top-k query to return all the top-k tuples, since it requires heavy user effort during the interaction (e.g., answering many questions), we reduce the output size to strike for a trade-off between the user effort and the output size. To achieve this, we present an algorithm 2D-PI which asks an asymptotically optimal number of questions in a 2-dimensional space, and two algorithms HD-PI and RH with provable performance guarantee in a d-dimensional space (d >= 2), where they focus on the number of questions asked and the execution time, respectively. Experiments were conducted on synthetic and real datasets, showing that our algorithms outperform existing ones by asking fewer questions within less time to return satisfied tuples.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457322",
        "category": "Databases"
    },
    {
        "title": "Indexing cloud data lakes within the lakes",
        "authors": "['Grisha Weintraub', 'Ehud Gudes', 'Shlomi Dolev']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "Cloud data lakes are a modern approach for storing large amounts of data in a convenient and inexpensive way. The main idea is the separation of compute and storage layers. However, to perform analytics on the data in this architecture, the data should be moved from the storage layer to the compute layer over the network for each calculation. Obviously, that hurts calculation performance and requires huge network bandwidth. We are exploring different approaches for adding indexing to the cloud data lakes with the goal of reducing the amounts of data read from the storage, and as a result, improving query execution time.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463828",
        "category": "Databases"
    },
    {
        "title": "Parallelizing Intra-Window Join on Multicores: An Experimental Study",
        "authors": "['Shuhao Zhang', 'Yancan Mao', 'Jiong He', 'Philipp M. Grulich', 'Steffen Zeuch', 'Bingsheng He', 'Richard T. B. Ma', 'Volker Markl']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The intra-window join (IaWJ), i.e., joining two input streams over a single window, is a core operation in modern stream processing applications. This paper presents the first comprehensive study on parallelizing the IaWJ on modern multicore architectures. In particular, we classify IaWJ algorithms into lazy and eager execution approaches. For each approach, there are further design aspects to consider, including different join methods and partitioning schemes, leading to a large design space. Our results show that none of the algorithms always performs the best, and the choice of the most performant algorithm depends on: (i) workload characteristics, (ii) application requirements, and (iii) hardware architectures. Based on the evaluation results, we propose a decision tree that can guide the selection of an appropriate algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452793",
        "category": "Databases"
    },
    {
        "title": "Debugging missing answers for spark queries over nested data with breadcrumb",
        "authors": "['Ralf Diestelkämper', 'Seokki Lee', 'Boris Glavic', 'Melanie Herschel']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "We present Breadcrumb, a system that aids developers in debugging queries through query-based explanations for missing answers. Given as input a query and an expected, but missing, query result, Breadcrumb identifies operators in the input query that are responsible for the failure to derive the missing answer. These operators form explanations that guide developers who can then focus their debugging efforts on fixing these parts of the query. Breadcrumb is implemented on top of Apache Spark. Our approach is the first that scales to big data dimensions and is capable of finding explanations for common errors in queries over nested and de-normalized data, e.g., errors based on misinterpreting schema semantics.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476331",
        "category": "Databases"
    },
    {
        "title": "Hybrid Edge Partitioner: Partitioning Large Power-Law Graphs under Memory Constraints",
        "authors": "['Ruben Mayer', 'Hans-Arno Jacobsen']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Distributed systems that manage and process graph-structured data internally solve a graph partitioning problem to minimize their communication overhead and query run-time. Besides computational complexity---optimal graph partitioning is NP-hard---another important consideration is the memory overhead. Real-world graphs often have an immense size, such that loading the complete graph into memory for partitioning is not economical or feasible. Currently, the common approach to reduce memory overhead is to rely on streaming partitioning algorithms. While the latest streaming algorithms lead to reasonable partitioning quality on some graphs, they are still not completely competitive to in-memory partitioners. In this paper, we propose a new system, Hybrid Edge Partitioner (HEP), that can partition graphs that fit partly into memory while yielding a high partitioning quality. HEP can flexibly adapt its memory overhead by separating the edge set of the graph into two sub-sets. One sub-set is partitioned by NE++, a novel, efficient in-memory algorithm, while the other sub-set is partitioned by a streaming approach. Our evaluations on large real-world graphs show that in many cases, HEP outperforms both in-memory partitioning and streaming partitioning at the same time. Hence, HEP is an attractive alternative to existing solutions that cannot fine-tune their memory overheads. Finally, we show that using HEP, we achieve a significant speedup of distributed graph processing jobs on Spark/GraphX compared to state-of-the-art partitioning algorithms.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457300",
        "category": "Databases"
    },
    {
        "title": "Accelerating Triangle Counting on GPU",
        "authors": "['Lin Hu', 'Lei Zou', 'Yu Liu']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Triangle counting is an important problem in graph mining, which has achieved great performance improvement on GPU in recent years. Instead of proposing a new GPU triangle counting algorithm, in this paper, we propose a novel lightweight graph preprocessing method to boost many state-of-the-art GPU triangle counting algorithms without changing their implementations and data structures. Specifically, we find common computing patterns in existing algorithms, and abstract two analytic models to measure how workload imbalance and diversity in these computing patterns affect performance exactly. Then, due to the NP-hardness of the model optimization, we propose approximate solutions by determining edge directions to balance workloads and reordering vertices to maximize the degree of parallelism within GPU blocks. Finally, extensive experiments confirm the significant performance improvement and high usability of our approach.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452815",
        "category": "Databases"
    },
    {
        "title": "BDS&#x002B;: An Inter-Datacenter Data Replication System With Dynamic Bandwidth Separation",
        "authors": "['Yuchao Zhang', 'Xiaohui Nie', 'Junchen Jiang', 'Wendong Wang', 'Ke Xu', 'Youjian Zhao', 'Martin J. Reed', 'Kai Chen', 'Haiyang Wang', 'Guang Yao']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Networking",
        "abstract": "Many important cloud services require replicating massive data from one datacenter (DC) to multiple DCs. While the performance of pair-wise inter-DC data transfers has been much improved, prior solutions are insufficient to optimize bulk-data multicast, as they fail to explore the rich inter-DC overlay paths that exist in geo-distributed DCs, as well as the remaining bandwidth reserved for online traffic under fixed bandwidth separation scheme. To take advantage of these opportunities, we present <italic>BDS&#x002B;</italic>, a near-optimal network system for large-scale inter-DC data replication. BDS&#x002B; is an application-level multicast overlay network with a <italic>fully centralized</italic> architecture, allowing a central controller to maintain an up-to-date global view of data delivery status of intermediate servers, in order to fully utilize the available overlay paths. Furthermore, in each overlay path, it leverages dynamic bandwidth separation to make use of the remaining available bandwidth reserved for online traffic. By constantly estimating online traffic demand and rescheduling bulk-data transfers accordingly, BDS&#x002B; can further speed up the massive data multicast. Through a pilot deployment in one of the largest online service providers and large-scale real-trace simulations, we show that BDS&#x002B; can achieve 3-<inline-formula> <tex-math notation=\"LaTeX\">$5\\times $ </tex-math></inline-formula> speedup over the provider&#x2019;s existing system and several well-known overlay routing baselines of static bandwidth separation. Moreover, dynamic bandwidth separation can further reduce the completion time of bulk data transfers by 1.2 to 1.3 times.",
        "link": "https://dl.acm.org/doi/10.1109/TNET.2021.3054924",
        "category": "Databases"
    },
    {
        "title": "Handwritten New Tai Lue Character Recognition Using Convolutional Prior Features and Deep Variationally Sparse Gaussian Process Modeling",
        "authors": "['H. Guo', 'N. Dong', 'J. Y. Zhao', 'Y. F. Liu']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "New Tai Lue is widely used in Southwest China and Southeast Asia. Hence, it is important to study related handwritten character recognition. Considering the many similar characters in handwritten New Tai Lue, this paper proposes an offline handwritten New Tai Lue character recognition method based on convolutional prior features and deep variationally sparse Gaussian process (DVSGP) modeling. An offline handwritten database is constructed, a convolutional neural network is trained to extract the convolutional features of New Tai Lue character images as prior features, and a DVSGP model is built. The extracted features are input into the DVSGP model to construct a recognition model. The experimental results show that the accuracy of the model is 97.67% and that the precision, recall, and F1-score are 0.9769, 0.9767, and 0.9767, respectively, which are better than those of other methods. The proposed method also achieves high accuracy on the MNIST recognition task, verifying its universal applicability.",
        "link": "https://dl.acm.org/doi/10.1145/3506700",
        "category": "Databases"
    },
    {
        "title": "Analyzing Code-mixing in Linguistic Corpora Using Kratylos",
        "authors": "['Raphael Finkel', 'Daniel Kaufman', 'Ahmed Shamim']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "Code-switching, code-mixing, and, more generally, multilingualism pose technological challenges for language documentation, the sub-discipline of linguistics that deals with the annotation and basic analysis of field recordings and other primary data. We focus here on a case study involving code-mixing in the endangered Koda language, which poses special problems for morphosyntactic analysis. We offer a robust approach to multilingual annotations that involves a combination of the popular open source software FieldWorks Language Explorer (FLEx) with Kratylos, a web-based corpus tool for display and query. Kratylos exposes linguistic data from various formats to powerful regular-expression queries that can exploit tier structure and other aspects of interlinear glossed text. We show how Kratylos can target mixed structures in our FLEx database of Koda that cannot be easily identified within the original FLEx software itself.",
        "link": "https://dl.acm.org/doi/10.1145/3480238",
        "category": "Databases"
    },
    {
        "title": "TraNCE: transforming nested collections efficiently",
        "authors": "['Jaclyn Smith', 'Michael Benedikt', 'Brandon Moore', 'Milos Nikolic']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Nested relational query languages have long been seen as an attractive tool for scenarios involving large hierarchical datasets. There has been a resurgence of interest in nested relational languages. One driver has been the affinity of these languages for large-scale processing platforms such as Spark and Flink.This demonstration gives a tour of TraNCE, a new system for processing nested data on top of distributed processing systems. The core innovation of the system is a compiler that processes nested relational queries in a series of transformations; these include variants of two prior techniques, shredding and unnesting, as well as a materialization transformation that customizes the way levels of the nested output are generated. The TraNCE platform builds on these techniques by adding components for users to create and visualize queries, as well as data exploration and notebook execution targets to facilitate the construction of large-scale data science applications. The demonstration will both showcase the system from the viewpoint of usability by data scientists and illustrate the data management techniques employed.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476330",
        "category": "Databases"
    },
    {
        "title": "Heterogeneous Face Recognition with Attention-guided Feature Disentangling",
        "authors": "['Shanmin Yang', 'Xiao Yang', 'Yi Lin', 'Peng Cheng', 'Yi Zhang', 'Jianwei Zhang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "This paper proposes an attention-guided feature disentangling framework (AgFD) to eliminate the large cross-modality discrepancy for Heterogeneous Face Recognition (HFR). Existing HFR methods either focus only on extracting identity features or impose linear/no independence constraints on the decomposed components. Instead, our AgFD disentangles the facial representation and forces intrinsic independence between identity features and identity-irrelevant variations. To this end, an Attention-based Residual Decomposition Module (AbRDM) and an Adversarial Decorrelation Module (ADM) are presented. AbRDM provides hierarchical complementary feature disentanglement, while ADM is introduced for decorrelation learning. Extensive experiments on the challenging CASIA NIR-VIS 2.0 Database, Oulu-CASIA NIR&VIS Database, BUAA-VisNir Database, and IIIT-D Viewed Sketch Database demonstrate the generalization ability and competitive performance of the proposed method.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475546",
        "category": "Databases"
    },
    {
        "title": "Examining Security for Different Data Models*",
        "authors": "['Vartika Puri', 'Shelly Sachdeva']",
        "date": "August 2021",
        "source": "IC3-2021: Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing",
        "abstract": "Efficient data storage and retrieval in many sectors led to the development of various modeling techniques such as relational model, Entity Attribute Value model and dynamic tables. The aim of this study is to classify the standard threats of the database according to the violation of security properties followed by the examination of different data models from their security viewpoint. A system is said to be secure if it follows three basic pillars of security i.e. confidentiality, integrity and availability. The current research analyses the security threats in database according to violation of basic pillars of security with detailed analysis of SQL injection attack for three data modeling techniques, namely relational model, Entity Attribute Value (EAV) model and dynamic tables. It presents a comparison of achieving security parameters by performing various experiments on the database stored in MySQL and proposes techniques for the application of mandatory access control in EAV model and dynamic table. After the rigorous survey and experiments performed, it has been found that EAV model is still not a completely secured model. The data leakage in EAV model is more and the application of security properties is relatively more complex than relational model and dynamic tables. Researches have been conducted in the past on these models but very few of them have discussed the security concerns of EAV model and dynamic tables. This paper tries to compare various data models based on security concerns and highlights the security issues in EAV model.",
        "link": "https://dl.acm.org/doi/10.1145/3474124.3474195",
        "category": "Databases"
    },
    {
        "title": "Study on the Mechanism of Coptis Chinensis in Treating Gastric Ulcer Based on Network Pharmacology and Computer Technology",
        "authors": "['Qionghui Duan', 'Yong Li']",
        "date": "January 2022",
        "source": "ICBBB '22: Proceedings of the 2022 12th International Conference on Bioscience, Biochemistry and Bioinformatics",
        "abstract": "Objective: Investigate the mechanism of Coptis chinensis in the treatment of Gastric Ulcer based on network pharmacology and computer technology. Methods: The chemical constituts were collected based on the TCMSP database by using Coptis chinensis as the key word; GeneCards were used to select the target genes related to Gastric Ulcer from the human gene database, and the Gastric Ulcer's related target genes were screened by Genemap in the OMIM database, Disgenet database and Genecards database. We used R language VennDiagram package to crosse the gene targets of drugs and diseases, and screenthe target of the main components of Coptis chinensis in the treatment of Gastric Ulcer, and to use Cytoscape software to map the gene regulatory network of galangal in the treatment of Gastric Ulcer, and to use the String database to construct the gene-protein of Coptis chinensis. The interaction visualization network map for protein-protein interaction (PPI) were screened out the core genes. On this basis, gene ontology process analysis and Kyoto Encyclopedia of genes and genomes pathway enrichment analysis were performed using Bioconductor database. Results: Fourteen main active compounds of Coptis chinensis were screened, and 138 gene targets may be involved in the treatment of Gastric Ulcer. The main function of Coptis in the treatment of gastric ulcer is to combine various proteins with different functions such as Akt1, EGFR, MAPK3, TNF etc. The results of GO and KEGG pathway analysis indicated that the pathways involved include P13K/Akt, VEGF and TNF signaling pathway and a variety of cancer pathways. Conclusions: Coptis in the treatment of gastric ulcer in the process of the use of multi-channel joint collaborative approach, laid the foundation for further research.",
        "link": "https://dl.acm.org/doi/10.1145/3510427.3510435",
        "category": "Databases"
    },
    {
        "title": "Lock-free Contention Adapting Search Trees",
        "authors": "['Kjell Winblad', 'Konstantinos Sagonas', 'Bengt Jonsson']",
        "date": "None",
        "source": "ACM Transactions on Parallel Computing",
        "abstract": "Concurrent key-value stores with range query support are crucial for the scalability and performance of many applications. Existing lock-free data structures of this kind use a fixed synchronization granularity. Using a fixed synchronization granularity in a concurrent key-value store with range query support is problematic as the best performing synchronization granularity depends on a number of factors that are difficult to predict, such as the level of contention and the number of items that are accessed by range queries. We present the first linearizable lock-free key-value store with range query support that dynamically adapts its synchronization granularity. This data structure is called the lock-free contention adapting search tree (LFCA tree). An LFCA tree automatically performs local adaptations of its synchronization granularity based on heuristics that take contention and the performance of range queries into account. We show that the operations of LFCA trees are linearizable, that the lookup operation is wait-free, and that the remaining operations (insert, remove and range query) are lock-free. Our experimental evaluation shows that LFCA trees achieve more than twice the throughput of related lock-free data structures in many scenarios. Furthermore, LFCA trees are able to perform substantially better than data structures with a fixed synchronization granularity over a wide range of scenarios due to their ability to adapt to the scenario at hand.",
        "link": "https://dl.acm.org/doi/10.1145/3460874",
        "category": "Databases"
    },
    {
        "title": "Quality Assessment of End-to-End Learned Image Compression: The Benchmark and Objective Measure",
        "authors": "['Yang Li', 'Shiqi Wang', 'Xinfeng Zhang', 'Shanshe Wang', 'Siwei Ma', 'Yue Wang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Recently, learning-based lossy image compression has achieved notable breakthroughs with their excellent modeling and representation learning capabilities. Comparing to traditional image codecs based on block partitioning and transform, these data-driven approaches with artificial-neural-network (ANN) structures bring significantly different distortion patterns. Efficient objective image quality assessment (IQA) measures play the key role in quantitative evaluation and optimization of image compression algorithms. In this paper, we construct a large-scale image database for quality assessment of compressed images. In the proposed database, 100 reference images are compressed to different quality levels by 10 codecs, involving both traditional and learning-based codecs. Based on this database, we present a benchmark for existing IQA methods and reveal the challenges of IQA on learning-based compression distortions. Furthermore, we develop an objective quality assessment framework in which a self-attention module is adopted to leverage multi-level features from reference and compressed images. Extensive experiments demonstrate the superiority of our method in terms of prediction accuracy. The subjective and objective study of various compressed images also shed lights on the optimization of image compression methods.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475569",
        "category": "Databases"
    },
    {
        "title": "P2H: Efficient Distance Querying on Road Networks by Projected Vertex Separators",
        "authors": "['Zitong Chen', 'Ada Wai-Chee Fu', 'Minhao Jiang', 'Eric Lo', 'Pengfei Zhang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The most efficient known approach for shortest distance querying on road networks is via a tree decomposition based 2-hop labeling index. A major challenge here is how to reduce the query time by reducing the label size. To this end, we propose P2H with the novel ideas of projected vertex separators and optimized selection of vertex separators. We also introduce mechanisms for index maintenance for edge weight updating. Our experiments on multiple real road networks show that P2H can greatly reduce the effective label sizes and query time over existing algorithms. For larger datasets, P2H is around twice as efficient as the best known algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3459245",
        "category": "Databases"
    },
    {
        "title": "Timely Reporting of Heavy Hitters Using External Memory",
        "authors": "['Shikha Singh', 'Prashant Pandey', 'Michael A. Bender', 'Jonathan W. Berry', 'Martín Farach-Colton', 'Rob Johnson', 'Thomas M. Kroeger', 'Cynthia A. Phillips']",
        "date": "None",
        "source": "ACM Transactions on Database Systems",
        "abstract": "Given an input stream S of size N, a ɸ-heavy hitter is an item that occurs at least ɸN times in S. The problem of finding heavy-hitters is extensively studied in the database literature.We study a real-time heavy-hitters variant in which an element must be reported shortly after we see its T = ɸ N-th occurrence (and hence it becomes a heavy hitter). We call this the Timely Event Detection (TED) Problem. The TED problem models the needs of many real-world monitoring systems, which demand accurate (i.e., no false negatives) and timely reporting of all events from large, high-speed streams with a low reporting threshold (high sensitivity).Like the classic heavy-hitters problem, solving the TED problem without false-positives requires large space (Ω (N) words). Thus in-RAM heavy-hitters algorithms typically sacrifice accuracy (i.e., allow false positives), sensitivity, or timeliness (i.e., use multiple passes).We show how to adapt heavy-hitters algorithms to external memory to solve the TED problem on large high-speed streams while guaranteeing accuracy, sensitivity, and timeliness. Our data structures are limited only by I/O-bandwidth (not latency) and support a tunable tradeoff between reporting delay and I/O overhead. With a small bounded reporting delay, our algorithms incur only a logarithmic I/O overhead.We implement and validate our data structures empirically using the Firehose streaming benchmark. Multi-threaded versions of our structures can scale to process 11M observations per second before becoming CPU bound. In comparison, a naive adaptation of the standard heavy-hitters algorithm to external memory would be limited by the storage device’s random I/O throughput, i.e., ≈100K observations per second.",
        "link": "https://dl.acm.org/doi/10.1145/3472392",
        "category": "Databases"
    },
    {
        "title": "LandmarkMiner: Street-level Network Landmarks Mining Method for IP Geolocation",
        "authors": "['Ruixiang Li', 'Rui Xu', 'Yuanyuan Ma', 'Xiangyang Luo']",
        "date": "None",
        "source": "ACM Transactions on Internet of Things",
        "abstract": "High-confidence network landmarks are the basis of IP geolocation. However, existing landmarks acquisition methods had weakness such as high time cost and insufficient landmarks number. For this, LandmarkMiner, a street-level network landmarks mining method, is proposed based on service identification and domain name association. First, LandmarkMiner trains classifiers using the scanning results of IPs with known hosting service type, identifies the hosting service type of target IPs using the trained classifiers, and obtains the classified IPs’ domain names using DNS. Then, according to institutional names, a database associating institutional name with possible domain names is built by statistical relationship, which is obtained between the known institutional names and their domain names. Finally, geographical location of IP's domain name after classification is matched in the database and online maps, thereby obtaining landmarks and evaluating reliability of them. LandmarkMiner has mined 9,423 reliable street-level landmarks from 304M IPs in 18 cities. Comparing with existing methods, LandmarkMiner increases the number of reliable street-level landmarks significantly and can be applied in different network connectivity conditions.",
        "link": "https://dl.acm.org/doi/10.1145/3457409",
        "category": "Databases"
    },
    {
        "title": "Social-Spatial Group Queries with Keywords",
        "authors": "['Sajid Hasan Apon', 'Mohammed Eunus Ali', 'Bishwamittra Ghosh', 'Timos Sellis']",
        "date": "None",
        "source": "ACM Transactions on Spatial Algorithms and Systems",
        "abstract": "Social networks with location enabling technologies, also known as geo-social networks, allow users to share their location-specific activities and preferences through check-ins. A user in such a geo-social network can be attributed to an associated location (spatial), her preferences as keywords (textual), and the connectivity (social) with her friends. The fusion of social, spatial, and textual data of a large number of users in these networks provide an interesting insight for finding meaningful geo-social groups of users supporting many real-life applications, including activity planning and recommendation systems. In this article, we introduce a novel query, namely, Top-k Flexible Socio-Spatial Keyword-aware Group Query (SSKGQ), which finds the best k groups of varying sizes around different points of interest (POIs), where the groups are ranked based on the social and textual cohesiveness among members and spatial closeness with the corresponding POI and the number of members in the group. We develop an efficient approach to solve the SSKGQ problem based on our theoretical upper bounds on distance, social connectivity, and textual similarity. We prove that the SSKGQ problem is NP-Hard and provide an approximate solution based on our derived relaxed bounds, which run much faster than the exact approach by sacrificing the group quality slightly. Our extensive experiments on real data sets show the effectiveness of our approaches in different real-life settings.",
        "link": "https://dl.acm.org/doi/10.1145/3475962",
        "category": "Databases"
    },
    {
        "title": "IoTranx: Transactions for Safer Smart Spaces",
        "authors": "['Chao Chen', 'Abdelsalam (Sumi) Helal', 'Zhi Jin', 'Mingyue Zhang', 'Choonhwa Lee']",
        "date": "None",
        "source": "ACM Transactions on Cyber-Physical Systems",
        "abstract": "Smart spaces such as smart homes deliver digital services to optimize space use and enhance user experience. They are composed of an Internet of Things (IoT), people, and physical content. They differ from traditional computer systems in that their cyber-physical nature ties intimately with the users and the built environment. The impact of ill-programmed applications in such spaces goes beyond loss of data or a computer crash, risking potentially physical harm to the space and its users. Ensuring smart space safety is therefore critically important to successfully deliver intimate and convenient services surrounding our daily lives. By modeling smart space as a highly dynamic database, we present IoT Transactions, an analogy to database transactions, as an abstraction for programming and executing the services as the handling of the devices in smart space. Unlike traditional database management systems that take a “clear room approach,” smart spaces take a “dirty room approach” where imperfection and unattainability of full control and guarantees are the new normal. We identify Atomicity, Isolation, Integrity and Durability (AI2D) as the set of properties necessary to define the safe runtime behavior for IoT transactions for maintaining “permissible device settings” of execution and to avoid or detect and resolve “impermissible settings.” Furthermore, we introduce a lock protocol, utilizing variations of lock concepts, that enforces AI2D safety properties during transaction processing. We show a brief proof of the protocol correctness and a detailed analytical model to evaluate its performance.",
        "link": "https://dl.acm.org/doi/10.1145/3471937",
        "category": "Databases"
    },
    {
        "title": "The Study of Emotional Brain to Detect Emotions Using Brain EEG Signals and Improving Accuracy of Emotion Detection System Using Feature Selection Techniques",
        "authors": "['Nisha Vishnupant Kimmatkar', 'Vijay Babu']",
        "date": "February 2022",
        "source": "ICMVA '22: Proceedings of the 2022 5th International Conference on Machine Vision and Applications",
        "abstract": "Now a days Emotion detection using brain EEG signal is becoming interest area of many researchers because of it's tremendous application in healthcare and BCI field. Database acquisition, pre-processing, feature extraction and classification are the main stages in this process. In this research study first existing database of brain EEG signal are studied. Most of the researchers used DEAP database for emotion classification. DEAP database is especially made for music recommendation system. Because of the non-linear and non- stationary nature and poor spatial resolution of Brain EEG signals, researchers faced challenges in each phase of emotion detection process. It is found that the classification accuracy is very low. It becomes necessary to study emotional brain and according to that select electrodes for emotion detection to improve classification accuracy. In this research study self-created dataset is used. Two way approach is used for feature selection to improve accuracy. In the first approach least correlated features are omitted from feature set. and in the second approach RFE recursive feature elimination technique is used for feature ranking. The features ranked high are considered in feature set. It is found that classification accuracy is improved using these techniques.",
        "link": "https://dl.acm.org/doi/10.1145/3523111.3523115",
        "category": "Databases"
    },
    {
        "title": "Pipa: Privacy-preserving Password Checkup via Homomorphic Encryption",
        "authors": "['Jie Li', 'Yamin Liu', 'Shuang Wu']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Data-breach is not rare on Internet, and once it happens, web users may suffer from privacy leakage and property loss. In order to enable web users to conveniently check whether their confidential information is compromised in data-breach events while preserving their privacy, we design Pipa, which is essentially a special case of private set intersection protocol instantiated with homomorphic encryption. We choose the password checkup scenario for an entry point. In the architecture of Pipa, a server is needed to maintain the database of leaked accounts, namely usernames and passwords, and a homomorphic encryption (HE) module is deployed at the user-end. Once the user issues a checkup query to the server, the HE model encrypts the hash of the user's account information and sends the ciphertexts to the server. The server then evaluates a compare-add-multiply circuit on the ciphertexts and the database, and sends a result ciphertext back to the user-end. Finally the HE module decrypts the result and informs the user if the account information is leaked. The server will never know the username or password, or whether the user's information has matched some entry in the database. We have tested the prototype implementation of Pipa with the Brakerski-Fan-Vercauteren (BFV) HE scheme. By choosing proper parameters, the implementation is pretty practical on PC. For the most light-weight parameter settings in the paper, the total communication volume can be as low as about 2.2MB, and it only takes the server 0.17 seconds to finish the homomorphic computation on encrypted data.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3457535",
        "category": "Databases"
    },
    {
        "title": "Meteorological forecasting based on big data analysis",
        "authors": "['Shadi Aljawarneh', 'Juan Alfonso Lara Torralbo']",
        "date": "April 2021",
        "source": "DATA'21: International Conference on Data Science, E-learning and Information Systems 2021",
        "abstract": "In this paper, we present the main ideas behind the development of a system that can be used to deal with meteorological big data. In particular, the system captures data online and downloads it locally onto a MongoDB database. After that, the user can create a particular database and corresponding minable views for analysis. The results provided by the systems are predictive models with the ability to predict some weather-related variables, such as temperature and rainfall. The system has been validated from a triple perspective (usability, experts’ validation, and performance assessment), obtaining satisfactory results. This paper aims to be a brief guide for authors who intend to developed similar systems either in the meteorological field or other domains generating big amounts of data.",
        "link": "https://dl.acm.org/doi/10.1145/3460620.3460622",
        "category": "Databases"
    },
    {
        "title": "Digging into Big Provenance (with SPADE): A user interface for querying provenance",
        "authors": "['Ashish Gehani', 'Raza Ahmad', 'Hassan Irshad', 'Jianqiao Zhu', 'Jignesh Patel']",
        "date": "May-June 2021",
        "source": "Queue",
        "abstract": "Several interfaces exist for querying provenance. Many are not flexible in allowing users to select a database type of their choice. Some provide query functionality in a data model that is different from the graph-oriented one that is natural for provenance. Others have intuitive constructs for finding results but have limited support for efficiently chaining responses, as needed for faceted search. This article presents a user interface for querying provenance that addresses these concerns and is agnostic to the underlying database being used.",
        "link": "https://dl.acm.org/doi/10.1145/3475965.3476885",
        "category": "Databases"
    },
    {
        "title": "QRS complex recognition based on adaptive wavelet threshold and Hilbert transform for continuous ECGs",
        "authors": "['Xiaolei Chen', 'Tingting Sun', 'Yilin Xie', 'Dunan Li', 'Muhammad Saad Khan']",
        "date": "January 2022",
        "source": "BIC 2022: 2022 2nd International Conference on Bioinformatics and Intelligent Computing",
        "abstract": "The wearable continuous ECG monitoring and cardiovascular disease detection system has the characteristics of strong noises and big continuous data. It puts forward higher requirements on the accuracy and efficiency of the QRS recognition algorithm. The currently used QRS detection algorithms still have the problem of missed detection and false detection for continuous ECG data. Therefore, a fast R-peak recognition method based on self-adaptive wavelet threshold and Hilbert transform is proposed for processing noisy continuous ECGs. First, wavelet self-adaptive threshold filter is used to denoise the dynamic ECGs, and then the first-order difference, Shannon energy envelope extraction, Hilbert transform and self-adaptive threshold back-check technology are employed for R-peak detection. Using the proposed method, experimental results show that the accuracy, sensitivity, and specificity of the MIT-BIH arrhythmia database are 99.79%, 99.92%, and 99.87%, respectively. Using the PhysioNet/CinC Challenge 2014 dynamic ECG database, the accuracy, sensitivity and specificity are 96.89%, 97.92% and 98.92%, respectively. Furthermore, the realtime ECGs collecting from the portable ECG detector mECG-101 are also used to evaluate the method. The experimental results show that the accuracy, sensitivity and specificity reach to 97.75%, 98.25% and 99.47%, respectively. Compared with Pan-Tompkins algorithm and wavelet transform algorithm, the proposed method has higher detection accuracy and generalization ability, especially for the wide and low-amplitude QRS complexes.",
        "link": "https://dl.acm.org/doi/10.1145/3523286.3524579",
        "category": "Databases"
    },
    {
        "title": "Informing Cyber Threat Intelligence through Dark Web Situational Awareness: The AZSecure Hacker Assets Portal",
        "authors": "['Sagar Samtani', 'Weifeng Li', 'Victor Benjamin', 'Hsinchun Chen']",
        "date": "None",
        "source": "Digital Threats: Research and Practice",
        "abstract": "To increase situational awareness, major cybersecurity platforms offer Cyber Threat Intelligence (CTI) about emerging cyber threats, key threat actors, and their modus operandi. However, this intelligence is often reactive, as it analyzes event log files after attacks have already occurred, lacking more active scrutiny of potential threats brewing in cyberspace before an attack has occurred. One intelligence source receiving significant attention is the Dark Web, where significant quantities of malicious hacking tools and other cyber assets are hosted. We present the AZSecure Hacker Assets Portal (HAP). The Dark Web-based HAP collects, analyzes, and reports on the major Dark Web data sources to offer unique perspective of hackers, their cybercriminal assets, and their intentions and motivations, ultimately contributing CTI insights to improve situational awareness. HAP currently supports 200+ users internationally from academic institutions such as UT San Antonio and National Taiwan University, law enforcement entities such as Calgary and Ontario Provincial Police, and industry organizations including General Electric and PayPal.",
        "link": "https://dl.acm.org/doi/10.1145/3450972",
        "category": "Databases"
    },
    {
        "title": "Current Trends in Data Summaries",
        "authors": "['Graham Cormode']",
        "date": "December 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "The research area of data summarization seeks to find small data structures that can be updated flexibly, and answer certain queries on the input accurately. Summaries are widely used across the area of data management, and are studied from both theoretical and practical perspectives. They are the subject of ongoing research to improve their performance and broaden their applicability. In this column, recent developments in data summarization are surveyed, with the intent of inspiring further advances.",
        "link": "https://dl.acm.org/doi/10.1145/3516431.3516433",
        "category": "Databases"
    },
    {
        "title": "Conformance Constraint Discovery: Measuring Trust in Data-Driven Systems",
        "authors": "['Anna Fariha', 'Ashish Tiwari', 'Arjun Radhakrishna', 'Sumit Gulwani', 'Alexandra Meliou']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The reliability of inferences made by data-driven systems hinges on the data's continued conformance to the systems' initial settings and assumptions. When serving data (on which we want to apply inference) deviates from the profile of the initial training data, the outcome of inference becomes unreliable. We introduce conformance constraints, a new data profiling primitive tailored towards quantifying the degree of non-conformance, which can effectively characterize if inference over that tuple is untrustworthy. Conformance constraints are constraints over certain arithmetic expressions (called projections) involving the numerical attributes of a dataset, which existing data profiling primitives such as functional dependencies and denial constraints cannot model. Our key finding is that projections that incur low variance on a dataset construct effective conformance constraints. This principle yields the surprising result that low-variance components of a principal component analysis, which are usually discarded for dimensionality reduction, generate stronger conformance constraints than the high-variance components. Based on this result, we provide a highly scalable and efficient technique--linear in data size and cubic in the number of attributes--for discovering conformance constraints for a dataset. To measure the degree of a tuple's non-conformance with respect to a dataset, we propose a quantitative semantics that captures how much a tuple violates the conformance constraints of that dataset. We demonstrate the value of conformance constraints on two applications: trusted machine learning and data drift. We empirically show that conformance constraints offer mechanisms to (1) reliably detect tuples on which the inference of a machine-learned model should not be trusted, and (2) quantify data drift more accurately than the state of the art.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452795",
        "category": "Databases"
    },
    {
        "title": "Self-assessed Emotion Classification from Acoustic and Physiological Features within Small-group Conversation",
        "authors": "['Woan-Shiuan Chien', 'Huang-Cheng Chou', 'Chi-Chun Lee']",
        "date": "October 2021",
        "source": "ICMI '21 Companion: Companion Publication of the 2021 International Conference on Multimodal Interaction",
        "abstract": "Individual (personalized) self-assessed emotion recognition has recently received more attention, such as Human-Centered Artificial Intelligence (AI). In most previous studies, researchers utilized the physiological changes and reactions in the body evoked by multi-media stimuli, e.g., video or music, to build a model for recognizing individuals’ emotions. However, this elicitation approach is less impractical in the human-human interaction because the conversation is dynamic. In this paper, we firstly investigate the individual emotion recognition task under three-person small group conversations. While predicting personalized emotions from physiological signals is well-studied, few studies focus on emotion classification (e.g., happiness and sadness). Most prior works only focus on binary dimensional emotion recognition or regression, such as valence and arousal. Hence, we formulate the individual emotion recognition task into an individual-level emotion classification. In the proposed method, we consider the physiological changes in each individual’s body and acoustic turn-taking dynamics during group conversations for predicting individual emotions. Meanwhile, we assume that the emotional states of humans might be affected by the expressive behaviors of other members during group conversations. Also, we hypothesize that people have a higher probability of feeling specific emotions under the related emotional atmosphere. Therefore, we design an ad-hoc technique by simply summing up the Self-assessed emotional annotations of all group members as the group emotional atmosphere (climate) to help the model predict individuals’ emotions. We propose a Multi-modal Multi-label Emotion based on Transformer BLSTM at Group Emotional Atmosphere Network (MMETBGEAN) that explicitly considers individual changes and dynamic interaction via physiological and acoustic features during a group conversation integrates group emotional atmosphere information for recognizing individuals’ multi-label emotions. We assess the proposed framework on our recently collected extensive Mandarin Chinese collective task group database, NTUBA. The results show that the method outperforms the existing approaches on multi-modal multi-label emotion classification on this database.",
        "link": "https://dl.acm.org/doi/10.1145/3461615.3485411",
        "category": "Databases"
    },
    {
        "title": "The future of data(base) education: is the \"cow book\" dead?",
        "authors": "['Zachary G. Ives', 'Rachel Pottinger', 'Arun Kumar', 'Johannes Gehrke', 'Jana Giceva']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "This panel encourages a debate over the future of database education and its relationship to Data Science: Are Computer Science (CS) and Data Science (DS) different disciplines about to split, and how does that effect how we teach our field? Is there a \"data\" course that belongs in CS that all of our students should take? Who is the traditional database course, e.g. based on the \"cow book\", relevant to? What traditional topics should we not be teaching in our core data course(s) and which ones should be added? What do we teach the student who has one elective for data science? How does our community position itself for leadership in CS given the popularity of DS?",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476394",
        "category": "Databases"
    },
    {
        "title": "The Design and Implementation of Religious Affairs Management Information System Based on WebGIS",
        "authors": "['Chen Zhang']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "As a work of information visualization, this thesis mainly combines information management with digital map services, tentatively structured the bridge between digital theory and anthropological theory, combined with the needs and characteristics of local religious affairs GIS applications, and established SQL Server+ArcSDE spatial database, using WebGIS technology and .NET as the development platform, integrates the spatial database with the attribute database, and realizes the functions of data entry, graphics visualization, query statistics, multi-dimensional topic management, and classified topics.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3501189",
        "category": "Databases"
    },
    {
        "title": "Drop It In Like It's Hot: An Analysis of Persistent Memory as a Drop-in Replacement for NVMe SSDs",
        "authors": "['Maximilian Böther', 'Otto Kißig', 'Lawrence Benson', 'Tilmann Rabl']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "Solid-state drives (SSDs) have improved database system performance significantly due to the higher bandwidth that they provide over traditional hard disk drives. Persistent memory (PMem) is a new storage technology that offers DRAM-like speed at SSD-like capacity. Due to its byte-addressability, research has mainly treated PMem as a replacement of, or an addition to DRAM, e.g., by proposing highly-optimized, DRAM-PMem-hybrid data structures and system designs. However, PMem can also be used via a regular file system interface and standard Linux I/O operations. In this paper, we analyze PMem as a drop-in replacement for Non-Volatile Memory Express (NVMe) SSDs and evaluate possible performance gains while requiring no or only minor changes to existing applications. This drop-in approach speeds-up database systems like Postgres, without requiring any code changes. We systematically evaluate PMem and NVMe SSDs in three database microbenchmarks and the widely used TPC-H benchmark on Postgres. Our experiments show that PMem outperforms a RAID of four NVMe SSDs in read-intensive OLAP workloads by up to 4x without any modifications while achieving similar performance in write-intensive workloads. Finally, we give four practical insights to aid decision-making on when to use PMem as an SSD drop-in replacement and how to optimize for it.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466010",
        "category": "Databases"
    },
    {
        "title": "Learning from the Past: Efficient High-level Synthesis Design Space Exploration for FPGAs",
        "authors": "['Zi Wang', 'Benjamin Carrion Schafer']",
        "date": "None",
        "source": "ACM Transactions on Design Automation of Electronic Systems",
        "abstract": "The quest to democratize the use of Field-Programmable Gate Arrays (FPGAs) has given High-Level Synthesis (HLS) the final push to be widely accepted with FPGA vendors strongly supporting this VLSI design methodology to expand the FPGA user base. HLS takes as input an untimed behavioral description and generates efficient RTL (Verilog or VHDL). One major advantage of HLS is that it allows us to generate a variety of different micro-architectures from the same behavioral description by simply specifying different combination of synthesis options. In particular, commercial HLS tools make extensive use of synthesize directives in the form pragmas. This strength is also a weakness as it forces HLS users to fully understand how these synthesis options work and how they interact to efficiently set them to get a hardware implementation with the desired characteristics. Luckily, this process can be automated. Unfortunately, the search space grows supra-linearly with the number of synthesis options. To address this, this work proposes an automatic synthesis option tuner dedicated for FPGAs. We have explored a larger number of behavioral descriptions targeting ASICs and FPGAs and found out that due to the internal structure of the FPGA a large number of synthesis options combinations never lead to a Pareto-optimal design and, hence, the search space can be drastically reduced. Moreover, we make use of large database of DSE results that we have generated since we started working in this field to further accelerate the exploration process. For this, we use a technique based on perceptual hashing that allows our proposed explorer to recognize similar program structures in the new description to be explored and match them with structures in our database. This allows us to directly retrieve the pragma settings that lead to Pareto-optimal configurations. Experimental results show that the search space can be accelerated substantially while leading to finding most of the Pareto-optimal designs.",
        "link": "https://dl.acm.org/doi/10.1145/3495531",
        "category": "Databases"
    },
    {
        "title": "MMConv: An Environment for Multimodal Conversational Search across Multiple Domains",
        "authors": "['Lizi Liao', 'Le Hong Long', 'Zheng Zhang', 'Minlie Huang', 'Tat-Seng Chua']",
        "date": "July 2021",
        "source": "SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "abstract": "Although conversational search has become a hot topic in both dialogue research and IR community, the real breakthrough has been limited by the scale and quality of datasets available. To address this fundamental obstacle, we introduce the Multimodal Multi-domain Conversational dataset (MMConv), a fully annotated collection of human-to-human role-playing dialogues spanning over multiple domains and tasks. The contribution is two-fold. First, beyond the task-oriented multimodal dialogues among user and agent pairs, dialogues are fully annotated with dialogue belief states and dialogue acts. More importantly, we create a relatively comprehensive environment for conducting multimodal conversational search with real user settings, structured venue database, annotated image repository as well as crowd-sourced knowledge database. A detailed description of the data collection procedure along with a summary of data structure and analysis is provided. Second, a set of benchmark results for dialogue state tracking, conversational recommendation, response generation as well as a unified model for multiple tasks are reported. We adopt the state-of-the-art methods for these tasks respectively to demonstrate the usability of the data, discuss limitations of current methods and set baselines for future studies.",
        "link": "https://dl.acm.org/doi/10.1145/3404835.3462970",
        "category": "Databases"
    },
    {
        "title": "Unravelling Spatial Privacy Risks of Mobile Mixed Reality Data",
        "authors": "['Jaybie Agullo de Guzman', 'Aruna Seneviratne', 'Kanchana Thilakarathna']",
        "date": "None",
        "source": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "abstract": "Previously, 3D data---particularly, spatial data---have primarily been utilized in the field of geo-spatial analyses, or robot navigation (e.g. self-automated cars) as 3D representations of geographical or terrain data (usually extracted from lidar). Now, with the increasing user adoption of augmented, mixed, and virtual reality (AR/MR/VR; we collectively refer to as MR) technology on user mobile devices, spatial data has become more ubiquitous. However, this ubiquity also opens up a new threat vector for adversaries: aside from the traditional forms of mobile media such as images and video, spatial data poses additional and, potentially, latent risks to users of AR/MR/VR. Thus, in this work, we analyse MR spatial data using various spatial complexity metrics---including a cosine similarity-based, and a Euclidean distance-based metric---as heuristic or empirical measures that can signify the inference risk a captured space has. To demonstrate the risk, we utilise 3D shape recognition and classification algorithms for spatial inference attacks over various 3D spatial data captured using mobile MR platforms: i.e. Microsoft HoloLens, and Android with Google ARCore. Our experimental evaluation and investigation shows that the cosine similarity-based metric is a good spatial complexity measure of captured 3D spatial maps and can be utilised as an indicator of spatial inference risk.",
        "link": "https://dl.acm.org/doi/10.1145/3448103",
        "category": "Databases"
    },
    {
        "title": "One WITH RECURSIVE is Worth Many GOTOs",
        "authors": "['Denis Hirn', 'Torsten Grust']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "PL/SQL integrates an imperative statement-by-statement style of programming with the plan-based evaluation of SQL queries. The disparity of both leads to friction at runtime, slowing PL/SQL execution down significantly. This work describes a compiler from PL/SQL UDFs to plain SQL queries. Post-compilation, evaluation entirely happens on the SQL side of the fence. With the friction gone, we observe execution times to improve by about a factor of 2, even for complex UDFs. The compiler builds on techniques long established by the programming language community. In particular, it uses trampolined style to compile arbitrarily nested iterative control flow in PL/SQL into SQL's recursive common table expressions.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457272",
        "category": "Databases"
    },
    {
        "title": "Amoeba: aligning stream processing operators with externally-managed state",
        "authors": "['Antonis Papaioannou', 'Kostas Magoutis']",
        "date": "December 2021",
        "source": "UCC '21: Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing",
        "abstract": "Scalable stream processing systems (SPS) often require external storage systems for long-term storage of non-emphemeral state. Such state cannot be accommodated in the internal stores of SPSes that are mainly geared for fault tolerance of streaming jobs, lack externally visible APIs, and their state is disposed of at the end of such jobs. Recent research have pointed to scalable in-memory key-value stores (KVS) as an efficient solution to manage external state. While such data stores have been interconnected with scalable streaming systems, they are currently managed independently, missing opportunities for optimizations, such as exploiting locality between stream partitions and table shards, as well as coordinating elasticity actions. Both processing and data management systems are typically designed for scalability, however coordination between them poses a significant challenge. In this work we describe Amoeba, a system that dynamically adapts data-partitioning schemes and/or task or data placement across systems to eliminate unnecessary network communication across nodes. Our evaluation using state-of-the art systems, such as the Flink SPS and Redis KVS, demonstrated 2.6x performance improvement when aligning SPS tasks with KVS shards in AWS deployments of up to 64 nodes.",
        "link": "https://dl.acm.org/doi/10.1145/3468737.3494096",
        "category": "Databases"
    },
    {
        "title": "Highly-available and consistent group collaboration at the edge with colony",
        "authors": "['Ilyas Toumlilt', 'Pierre Sutra', 'Marc Shapiro']",
        "date": "December 2021",
        "source": "Middleware '21: Proceedings of the 22nd International Middleware Conference",
        "abstract": "Edge applications, such as gaming, cooperative engineering, or in-the-field information sharing, enjoy immediate response, autonomy and availability by distributing and replicating data at the edge. However, application developers and users demand the highest possible consistency guarantees, and specific support for group collaboration. To address this challenge, Colony guarantees Transactional Causal Plus Consistency (TCC+) globally, strengthened to Snapshot Isolation within edge groups. To help with scalability, fault tolerance and security, its logical communication topology is forest-like, with replicated roots in the core cloud, but with the flexibility to migrate a node or a group. Despite this hybrid approach, applications enjoy the same semantics everywhere in the topology. Our experiments show that local caching and peer groups improve throughput and response time significantly, performance is not affected in offline mode, and that migration is seamless.",
        "link": "https://dl.acm.org/doi/10.1145/3464298.3493405",
        "category": "Databases"
    },
    {
        "title": "Model Provenance Management in MLOps Pipeline",
        "authors": "['Songzhu Mei', 'Cong Liu', 'Qinglin Wang', 'Huayou Su']",
        "date": "January 2022",
        "source": "ICCDE '22: Proceedings of the 2022 8th International Conference on Computing and Data Engineering",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3512850.3512861",
        "category": "Databases"
    },
    {
        "title": "C3PO: Cloud-based Confidentiality-preserving Continuous Query Processing",
        "authors": "['Savvas Savvides', 'Seema Kumar', 'Julian James Stephen', 'Patrick Eugster']",
        "date": "None",
        "source": "ACM Transactions on Privacy and Security",
        "abstract": "With the advent of the Internet of things (IoT), billions of devices are expected to continuously collect and process sensitive data (e.g., location, personal health factors). Due to the limited computational capacity available on IoT devices, the current de facto model for building IoT applications is to send the gathered data to the cloud for computation. While building private cloud infrastructures for handling large amounts of data streams can be expensive, using low-cost public (untrusted) cloud infrastructures for processing continuous queries including sensitive data leads to strong concerns over data confidentiality.This article presents C3PO, a confidentiality-preserving, continuous query processing engine, that leverages the public cloud. The key idea is to intelligently utilize partially homomorphic and property-preserving encryption to perform as many computationally intensive operations as possible—without revealing plaintext—in the untrusted cloud. C3PO provides simple abstractions to the developer to hide the complexities of applying complex cryptographic primitives, reasoning about the performance of such primitives, deciding which computations can be executed in an untrusted tier, and optimizing cloud resource usage. An empirical evaluation with several benchmarks and case studies shows the feasibility of our approach. We consider different classes of IoT devices that differ in their computational and memory resources (from a Raspberry Pi 3 to a very small device with a Cortex-M3 microprocessor) and through the use of optimizations, we demonstrate the feasibility of using partially homomorphic and property-preserving encryption on IoT devices.",
        "link": "https://dl.acm.org/doi/10.1145/3472717",
        "category": "Databases"
    },
    {
        "title": "MxTasks: How to Make Efficient Synchronization and Prefetching Easy",
        "authors": "['Jan Mühlig', 'Jens Teubner']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "The hardware environment has changed rapidly in recent years: Many cores, multiple sockets, and large amounts of main memory have become a commodity. To benefit from these highly parallel systems, the software has to be adapted. Sophisticated latch-free data structures and algorithms are often meant to address the situation. But they are cumbersome to develop and may still not provide the desired scalability. As a remedy, we present MxTasking, a task-based framework that assists the design of latch-free and parallel data structures. MxTasking eases the information exchange between applications and the operating system, resulting in novel opportunities to manage resources in a truly hardware- and application-conscious way.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457268",
        "category": "Databases"
    },
    {
        "title": "The Cost of Stateless Network Functions in 5G",
        "authors": "['Umakant Kulkarni', 'Amit Sheoran', 'Sonia Fahmy']",
        "date": "December 2021",
        "source": "ANCS '21: Proceedings of the Symposium on Architectures for Networking and Communications Systems",
        "abstract": "The adoption of a cloud-native architecture in 5G networks has facilitated rapid deployment and update of cellular services. An important part of this architecture is the implementation of 5G network functions statelessly. However, statelessness and its associated serialization and de-serialization of data and database interaction significantly increase latency. In this work, we take the first steps towards quantifying the cost of statelessness in a cloud-native 5G system. We compare the cost of different state management paradigms, and propose a number of optimizations to reduce this cost. Our preliminary results indicate that sharing user state among 5G functions reduces the overall cost by on an average of 10% in experiments with 100 to 1000 simultaneous requests. Optimizations such as non-blocking calls and custom database APIs also reduce cost, albeit to a lower extent. We believe that the paradigms proposed in this paper can aid operators and software vendors as they design cloud-native 5G networks.",
        "link": "https://dl.acm.org/doi/10.1145/3493425.3502749",
        "category": "Databases"
    },
    {
        "title": "Dynamic and Scalable Enforcement of Access Control Policies for Big Data",
        "authors": "['Marco Anisetti', 'Claudio A. Ardagna', 'Chiara Braghin', 'Ernesto Damiani', 'Antongiacomo Polimeno', 'Alessandro Balestrucci']",
        "date": "November 2021",
        "source": "MEDES '21: Proceedings of the 13th International Conference on Management of Digital EcoSystems",
        "abstract": "The conflict between the need of protecting and sharing data is hampering the spread of big data applications. Security and privacy assurance is required to protect data owners, while data access and sharing are fundamental to implement smart big data solutions. In this context, access control systems can assume a central role in balancing data protection and data sharing. However, existing access control solutions are not general and scalable enough to address the software and technological complexity of big data ecosystems, being unable to support such a dynamic and collaborative environment. In this paper, we propose an access control system that enforces access to data in a distributed, multi-party big data environment. It is based on data annotations and secure data transformations performed at ingestion time. We show the feasibility of our approach in the smart city domain using an Apache-based big data engine.",
        "link": "https://dl.acm.org/doi/10.1145/3444757.3485107",
        "category": "Databases"
    },
    {
        "title": "Frequent Elements with Witnesses in Data Streams",
        "authors": "['Christian Konrad']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Detecting frequent elements is among the oldest and most-studied problems in the area of data streams. Given a stream of m data items in \\1, 2, \\dots, n\\, the objective is to output items that appear at least d times, for some threshold parameter d, and provably optimal algorithms are known today. However, in many applications, knowing only the frequent elements themselves is not enough: For example, an Internet router may not only need to know the most frequent destination IP addresses of forwarded packages, but also the timestamps of when these packages appeared or any other meta-data that \"arrived'' with the packages, e.g., their source IP addresses. In this paper, we introduce the witness version of the frequent elements problem: Given a desired approximation guarantee α \\ge 1$ and a desired frequency $d łe Δ$, where Δ is the frequency of the most frequent item, the objective is to report an item together with at least $d / α$ timestamps of when the item appeared in the stream (or any other meta-data that arrived with the items). We give provably optimal algorithms for both the insertion-only and insertion-deletion stream settings: In insertion-only streams, we show that space $\\tildeO (n + d \\cdot n^\\frac1 α )$ is necessary and sufficient for every integral $1 łe α łe łog n$. In insertion-deletion streams, we show that space $\\tildeO (\\fracn \\cdot d α^2 )$ is necessary and sufficient, for every α łe \\sqrtn $.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458330",
        "category": "Databases"
    },
    {
        "title": "Cross-Organizational Data Sharing Technology for Data Management Platform in the Manufacturing Industry",
        "authors": "['Hideya Yoshiuchi', 'Ikumi Inoue', 'Hiroki Miyamoto']",
        "date": "September 2021",
        "source": "CCIOT '21: Proceedings of the 2021 6th International Conference on Cloud Computing and Internet of Things",
        "abstract": "This paper proposes a cross-organizational data sharing technology for data management platform in the manufacturing industry in order to improve and optimize manufacturing operations. We examined a technology that enables requesting and acquiring data among organizations by modeling the manufacturing process and providing models to other organizations. By specifying the range of data that can be provided to the outside for the created data model at the manufacturing site, it enables sharing the necessary and sufficient data with other organization and to keep the data confidential. We evaluated the proposed technology from the viewpoint of storage capacity. The data sharing method has two management type, centralized management and distributed management. As a result of examining for the feasibility of the proposed distributed management technology, the required maximum storage capacity is 128.6 GB per each organization' data management system. Therefore, we got a certain outlook that distributed management technology is feasible.",
        "link": "https://dl.acm.org/doi/10.1145/3493287.3493292",
        "category": "Databases"
    },
    {
        "title": "Efficient 3D Spatial Queries for Complex Objects",
        "authors": "['Dejun Teng', 'Yanhui Liang', 'Hoang Vo', 'Jun Kong', 'Fusheng Wang']",
        "date": "None",
        "source": "ACM Transactions on Spatial Algorithms and Systems",
        "abstract": "3D spatial data has been generated at an extreme scale from many emerging applications, such as high definition maps for autonomous driving and 3D Human BioMolecular Atlas. In particular, 3D digital pathology provides a revolutionary approach to map human tissues in 3D, which is highly promising for advancing computer-aided diagnosis and understanding diseases through spatial queries and analysis. However, the exponential increase of data at 3D leads to significant I/O, communication, and computational challenges for 3D spatial queries. The complex structures of 3D objects such as bifurcated vessels make it difficult to effectively support 3D spatial queries with traditional methods. In this article, we present our work on building an efficient and scalable spatial query system, iSPEED, for large-scale 3D data with complex structures. iSPEED adopts effective progressive compression for each 3D object with successive levels of detail. Further, iSPEED exploits structural indexing for complex structured objects in distance-based queries. By querying with data represented in successive levels of details and structural indexes, iSPEED provides an option for users to balance between query efficiency and query accuracy. iSPEED builds in-memory indexes and decompresses data on-demand, which has a minimal memory footprint. iSPEED provides a 3D spatial query engine that can be invoked on-demand to run many instances in parallel implemented with, but not limited to, MapReduce. We evaluate iSPEED with three representative queries: 3D spatial joins, 3D nearest neighbor query, and 3D spatial proximity estimation. The extensive experiments demonstrate that iSPEED significantly improves the performance of existing spatial query systems.",
        "link": "https://dl.acm.org/doi/10.1145/3502221",
        "category": "Databases"
    },
    {
        "title": "There is no AI without data",
        "authors": "['Christoph Gröger']",
        "date": "November 2021",
        "source": "Communications of the ACM",
        "abstract": "Industry experiences on the data challenges of AI and the call for a data ecosystem for industrial enterprises.",
        "link": "https://dl.acm.org/doi/10.1145/3448247",
        "category": "Databases"
    },
    {
        "title": "EtherH: A Hybrid Index to Support Blockchain Data Query",
        "authors": "['Pengting Du', 'Yingjian Liu', 'Yue Li', 'Haoyu Yin', 'Limin Zhang']",
        "date": "July 2021",
        "source": "ACM TURC '21: Proceedings of the ACM Turing Award Celebration Conference - China",
        "abstract": "As the underlying technology of cryptocurrency, blockchain is characterized by decentralization and immutability. It is considered as a new generation of disruptive technology. However, the underlying storage of blockchain only provides limited support for data query. This limits the utility of blockchain technology. To improve the efficiency of access to blockchain data, we design EtherH based on Ethereum, the most representative open blockchain system. EtherH is a hybrid index that can provide Single-V query and Range query on Ethereum blockchain. To make the index build more specific, EtherH supports elastic data building. A portion of the blockchain data is selected for index building. We connect the client to the Rinkeby network to obtain block data and conduct extensive experiments. Experimental results prove that EtherH performs well in both data insertion and data query.",
        "link": "https://dl.acm.org/doi/10.1145/3472634.3472653",
        "category": "Databases"
    },
    {
        "title": "Efficient External Sorting for Memory-Constrained Embedded Devices with Flash Memory",
        "authors": "['Riley Jackson', 'Jonathan Gresl', 'Ramon Lawrence']",
        "date": "None",
        "source": "ACM Transactions on Embedded Computing Systems",
        "abstract": "Embedded devices are ubiquitous in areas of industrial and environmental monitoring, health and safety, and consumer appliances. A common use case is data collection, processing, and performing actions based on data analysis. Although many Internet of Things (IoT) applications use the embedded device simply for data collection, there are benefits to having more data processing done closer to data collection to reduce network transmissions and power usage and provide faster response. This work implements and evaluates algorithms for sorting data on embedded devices with specific focus on the smallest memory devices. In devices with less than 4 KB of available RAM, the standard external merge sort algorithm has limited application as it requires a minimum of three memory buffers and is not flash-aware. The contribution is a memory-optimized external sorting algorithm called no output buffer sort (NOBsort) that reduces the minimum memory required for sorting, has excellent performance for sorted or near-sorted data, and sorts on external memory such as SD cards or raw flash chips. When sorting large datasets, no output buffer sort reduces I/O and execution time by between 20% to 35% compared to standard external merge sort.",
        "link": "https://dl.acm.org/doi/10.1145/3446976",
        "category": "Databases"
    },
    {
        "title": "Blockchain-based Data Sharing System for Sensing-as-a-Service in Smart Cities",
        "authors": "['Chao Lin', 'Debiao He', 'Sherali Zeadally', 'Xinyi Huang', 'Zhe Liu']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "The sensing-as-a-service (SaaS) model has been explored to address the challenge of intractability of managing a large number of sensors faced by future smart cities. However, how to effectively share sensor data without compromising confidentiality, privacy protection, and fair trading without third parties is one of critical issues that must be solved in the SaaS in smart cities. While blockchain shows promise in solving these issues, the existing blockchain-based data sharing (BBDS) systems are difficult to apply to SaaS in smart cities because of many unresolved issues such as requiring a customized blockchain, huge storage, communication and computation costs, and dependence on a third party to achieve fair trading. We propose a BBDS system model with its security requirements before we present a concrete construction by combining \\(\\)-protocol, Paillier encryption scheme, and any secure symmetrical encryption and signature schemes. To demonstrate the utility of our proposed BBDS system, we present a security analysis and compare our system with other solutions. We implement the prototype in Remix to analyze the gas cost, and we conduct experiments to evaluate the communication and computation costs of the BBDS system using symmetric encryption (advanced encryption standard (AES)) and a signature scheme (elliptic curve digital signature algorithm (ECDSA)).",
        "link": "https://dl.acm.org/doi/10.1145/3397202",
        "category": "Databases"
    },
    {
        "title": "Complexity Analysis of Generalized and Fractional Hypertree Decompositions",
        "authors": "['Georg Gottlob', 'Matthias Lanzinger', 'Reinhard Pichler', 'Igor Razgon']",
        "date": "None",
        "source": "Journal of the ACM",
        "abstract": "Hypertree decompositions (HDs), as well as the more powerful generalized hypertree decompositions (GHDs), and the yet more general fractional hypertree decompositions (FHDs) are hypergraph decomposition methods successfully used for answering conjunctive queries and for solving constraint satisfaction problems. Every hypergraph H has a width relative to each of these methods: its hypertree width hw(H), its generalized hypertree width ghw(H), and its fractional hypertree width fhw(H), respectively. It is known that hw(H)≤ k can be checked in polynomial time for fixed k, while checking ghw(H)≤ k is NP-complete for k ≥ 3. The complexity of checking fhw(H)≤ k for a fixed k has been open for over a decade.We settle this open problem by showing that checking fhw(H)≤ k is NP-complete, even for k=2. The same construction allows us to prove also the NP-completeness of checking ghw(H)≤ k for k=2. After that, we identify meaningful restrictions that make checking for bounded ghw or fhw tractable or allow for an efficient approximation of the fhw.",
        "link": "https://dl.acm.org/doi/10.1145/3457374",
        "category": "Databases"
    },
    {
        "title": "Hybrid Evaluation for Distributed Iterative Matrix Computation",
        "authors": "['Zihao Chen', 'Chen Xu', 'Juan Soto', 'Volker Markl', 'Weining Qian', 'Aoying Zhou']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Distributed matrix computation is common in large-scale data processing and machine learning applications. Existing systems that support distributed matrix computation already explore incremental evaluation for iterative-convergent algorithms. However, they are oblivious to the fact that non-zero increments are scattered in different blocks in a distributed environment. Additionally, we observe that incremental evaluation does not always outperform full evaluation. To address these issues, we propose matrix reorganization to optimize the physical layout upon the state-of-art optimized partition schemes, and thereby accelerate the incremental evaluation. More importantly, we propose a hybrid evaluation to efficiently interleave full and incremental evaluation during the iterative process. In particular, it employs a cost model to compare the overhead costs of two types of evaluations and a selective comparison mechanism to reduce the overhead incurred by comparison itself. To demonstrate the efficiency of our techniques, we implement HyMAC, a hybrid matrix computation system based on SystemML. Our experiments show that HyMAC reduces execution time on large datasets by 23% on average in comparison to the state-of-art optimization technique and consequently outperforms SystemML, ScaLAPACK, and SciDB by an order of magnitude.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452843",
        "category": "Databases"
    },
    {
        "title": "Privacy-preserving Data Aggregation against Malicious Data Mining Attack for IoT-enabled Smart Grid",
        "authors": "['Jing Wang', 'Libing Wu', 'Sherali Zeadally', 'Muhammad Khurram Khan', 'Debiao He']",
        "date": "None",
        "source": "ACM Transactions on Sensor Networks",
        "abstract": "Internet of Things (IoT)-enabled smart grids can achieve more reliable and high-frequency data collection and transmission compared with existing grids. However, this frequent data processing may consume a lot of bandwidth, and even put the user’s privacy at risk. Although many privacy-preserving data aggregation schemes have been proposed to solve the problem, they still suffer from some security weaknesses or performance deficiency, such as lack of satisfactory data confidentiality and resistance to malicious data mining attack. To address these issues, we propose a novel privacy-preserving data aggregation scheme (called PDAM) for IoT-enabled smart grids, which can support efficient data source authentication and integrity checking, secure dynamic user join and exit. Unlike existing schemes, the PDAM is resilient to the malicious data mining attack launched by internal or external attackers and can achieve perfect data confidentiality against not only a malicious aggregator but also a curious control center for an authorized user. The detailed security and performance analysis show that our proposed PDAM can satisfy several well-known security properties and desirable efficiency for a smart grid system. Moreover, the comparative studies and experiments demonstrate that the PDAM is superior to other recently proposed works in terms of both security and performance.",
        "link": "https://dl.acm.org/doi/10.1145/3440249",
        "category": "Databases"
    },
    {
        "title": "On the Anonymization of Workflow Provenance without Compromising the Transparency of Lineage",
        "authors": "['Khalid Belhajjame']",
        "date": "None",
        "source": "Journal of Data and Information Quality",
        "abstract": "Workflows have been adopted in several scientific fields as a tool for the specification and execution of scientific experiments. In addition to automating the execution of experiments, workflow systems often include capabilities to record provenance information, which contains, among other things, data records used and generated by the workflow as a whole but also by its component modules. It is widely recognized that provenance information can be useful for the interpretation, verification, and re-use of workflow results, justifying its sharing and publication among scientists. However, workflow execution in some branches of science can manipulate sensitive datasets that contain information about individuals. To address this problem, we investigate, in this article, the problem of anonymizing the provenance of workflows. In doing so, we consider a popular class of workflows in which component modules use and generate collections of data records as a result of their invocation, as opposed to a single data record. The solution we propose offers guarantees of confidentiality without compromising lineage information, which provides transparency as to the relationships between the data records used and generated by the workflow modules. We provide algorithmic solutions that show how the provenance of a single module and an entire workflow can be anonymized and present the results of experiments that we conducted for their evaluation.",
        "link": "https://dl.acm.org/doi/10.1145/3460207",
        "category": "Databases"
    },
    {
        "title": "Snoopy: Surpassing the Scalability Bottleneck of Oblivious Storage",
        "authors": "['Emma Dauterman', 'Vivian Fang', 'Ioannis Demertzis', 'Natacha Crooks', 'Raluca Ada Popa']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Existing oblivious storage systems provide strong security by hiding access patterns, but do not scale to sustain high throughput as they rely on a central point of coordination. To overcome this scalability bottleneck, we present Snoopy, an object store that is both oblivious and scalable such that adding more machines increases system throughput. Snoopy contributes techniques tailored to the high-throughput regime to securely distribute and efficiently parallelize every system component without prohibitive coordination costs. These techniques enable Snoopy to scale similarly to a plaintext storage system. Snoopy achieves 13.7x higher throughput than Obladi, a state-of-the-art oblivious storage system. Specifically, Obladi reaches a throughput of 6.7K requests/s for two million 160-byte objects and cannot scale beyond a proxy and server machine. For the same data size, Snoopy uses 18 machines to scale to 92K requests/s with average latency under 500ms.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483562",
        "category": "Databases"
    },
    {
        "title": "Deciding Boundedness of Monadic Sirups",
        "authors": "['Stanislav Kikot', 'Agi Kurucz', 'Vladimir V. Podolskii', 'Michael Zakharyaschev']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "We show that deciding boundedness (aka FO-rewritability) of mon­adic single rule datalog programs (sirups) is 2\\Exp-hard, which matches the upper bound known since 1988 and finally settles a long-standing open problem. We obtain this result as a byproduct of an attempt to classify monadic 'disjunctive sirups'---Boolean conjunctive queries $\\q$ with unary and binary predicates mediated by a disjunctive rule $T(x) łor F(x) łeftarrow A(x)$---according to the data complexity of their evaluation. Apart from establishing that deciding FO-rewritability of disjunctive sirups with a dag-shaped $\\q$ is also 2\\Exp-hard, we make substantial progress towards obtaining a complete FO/Ł-hardness dichotomy of disjunctive sirups with ditree-shaped $\\q$.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458332",
        "category": "Databases"
    },
    {
        "title": "Subspace Exploration: Bounds on Projected Frequency Estimation",
        "authors": "['Graham Cormode', 'Charlie Dickens', 'David P. Woodruff']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "Given an $n \\times d$ dimensional dataset A, a projection query specifies a subset $C \\subseteq [d]$ of columns which yields a new $n \\times |C|$ array. We study the space complexity of computing data analysis functions over such subspaces, including heavy hitters and norms, when the subspaces are revealed only after observing the data. We show that this important class of problems is typically hard: for many problems, we show $2^Ømega(d) $ lower bounds. However, we present upper bounds which demonstrate space dependency better than $2^d$. That is, for $c,c' \\in (0,1)$ and a parameter $N=2^d$ an $N^c$-approximation can be obtained in space $\\min(N^c', n)$, showing that it is possible to improve on the naï ve approach of keeping information for all $2^d$ subsets of d columns. Our results are based on careful constructions of instances using coding theory and novel combinatorial reductions that exhibit such space-approximation tradeoffs.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458312",
        "category": "Databases"
    },
    {
        "title": "Academic Student Progression Predictive Analysis Application in Secondary Education Institution",
        "authors": "['Mideth Abisado', 'Peejay Vargas', 'Ma.Ian De Los Trinos']",
        "date": "August 2021",
        "source": "ICSET 2021: 2021 5th International Conference on E-Society, E-Education and E-Technology",
        "abstract": "The study developed an Academic Student Progression Predictive Analysis Application for Dr. Josefa Jara Martinez High School (DJJMHS) to provide an intranet platform for the centralized collection of data records and management of school forms. The system includes school details, school forms, real-time records of attendance, student academic progress, prediction, visualization, monitoring of employee performance, and report generation. It has enhanced security protection and can provide all users with a secured network connection. The system uses a graphical user interface to display all the functions to access all system features. The system was developed using standards and tools such as HTML5, CSS3, JavaScript, and Phyton-MySQL. It creates an interface of system transactions in utilizing technology issuance of School Forms such as SF1, SF2, SF4, SF5, SF6, SF9, and SF10. The information can be encoded once a database connection is established. It is ready to create records into the database tables using the execute method of the developed system. All required features signified functionality and reliability based on test cases performed in two cycles in a live environment. The system was evaluated “Very Good” by 30 respondents using the ISO 25010 Software Quality Matrix. This indicates that the system can be an accurate and reliable tool for accessing an end-user's data and other services.",
        "link": "https://dl.acm.org/doi/10.1145/3485768.3485787",
        "category": "Databases"
    },
    {
        "title": "The Security of Medical Data on Internet Based on Differential Privacy Technology",
        "authors": "['Zhihan Lv', 'Francesco Piccialli']",
        "date": "None",
        "source": "ACM Transactions on Internet Technology",
        "abstract": "The study aims at discussing the security of medical data in the Internet era. By using k-anonymity (K-A) and differential privacy (DP), an algorithm model combining K-A and DP was proposed, which was simulated through the experiments. In the Magic and EIA datasets, the algorithm constructed was compared with K-A and the L-diversity model to verify the performance of the model. The model constructed based on DP had the lowest privacy-leakage risks, which increased with the number of identifiers in the Magic and EIA datasets, and the information disclosure was the least. In addition, in its usability analysis, it was found that its value was the most obviously improved and its operation efficiency was the highest. The K-A-DP algorithm can effectively reduce the risk of privacy leakage and information loss, and has achieved excellent results. Despite the deficiencies in the process of the experiment, the study still provides a reference for solving the problem of medical data security.",
        "link": "https://dl.acm.org/doi/10.1145/3382769",
        "category": "Databases"
    },
    {
        "title": "BYANJON: A Ground Truth Preparation System for Online Handwritten Bangla Documents",
        "authors": "['Shibaprasad Sen', 'Ankan Bhattacharyya', 'Ram Sarkar', 'Kaushik Roy']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "The work reported in this article deals with the ground truth generation scheme for online handwritten Bangla documents at text-line, word, and stroke levels. The aim of the proposed scheme is twofold: firstly, to build a document level database so that future researchers can use the database to do research in this field. Secondly, the ground truth information will help other researchers to evaluate the performance of their algorithms developed for text-line extraction, word extraction, word segmentation, stroke recognition, and word recognition. The reported ground truth generation scheme starts with text-line extraction from the online handwritten Bangla documents, then words extraction from the text-lines, and finally segmentation of those words into basic strokes. After word segmentation, the basic strokes are assigned appropriate class labels by using modified distance-based feature extraction procedure and the MLP (Multi-layer Perceptron) classifier. The Unicode for the words are then generated from the sequence of stroke labels. XML files are used to store the stroke, word, and text-line levels ground truth information for the corresponding documents. The proposed system is semi-automatic and each step such as text-line extraction, word extraction, word segmentation, and stroke recognition has been implemented by using different algorithms. Thus, the proposed ground truth generation procedure minimizes huge manual intervention by reducing the number of mouse clicks required to extract text-lines, words from the document, and segment the words into basic strokes. The integrated stroke recognition module also helps to minimize the manual labor needed to assign appropriate stroke labels. The freely available and can be accessed at  https://byanjon.herokuapp.com/.",
        "link": "https://dl.acm.org/doi/10.1145/3464379",
        "category": "Databases"
    },
    {
        "title": "Achieving Transparency Report Privacy in Linear Time",
        "authors": "['Chien-Lun Chen', 'Leana Golubchik', 'Ranjan Pal']",
        "date": "None",
        "source": "Journal of Data and Information Quality",
        "abstract": "An accountable algorithmic transparency report (ATR) should ideally investigate (a) transparency of the underlying algorithm, and (b) fairness of the algorithmic decisions, and at the same time preserve data subjects’ privacy. However, a provably formal study of the impact to data subjects’ privacy caused by the utility of releasing an ATR (that investigates transparency and fairness), has yet to be addressed in the literature. The far-fetched benefit of such a study lies in the methodical characterization of privacy-utility trade-offs for release of ATRs in public, and their consequential application-specific impact on the dimensions of society, politics, and economics. In this paper, we first investigate and demonstrate potential privacy hazards brought on by the deployment of transparency and fairness measures in released ATRs. To preserve data subjects’ privacy, we then propose a linear-time optimal-privacy scheme, built upon standard linear fractional programming (LFP) theory, for announcing ATRs, subject to constraints controlling the tolerance of privacy perturbation on the utility of transparency schemes. Subsequently, we quantify the privacy-utility trade-offs induced by our scheme, and analyze the impact of privacy perturbation on fairness measures in ATRs. To the best of our knowledge, this is the first analytical work that simultaneously addresses trade-offs between the triad of privacy, utility, and fairness, applicable to algorithmic transparency reports.",
        "link": "https://dl.acm.org/doi/10.1145/3460001",
        "category": "Databases"
    },
    {
        "title": "Research of Identity Privacy-preserving Scheme Based on CP-ABE",
        "authors": "['Yang Yu']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "Data security and access control is one of the most challenging ongoing research work in cloud computing, because of users outsourcing their sensitive data to cloud providers. According to cloud computing privacy protection, an identity privacy-preserving scheme based on CP-ABE is proposed. It reduced the performance overhead, overcome untrustful three sides. Its frame is based on trusted third party. By analyzing its security, it indicates that the scheme can not only insure the access control of different users, but also can protect user's identity privacy.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501534",
        "category": "Databases"
    },
    {
        "title": "End-to-End Lip-Reading Without Large-Scale Data",
        "authors": "['Adriana Fernandez-Lopez', 'Federico M. Sukno']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Audio, Speech and Language Processing",
        "abstract": "The development of Automatic Lip-Reading (ALR) systems for continuous speech recognition has so far limited their applicability to English since this is the only language with large-scale datasets sufficient to train end-to-end ALR systems. In this work, we show that it is possible to train competitive end-to-end ALR systems in alternative languages with challenging small-scale data as long as the appropriate restrictions are made to the learning process of the visual front-end objective. To this end, we hypothesize that the visual front-end should be trained in a self-supervised setting, allowing it to target its own <italic>visual units</italic>. We specifically define <italic>visual units</italic> as a collection of visually similar images constrained by linguistics and provide an algorithmic implementation to automatically generate them. We show that <italic>visual units</italic> can be used to add an intermediate classification task between the visual and temporal modules that facilitates meaningful learning of visual features and, as a consequence, reduces the amount of data required to train an end-to-end ALR system. Additionally, we present a data augmentation strategy for enriching the temporal context. We synthesize realistic video sequences by appropriately combining characters-like sub-sequences from existing videos. We test the proposed ALR system on i) the VLRF dataset, a small-scale database that is one of the largest in Spanish, and achieve 44.77<inline-formula><tex-math notation=\"LaTeX\">$\\%$</tex-math></inline-formula> CER and 72.90&#x0025; WER, which are competitive with the state-of-the-art and significant for this volume of training material; ii) the TCD-TIMIT dataset, a comparable medium-scale database in English, where we achieve 36.58&#x0025; CER and 56.29&#x0025; WER, which are also state-of-the-art results on speaker-dependent experiments.",
        "link": "https://dl.acm.org/doi/10.1109/TASLP.2022.3182274",
        "category": "Databases"
    },
    {
        "title": "Adaptive Compression for Fast Scans on String Columns",
        "authors": "['Yannis Foufoulas', 'Lefteris Sidirourgos', 'Eleftherios Stamatogiannakis', 'Yannis Ioannidis']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "State-of-the-art OLAP systems tend to use columnar data representations, as these are both suitable for analytics and amenable to compression. Local dictionary value encoding has been shown to achieve high compression rates for string columns while still allowing fast filtered scans. In this paper, we argue that the effectiveness and efficiency of local dictionary compression is limited by data repetition across file blocks and by dictionary look-ups inside each block during filtered scan execution. To address this problem, we introduce an adaptive compression technique that is based on differential dictionaries and targets both storage efficiency and query performance. The proposed scheme reduces dramatically the need to store repeated values across different file blocks and significantly accelerates read operations by reducing the time needed for dictionary look-ups. A preliminary set of experiments has given very promising results, showing that, in many cases, the proposed new dictionary compression scheme is much more efficient than existing techniques, occasionally up to an order of magnitude.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452798",
        "category": "Databases"
    },
    {
        "title": "Sourav Bhowmick Speaks Out on Drawing Your Queries, Ethics, and Drawing for Ethics",
        "authors": "['Marianne Winslett', 'Vanessa Braganholo']",
        "date": "June 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "Welcome to ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today I have here with me Sourav Bhowmick, who is a professor at Nanyang Technological University in Singapore, better known as NTU. Sourav is an ACM Distinguished Member. He received the VLDB Service Award, and a Lecturer of the Year Award from NTU. Sourav is also a member of the Initiative on Diversity and Inclusion in Database Conference Venues. His PhD is from NTU. So, Sourav, welcome!",
        "link": "https://dl.acm.org/doi/10.1145/3484622.3484632",
        "category": "Databases"
    },
    {
        "title": "The Power of Nested Parallelism in Big Data Processing – Hitting Three Flies with One Slap –",
        "authors": "['Gábor E. Gévay', 'Jorge-Arnulfo Quiané-Ruiz', 'Volker Markl']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Many common data analysis tasks, such as performing hyperparameter optimization, processing a partitioned graph, and treating a matrix as a vector of vectors, offer natural opportunities for nested-parallel operations, i.e., launching parallel operations from inside other parallel operations. However, state-of-the-art dataflow engines, such as Spark and Flink, do not support nested parallelism. Users must implement workarounds, causing orders of magnitude slowdowns for their tasks, let alone the implementation effort. We present Matryoshka, a system that enables dataflow engines to support nested parallelism, even in the presence of control flow statements at inner nesting levels. Matryoshka achieves this via a novel two-phase flattening process, which translates nested-parallel programs to flat-parallel programs that can efficiently run on existing dataflow engines. The first phase introduces novel nesting primitives into the code, which allows for dynamic optimizations based on intermediate data characteristics in the second phase at runtime. We validate our system using several common data analysis tasks, such as PageRank and K-means.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457287",
        "category": "Databases"
    },
    {
        "title": "From data to physical artifact: challenges and opportunities in designing physical data artifacts for everyday life",
        "authors": "['Kim Sauvé', 'Steven Houben']",
        "date": "March - April 2022",
        "source": "Interactions",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3511670",
        "category": "Databases"
    },
    {
        "title": "Enhancing the SliceNBound Algorithm for the Closest-Pairs Query with Binary Space Partitioning",
        "authors": "['George Mavrommatis', 'Panagiotis Moutafis', 'Antonio Corral']",
        "date": "November 2021",
        "source": "PCI '21: Proceedings of the 25th Pan-Hellenic Conference on Informatics",
        "abstract": "Given two datasets P and Q, the (K) Closest-Pairs Query, KCPQ, finds the (K) pairs of objects between the datasets with the least distance. In a previous work, we presented SliceNBound, a fast distributed algorithm for the KCPQ on Apache Spark, consisting of four phases. The algorithm partitions the datasets in slices across an axis. Since it is well known that proper partitioning of the datasets directly affects the running time of every algorithm, in a subsequent work we tested and evaluated variations of the Binary Space Partitioning (BSP) for solving the KCPQ and found that this technique achieves better performance. In this paper we present an improvement of our distributed algorithm SliceNBound which consists in using a BSP scheme to create the partitions of data and reducing the samplings to one. The experiments show that this technique significantly reduces the total running time of the algorithm, thus improving an already fast and efficient algorithm.",
        "link": "https://dl.acm.org/doi/10.1145/3503823.3503844",
        "category": "Databases"
    },
    {
        "title": "Chucky: A Succinct Cuckoo Filter for LSM-Tree",
        "authors": "['Niv Dayan', 'Moshe Twitto']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Modern key-value stores typically rely on an LSM-tree in storage (SSD) to handle writes and Bloom filters in memory (DRAM) to optimize reads. With ongoing advances in SSD technology shrinking the performance gap between storage and memory devices, the Bloom filters are now emerging as a performance bottleneck. We propose Chucky, a new design that replaces the multiple Bloom filters by a single Cuckoo filter that maps each data entry to an auxiliary address of its location within the LSM-tree. We show that while such a design entails fewer memory accesses than with Bloom filters, its false positive rate off the bat is higher. The reason is that the auxiliary addresses occupy bits that would otherwise be used as parts of the Cuckoo filter's fingerprints. To address this, we harness techniques from information theory to succinctly encode the auxiliary addresses so that the fingerprints can stay large. As a result, Chucky achieves the best of both worlds: a modest access cost and a low false positive rate at the same time.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457273",
        "category": "Databases"
    },
    {
        "title": "Ushering a Trust-based Benefit Delivery Ecosystem in Rural India Powered by Blockchain",
        "authors": "['Ajay More', 'Abhishek Sah', 'Shilpa Singh']",
        "date": "October 2021",
        "source": "ICEGOV '21: Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance",
        "abstract": "In developing countries across the globe, the resources are scarce with governments, but the demand from citizens are enormous, resources are often so thinly spread by the government that they rarely manage to have the socio-economic impact they are intended to bring. To elucidate this point, we took the case of India, where multiple beneficiary-oriented programmes are implemented in rural areas of the country targeting about a billion people. Billions of dollars are spent by both Federal Governments and State Governments in undertaking similar exercises for outreach, identification, and enrolment of beneficiaries under various welfare programmes that provide cash transfers, subsidies and other social services to the disadvantaged youth, women and aged in society. Some countries have also undertaken household surveys to identify gaps in demand and the supply of government services. However, limited success has been achieved in adopting such static database as the single source of truth for the identification of deprived households across all government programmes, due to its inherent potential for creating errors of inclusion and exclusion. Another major deterrent in the adoption of such a singular identity associated with socio-economic attribute is the concentration of information and associated risks. Through this paper, an attempt is being made to bridge this ‘trust deficit’ in governance through the application of distributed ledger technology for social inclusion. We further discuss how government is better placed than the private sector to tap the full potential of Blockchain technology as several public organizations working in the same sector are often controlled by the same parent government department. A mandate from government can give the necessary thrust to reap the benefits of this ecosystem-centric technology. At the last, feasibility, approaches and challenges are analyzed. The results from the adoption of this approach to governance can transform the entire service delivery ecosystem.",
        "link": "https://dl.acm.org/doi/10.1145/3494193.3494200",
        "category": "Databases"
    },
    {
        "title": "Understanding and Improvement of Adversarial Training for Network Embedding from an Optimization Perspective",
        "authors": "['Lun Du', 'Xu Chen', 'Fei Gao', 'Qiang Fu', 'Kunqing Xie', 'Shi Han', 'Dongmei Zhang']",
        "date": "February 2022",
        "source": "WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
        "abstract": "Network Embedding aims to learn a function mapping the nodes to Euclidean space contribute to multiple learning analysis tasks on networks. However, both the noisy information behind the real-world networks and the overfitting problem negatively impact the quality of embedding vectors. To tackle these problems, researchers utilize Adversarial Perturbations on Parameters (APP) and achieve state-of-the-art performance. Unlike the mainstream methods introducing perturbations on the network structure or the data feature, Adversarial Training for Network Embedding (AdvTNE) adopts APP to directly perturb the model parameters, thus providing a new chance to understand the mechanism behind it. In this paper, we explain APP theoretically from an optimization perspective. Considering the Power-law property of networks and the optimization objective, we analyze the reason for its remarkable results on network embedding. Based on the above analysis and the Sigmoid saturation region problem, we propose a new Sine-base activation to enhance the performance of AdvTNE. We conduct extensive experiments on four real networks to validate the effectiveness of our method in node classification and link prediction. The results demonstrate that our method is competitive with state-of-the-art methods.",
        "link": "https://dl.acm.org/doi/10.1145/3488560.3498474",
        "category": "Databases"
    },
    {
        "title": "Semi-Supervised Ensemble Learning for Dealing with Inaccurate and Incomplete Supervision",
        "authors": "['Mona Nashaat', 'Aindrila Ghosh', 'James Miller', 'Shaikh Quader']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "In real-world tasks, obtaining a large set of noise-free data can be prohibitively expensive. Therefore, recent research tries to enable machine learning to work with weakly supervised datasets, such as inaccurate or incomplete data. However, the previous literature treats each type of weak supervision individually, although, in most cases, different types of weak supervision tend to occur simultaneously. Therefore, in this article, we present Smart MEnDR, a Classification Model that applies Ensemble Learning and Data-driven Rectification to deal with inaccurate and incomplete supervised datasets. The model first applies a preliminary phase of ensemble learning in which the noisy data points are detected while exploiting the unlabelled data. The phase employs a semi-supervised technique with maximum likelihood estimation to decide on the disagreement rate. Second, the proposed approach applies an iterative meta-learning step to tackle the problem of knowing which points should be made correct to improve the performance of the final classifier. To evaluate the proposed framework, we report the classification performance, noise detection, and the labelling accuracy of the proposed method against state-of-the-art techniques. The experimental results demonstrate the effectiveness of the proposed framework in detecting noise, providing correct labels, and attaining high classification performance.",
        "link": "https://dl.acm.org/doi/10.1145/3473910",
        "category": "Databases"
    },
    {
        "title": "RedCASTLE: practically applicable ks-anonymity for IoT streaming data at the edge in node-RED",
        "authors": "['Frank Pallas', 'Julian Legler', 'Niklas Amslgruber', 'Elias Grünewald']",
        "date": "December 2021",
        "source": "M4IoT '21: Proceedings of the 8th International Workshop on Middleware and Applications for the Internet of Things",
        "abstract": "In this paper, we present RedCASTLE, a practically applicable solution for Edge-based ks-anonymization of IoT streaming data in Node-RED. RedCASTLE builds upon a pre-existing, rudimentary implementation of the CASTLE algorithm and significantly extends it with functionalities indispensable for real-world IoT scenarios. In addition, RedCASTLE provides an abstraction layer for smoothly integrating ks -anonymization into Node-RED, a visually programmable middleware for streaming dataflows widely used in Edge-based IoT scenarios. Last but not least, RedCASTLE also provides further capabilities for basic information reduction that complement ks-anonymization in the privacy-friendly implementation of usecases involving IoT streaming data. A preliminary performance assessment finds that RedCASTLE comes with reasonable overheads and demonstrates its practical viability.",
        "link": "https://dl.acm.org/doi/10.1145/3493369.3493601",
        "category": "Databases"
    },
    {
        "title": "Understanding and Improvement of Adversarial Training for Network Embedding from an Optimization Perspective",
        "authors": "['Lun Du', 'Xu Chen', 'Fei Gao', 'Qiang Fu', 'Kunqing Xie', 'Shi Han', 'Dongmei Zhang']",
        "date": "February 2022",
        "source": "WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
        "abstract": "Network Embedding aims to learn a function mapping the nodes to Euclidean space contribute to multiple learning analysis tasks on networks. However, both the noisy information behind the real-world networks and the overfitting problem negatively impact the quality of embedding vectors. To tackle these problems, researchers utilize Adversarial Perturbations on Parameters (APP) and achieve state-of-the-art performance. Unlike the mainstream methods introducing perturbations on the network structure or the data feature, Adversarial Training for Network Embedding (AdvTNE) adopts APP to directly perturb the model parameters, thus providing a new chance to understand the mechanism behind it. In this paper, we explain APP theoretically from an optimization perspective. Considering the Power-law property of networks and the optimization objective, we analyze the reason for its remarkable results on network embedding. Based on the above analysis and the Sigmoid saturation region problem, we propose a new Sine-base activation to enhance the performance of AdvTNE. We conduct extensive experiments on four real networks to validate the effectiveness of our method in node classification and link prediction. The results demonstrate that our method is competitive with state-of-the-art methods.",
        "link": "https://dl.acm.org/doi/10.1145/3488560.3498474",
        "category": "Databases"
    },
    {
        "title": "Semi-Supervised Ensemble Learning for Dealing with Inaccurate and Incomplete Supervision",
        "authors": "['Mona Nashaat', 'Aindrila Ghosh', 'James Miller', 'Shaikh Quader']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "In real-world tasks, obtaining a large set of noise-free data can be prohibitively expensive. Therefore, recent research tries to enable machine learning to work with weakly supervised datasets, such as inaccurate or incomplete data. However, the previous literature treats each type of weak supervision individually, although, in most cases, different types of weak supervision tend to occur simultaneously. Therefore, in this article, we present Smart MEnDR, a Classification Model that applies Ensemble Learning and Data-driven Rectification to deal with inaccurate and incomplete supervised datasets. The model first applies a preliminary phase of ensemble learning in which the noisy data points are detected while exploiting the unlabelled data. The phase employs a semi-supervised technique with maximum likelihood estimation to decide on the disagreement rate. Second, the proposed approach applies an iterative meta-learning step to tackle the problem of knowing which points should be made correct to improve the performance of the final classifier. To evaluate the proposed framework, we report the classification performance, noise detection, and the labelling accuracy of the proposed method against state-of-the-art techniques. The experimental results demonstrate the effectiveness of the proposed framework in detecting noise, providing correct labels, and attaining high classification performance.",
        "link": "https://dl.acm.org/doi/10.1145/3473910",
        "category": "Databases"
    },
    {
        "title": "Small Selectivities Matter: Lifting the Burden of Empty Samples",
        "authors": "['Axel Hertzschuch', 'Guido Moerkotte', 'Wolfgang Lehner', 'Norman May', 'Florian Wolf', 'Lars Fricke']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Every year more and more advanced approaches to cardinality estimation are published, using learned models or other data and workload specific synopses. In contrast, the majority of commercial in-memory systems still relies on sampling. It is arguably the most general and easiest estimator to implement. While most methods do not seem to improve much over sampling-based estimators in the presence of non-selective queries, sampling struggles with highly selective queries due to limitations of the sample size. Especially in situations where no sample tuple qualifies, optimizers fall back to basic heuristics that ignore attribute correlations and lead to large estimation errors. In this work, we present a novel approach, dealing with these 0-Tuple Situations. It is ready to use in any DBMS capable of sampling, showing a negligible impact on optimization time. Our experiments on real world and synthetic data sets demonstrate up to two orders of magnitude reduced estimation errors. Enumerating single filter predicates according to our estimates reveals 1.3 to 1.8 times faster query responses for complex filters.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452805",
        "category": "Databases"
    },
    {
        "title": "From data to physical artifact: challenges and opportunities in designing physical data artifacts for everyday life",
        "authors": "['Kim Sauvé', 'Steven Houben']",
        "date": "March - April 2022",
        "source": "Interactions",
        "abstract": "None",
        "link": "https://dl.acm.org/doi/10.1145/3511670",
        "category": "Databases"
    },
    {
        "title": "Relational e-matching",
        "authors": "['Yihong Zhang', 'Yisu Remy Wang', 'Max Willsey', 'Zachary Tatlock']",
        "date": "January 2022",
        "source": "Proceedings of the ACM on Programming Languages",
        "abstract": "We present a new approach to e-matching based on relational join; in particular, we apply recent database query execution techniques to guarantee worst-case optimal run time. Compared to the conventional backtracking approach that always searches the e-graph \"top down\", our new relational e-matching approach can better exploit pattern structure by searching the e-graph according to an optimized query plan. We also establish the first data complexity result for e-matching, bounding run time as a function of the e-graph size and output size. We prototyped and evaluated our technique in the state-of-the-art egg e-graph framework. Compared to a conventional baseline, relational e-matching is simpler to implement and orders of magnitude faster in practice.",
        "link": "https://dl.acm.org/doi/10.1145/3498696",
        "category": "Databases"
    },
    {
        "title": "Private Hierarchical Clustering in Federated Networks",
        "authors": "['Aashish Kolluri', 'Teodora Baluta', 'Prateek Saxena']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Analyzing structural properties of social networks, such as identifying their clusters or finding their central nodes, has many applications. However, these applications are not supported by federated social networks that allow users to store their social contacts locally on their end devices. In the federated regime, users want access to personalized services while also keeping their social contacts private. In this paper, we take a step towards enabling analytics on federated networks with differential privacy guarantees about protecting the user's social contacts. Specifically, we present the first work to compute hierarchical cluster trees using local differential privacy. Our algorithms for computing them are novel and come with theoretical bounds on the quality of the trees learned. Empirically, our differentially private algorithms learn trees that are of comparable quality (with at most about 10% utility loss) to the trees obtained from the non-private algorithms, while having reasonable privacy (0.5 łeq ε łeq 2). Private hierarchical cluster trees enable new application setups where a service provider can query the community structure around a target user without having their social contacts. We show the utility of such queries by redesigning two state-of-the-art social recommendation algorithms for the federated social network setup. Our recommendation algorithms significantly outperform the baselines that do not use social contacts.",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484822",
        "category": "Databases"
    },
    {
        "title": "Towards crowd-aware indoor path planning",
        "authors": "['Tiantian Liu', 'Huan Li', 'Hua Lu', 'Muhammad Aamir Cheema', 'Lidan Shou']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Indoor venues accommodate many people who collectively form crowds. Such crowds in turn influence people's routing choices, e.g., people may prefer to avoid crowded rooms when walking from A to B. This paper studies two types of crowd-aware indoor path planning queries. The Indoor Crowd-Aware Fastest Path Query (FPQ) finds a path with the shortest travel time in the presence of crowds, whereas the Indoor Least Crowded Path Query (LCPQ) finds a path encountering the least objects en route. To process the queries, we design a unified framework with three major components. First, an indoor crowd model organizes indoor topology and captures object flows between rooms. Second, a time-evolving population estimator derives room populations for a future timestamp to support crowd-aware routing cost computations in query processing. Third, two exact and two approximate query processing algorithms process each type of query. All algorithms are based on graph traversal over the indoor crowd model and use the same search framework with different strategies of updating the populations during the search process. All proposals are evaluated experimentally on synthetic and real data. The experimental results demonstrate the efficiency and scalability of our framework and query processing algorithms.",
        "link": "https://dl.acm.org/doi/10.14778/3457390.3457401",
        "category": "Databases"
    },
    {
        "title": "One Size Doesn't Fit All: Diversifying Data Science Course Projects by Student Background and Interests",
        "authors": "['Wensheng Wu']",
        "date": "June 2021",
        "source": "ITiCSE '21: Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1",
        "abstract": "A key challenge to a data science program is how to tailor its curriculum to accommodate the needs and interests of students from diverse academic backgrounds. To address this challenge, we propose Proj4X, a general project model that gives students the flexibility in choosing subject areas for the projects based on their background and interests, while requiring every project to address the key knowledge and skills covered in the course. Although project-based learning has been well researched, only a few studies have addressed the challenges in managing student-initiated projects. We have applied Proj4X to a graduate database course in our data science program. We used student peer evaluation to measure the performance of projects objectively and study the effect of group size on the performance of projects and the correlation between specific aspects of a project and its overall rating. Student feedback shows that Proj4X can greatly foster the collaboration among students of diverse backgrounds and inspire them to work on a wide range of interesting topics: from satellite tracker, hospital search during pandemic, to wildlife diversity monitoring.",
        "link": "https://dl.acm.org/doi/10.1145/3430665.3456375",
        "category": "Databases"
    },
    {
        "title": "On-Shelf Utility Mining of Sequence Data",
        "authors": "['Chunkai Zhang', 'Zilin Du', 'Yuting Yang', 'Wensheng Gan', 'Philip S. Yu']",
        "date": "None",
        "source": "ACM Transactions on Knowledge Discovery from Data",
        "abstract": "Utility mining has emerged as an important and interesting topic owing to its wide application and considerable popularity. However, conventional utility mining methods have a bias toward items that have longer on-shelf time as they have a greater chance to generate a high utility. To eliminate the bias, the problem of on-shelf utility mining (OSUM) is introduced. In this article, we focus on the task of OSUM of sequence data, where the sequential database is divided into several partitions according to time periods and items are associated with utilities and several on-shelf time periods. To address the problem, we propose two methods, OSUM of sequence data (OSUMS) and OSUMS+, to extract on-shelf high-utility sequential patterns. For further efficiency, we also design several strategies to reduce the search space and avoid redundant calculation with two upper bounds time prefix extension utility (TPEU) and time reduced sequence utility (TRSU). In addition, two novel data structures are developed for facilitating the calculation of upper bounds and utilities. Substantial experimental results on certain real and synthetic datasets show that the two methods outperform the state-of-the-art algorithm. In conclusion, OSUMS may consume a large amount of memory and is unsuitable for cases with limited memory, while OSUMS+ has wider real-life applications owing to its high efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3457570",
        "category": "Databases"
    },
    {
        "title": "Copy-and-patch compilation: a fast compilation algorithm for high-level languages and bytecode",
        "authors": "['Haoran Xu', 'Fredrik Kjolstad']",
        "date": "October 2021",
        "source": "Proceedings of the ACM on Programming Languages",
        "abstract": "Fast compilation is important when compilation occurs at runtime, such as query compilers in modern database systems and WebAssembly virtual machines in modern browsers. We present copy-and-patch, an extremely fast compilation technique that also produces good quality code. It is capable of lowering both high-level languages and low-level bytecode programs to binary code, by stitching together code from a large library of binary implementation variants. We call these binary implementations stencils because they have holes where missing values must be inserted during code generation. We show how to construct a stencil library and describe the copy-and-patch algorithm that generates optimized binary code.  We demonstrate two use cases of copy-and-patch: a compiler for a high-level C-like language intended for metaprogramming and a compiler for WebAssembly. Our high-level language compiler has negligible compilation cost: it produces code from an AST in less time than it takes to construct the AST. We have implemented an SQL database query compiler on top of this metaprogramming system and show that on TPC-H database benchmarks, copy-and-patch generates code two orders of magnitude faster than LLVM -O0 and three orders of magnitude faster than higher optimization levels. The generated code runs an order of magnitude faster than interpretation and 14% faster than LLVM -O0. Our WebAssembly compiler generates code 4.9X-6.5X faster than Liftoff, the WebAssembly baseline compiler in Google Chrome. The generated code also outperforms Liftoff's by 39%-63% on the Coremark and PolyBenchC WebAssembly benchmarks.",
        "link": "https://dl.acm.org/doi/10.1145/3485513",
        "category": "Databases"
    },
    {
        "title": "Improved Differentially Private Euclidean Distance Approximation",
        "authors": "['Nina Mesing Stausholm']",
        "date": "June 2021",
        "source": "PODS'21: Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
        "abstract": "This work shows how to privately and more accurately estimate Euclidean distance between pairs of vectors. Input vectors x and y are mapped to differentially private sketches $x'$ and $y'$, from which one can estimate the distance between x and y. Our estimator relies on the Sparser Johnson-Lindenstrauss constructions by Kane & Nelson (Journal of the ACM 2014), which for any 0<α,β<1/2 have optimal output dimension k=Θ(α^-2 łog(1/β)) and sparsity s=O(α^-1 łog(1/β)). We combine the constructions of Kane & Nelson with either the Laplace or the Gaussian mechanism from the differential privacy literature, depending on the privacy parameters $\\varepsilon$ and δ. We also suggest a differentially private version of Fast Johnson-Lindenstrauss Transform (FJLT) by Ailon & Chazelle (SIAM Journal of Computing 2009) which offers a tradeoff in speed for variance for certain parameters. We answer an open question by Kenthapadi et al. (Journal of Privacy and Confidentiality 2013) by analyzing the privacy and utility guarantees of an estimator for Euclidean distance, relying on Laplacian rather than Gaussian noise. We prove that the Laplace mechanism yields lower variance than the Gaussian mechanism whenever δ<β^O(1/α). Thus, our work poses an improvement over the work of Kenthapadi et al. by giving a more efficient estimator with lower variance for sufficiently small δ. Our sketch also achieves pure differential privacy as a neat side-effect of the Laplace mechanism rather than the approximate differential privacy guarantee of the Gaussian mechanism, which may not be sufficiently strong for some settings. Our main result is a special case of more general, technical results proving that one can generally construct unbiased estimators for Euclidean distance with a high level of utility even under the constraint of differential privacy. The bulk of our analysis is proving that the variance of the estimator does not suffer too much in the presence of differential privacy.",
        "link": "https://dl.acm.org/doi/10.1145/3452021.3458328",
        "category": "Databases"
    },
    {
        "title": "LIMA: Fine-grained Lineage Tracing and Reuse in Machine Learning Systems",
        "authors": "['Arnab Phani', 'Benjamin Rath', 'Matthias Boehm']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Machine learning (ML) and data science workflows are inherently exploratory. Data scientists pose hypotheses, integrate the necessary data, and run ML pipelines of data cleaning, feature engineering, model selection and hyper-parameter tuning. The repetitive nature of these workflows, and their hierarchical composition from building blocks exhibits high computational redundancy. Existing work addresses this redundancy with coarse-grained lineage tracing and reuse for ML pipelines. This approach allows using existing ML systems, but views entire algorithms as black boxes, and thus, fails to eliminate fine-grained redundancy and to handle internal non-determinism. In this paper, we introduce LIMA, a practical framework for efficient, fine-grained lineage tracing and reuse inside ML systems. Lineage tracing of individual operations creates new challenges and opportunities. We address the large size of lineage traces with multi-level lineage tracing and reuse, as well as lineage deduplication for loops and functions; exploit full and partial reuse opportunities across the program hierarchy; and integrate this framework with task parallelism and operator fusion. The resulting framework performs fine-grained lineage tracing with low overhead, provides versioning and reproducibility, and is able to eliminate fine-grained redundancy. Our experiments on a variety of ML pipelines show performance improvements up to 12.4x.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452788",
        "category": "Databases"
    },
    {
        "title": "KVCG: a heterogeneous key-value store for skewed workloads",
        "authors": "['dePaul Miller', 'Jacob Nelson', 'Ahmed Hassan', 'Roberto Palmieri']",
        "date": "June 2021",
        "source": "SYSTOR '21: Proceedings of the 14th ACM International Conference on Systems and Storage",
        "abstract": "We present KVCG, a novel heterogeneous key-value store whose primary objective is to serve client requests targeting frequently accessed (hot) keys at sub-millisecond latency and requests targeting less frequently accessed (cold) keys with high throughput. To accomplish this goal, KVCG deploys an architecture where requests on hot keys are routed to a software cache operated by CPU threads, while the remainder are offloaded to a data repository optimized for execution on modern GPU devices. Cold/hot partitioning is done at runtime through a model trained with the incoming workload. Against a state-of-the-art competitor, we obtain up to 34x improvement in latency.",
        "link": "https://dl.acm.org/doi/10.1145/3456727.3463779",
        "category": "Databases"
    },
    {
        "title": "Building Arabic Paraphrasing Benchmark based on Transformation Rules",
        "authors": "['Marwah Alian', 'Arafat Awajan', 'Ahmad Al-Hasan', 'Raeda Akuzhia']",
        "date": "None",
        "source": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "abstract": "Measuring semantic similarity between short texts is an important task in many applications of natural language processing, such as paraphrasing identification. This process requires a benchmark of sentence pairs that are labeled by Arab linguists and considered a standard that can be used by researchers when evaluating their results. This research describes an Arabic paraphrasing benchmark to be a good standard for evaluation algorithms that are developed to measure semantic similarity for Arabic sentences to detect paraphrasing in the same language. The transformed sentences are in accordance with a set of rules for Arabic paraphrasing. These sentences are constructed from the words in the Arabic word semantic similarity dataset and from different Arabic books, educational texts, and lexicons. The proposed benchmark consists of 1,010 sentence pairs wherein each pair is tagged with scores determining semantic similarity and paraphrasing. The quality of the data is assessed using statistical analysis for the distribution of the sentences over the Arabic transformation rules and exploration through hierarchical clustering (HCL). Our exploration using HCL shows that the sentences in the proposed benchmark are grouped into 27 clusters representing different subjects. The inter-annotator agreement measures show a moderate agreement for the annotations of the graduate students and a poor reliability for the annotations of the undergraduate students.",
        "link": "https://dl.acm.org/doi/10.1145/3446770",
        "category": "Databases"
    },
    {
        "title": "AMEBA: An Adaptive Approach to the Black-Box Evasion of Machine Learning Models",
        "authors": "['Stefano Calzavara', 'Lorenzo Cazzaro', 'Claudio Lucchese']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Machine learning models are vulnerable to evasion attacks, where the attacker starts from a correctly classified instance and perturbs it so as to induce a misclassification. In the black-box setting where the attacker only has query access to the target model, traditional attack strategies exploit a property known as transferability, i.e., the empirical observation that evasion attacks often generalize across different models. The attacker can thus rely on the following two-step attack strategy: (i) query the target model to learn how to train a surrogate model approximating it; and (ii) craft evasion attacks against the surrogate model, hoping that they \"transfer\" to the target model. This attack strategy is sub-optimal, because it assumes a strict separation of the two steps and under-approximates the possible actions that a real attacker might take. In this work we propose AMEBA, the first adaptive approach to the black-box evasion of machine learning models. AMEBA builds on a well-known optimization problem, known as Multi-Armed Bandit, to infer the best alternation of actions spent for surrogate model training and evasion attack crafting. We experimentally show on public datasets that AMEBA outperforms traditional two-step attack strategies.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453114",
        "category": "Databases"
    },
    {
        "title": "Trustworthy Data Analysis and Sensor Data Protection in Cyber-Physical Systems",
        "authors": "['Denis Ulybyshev', 'Ibrahim Yilmaz', 'Bradley Northern', 'Vadim Kholodilo', 'Michael Rogers']",
        "date": "April 2021",
        "source": "SAT-CPS '21: Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems",
        "abstract": "Cyber-Physical Systems are widely used in critical infrastructures such as the power grids, water purification systems, nuclear plants, oil refinery and compressor plants, food manufacturing, etc. Anomalies in these systems can be a result of cybersecurity attacks, failed sensors or communication channels. Undetected anomalies may lead to process failure, cause financial damage and have significant impact on human lives. Thus, it is important to detect anomalies at early stages and to protect data in Cyber-Physical Systems. In this paper, we propose the novel on-the-fly NIST-compliant key generation scheme for a secure data container used to transfer and store sensor data. The data container delivers data from the low-level field sensors to high-level data analysis servers in a protected form. It provides data confidentiality and integrity, as well as data origin integrity, a fine-grained role-based and attribute-based access control. As a result, the anomaly detector runs on trustworthy data sets, protected from unauthorized adversarial modifications. Our solution can be easily integrated with many existing Cyber-Physical Systems and IT infrastructures since our secure data container supports RESTful API and is implemented in two modifications: (1) signed, watermarked and encrypted spreadsheet file; (2) signed and encrypted JSON file. In addition, we implemented several machine learning models based on a Random Forest, a k-Nearest Neighbors, a Support Vector Machine and a Neural Network algorithms for the detection of various anomalies and attacks in a gas pipeline system. We will demonstrate that our anomaly detection models achieve high detection rate with an average accuracy of 97.7% for two industrial data sets collected by the Mississippi State University's Critical Infrastructure Protection Center and Oak Ridge National Laboratories (ORNL)",
        "link": "https://dl.acm.org/doi/10.1145/3445969.3450432",
        "category": "Databases"
    },
    {
        "title": "JSONSki: streaming semi-structured data with bit-parallel fast-forwarding",
        "authors": "['Lin Jiang', 'Zhijia Zhao']",
        "date": "February 2022",
        "source": "ASPLOS '22: Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
        "abstract": "Semi-structured data, such as JSON, are fundamental to the Web and document data stores. Streaming analytics on semi-structured data combines parsing and query evaluation into one pass to avoid generating parse trees. Though promising, its conventional design requires to parse the data stream in detail character by character, which limits the efficiency of streaming analytics.  This work reveals a wide range of opportunities to fast-forward the streaming over certain data substructures irrelevant to the query evaluation. However, identifying these substructures itself may need detailed parsing. To resolve this dilemma, this work designs a highly bit-parallel solution that intensively utilizes bitwise and SIMD operations to identify the irrelevant substructures during the streaming. It includes a new streaming model—recursive-descent streaming, for an easy adoption of fast-forward optimizations, a concept—structural intervals, for partitioning the data stream, and a group of bit-parallel algorithms implementing various fast-forward cases. The solution is implemented as a JSON streaming framework, called JSONSki. It offers a set of APIs that can be invoked during the streaming to dynamically fast-forward over different cases of irrelevant substructures. Evaluation using real-world datasets and standard path queries shows that JSONSki can achieve significant speedups over the state-of-the-art JSON processing tools while taking a minimum memory footprint.",
        "link": "https://dl.acm.org/doi/10.1145/3503222.3507719",
        "category": "Databases"
    },
    {
        "title": "A JPEG compression-resistant data watermark embedding and detection algorithm",
        "authors": "['Pengfei Yu', 'Xiuli Huang', 'Bo Hu', 'Xiaoming Zhou']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "When the image is processed by JPEG compression, the data watermark originally hidden in the image will not be extracted effectively according to resampling and noise addition. This would greatly limit the application of image data watermark in real scenes. In this paper, a novel JPEG compression-resistant data watermark embedding and detection algorithm is proposed for the robustness of data watermark embedding and extraction under lossy channels. The algorithm uses the compression invariance of DCT coefficient symbols to select candidate coefficients in the carrier image that can resist JPEG compression, followed by assigning distortion cost to each candidate coefficient, and then embedding the watermark information into the carrier image using error correction codes and Syndrome-Trellis Code encoding. The tests show that the proposed watermark embedding method improves the detection resistance performance by more than 7.7% when the relative embedding rate is 0.1, and the correct extraction rate of data watermark can reach more than 99% under JPEG compression with different quality factors.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501624",
        "category": "Databases"
    },
    {
        "title": "The Sensitive Data Feature Extraction Based on Low-Rank Multimodal Fusion",
        "authors": "['Fei Liang', 'Pengfei Yu']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "With the increasing level of information technology in society, personal sensitive data often appears in a variety of modal forms such as text, audio and video. In order to effectively identify and analyse personal sensitive data, feature extraction is required. However, the features of different modalities of personal sensitive data are often heterogeneous, with a high level of noise and redundant information. In order to efficiently and collaboratively characterise personal sensitive data, eliminate redundant and contradictory information, and effectively fuse multimodal features, this paper proposes a novel feature extraction method based on low-rank multimodal fusion for sensitive data, which addresses the problem of difficult extraction of multimodal interaction information. The method uses self-attentive modules, tensor outer product and low-rank decomposition to build a fusion algorithm that can effectively fuse text, audio and video modal information of sensitive data. Tests on MOSI and IEMOCAP datasets show that the proposed algorithm has good feature fusion capability.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501501",
        "category": "Databases"
    },
    {
        "title": "DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python",
        "authors": "['Jinglin Peng', 'Weiyuan Wu', 'Brandon Lockhart', 'Song Bian', 'Jing Nathan Yan', 'Linghao Xu', 'Zhixuan Chi', 'Jeffrey M. Rzeszotarski', 'Jiannan Wang']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3457330",
        "category": "Databases"
    },
    {
        "title": "On Generalizing Static Node Embedding to Dynamic Settings",
        "authors": "['Di Jin', 'Sungchul Kim', 'Ryan A. Rossi', 'Danai Koutra']",
        "date": "February 2022",
        "source": "WSDM '22: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
        "abstract": "Temporal graph embedding has been widely studied thanks to its superiority in tasks such as prediction and recommendation. Despite the advances in algorithms and novel frameworks such as deep learning, there has been relatively little work on systematically studying the properties of temporal network models and their cornerstones, the graph time-series representations that are used in these approaches. This paper aims to fill this gap by introducing a general framework that extends an arbitrary existing static embedding approach to handle dynamic tasks, and conducting a systematic study of seven base static embedding methods and six temporal network models. Our framework generalizes static node embeddings derived from the time-series representation of stream data to the dynamic setting by modeling the temporal dependencies with classic models such as the reachability graph. While previous works on dynamic modeling and embedding have focused on representing a stream of timestamped edges using a time-series of graphs based on a specific time-scale (\\eg, 1 month), we introduce the notion of an ε-graph time-series that uses a fixed number of edges for each graph, and show its superiority in practical settings over the standard solution. From the 42 methods that our framework subsumes, we find that leveraging the new ε-graph time-series representation and capturing temporal dependencies with the proposed reachability or summary graph tend to perform well. Furthermore, the new dynamic embedding methods based on our framework perform comparably and on average better than the state-of-the-art embedding methods designed specifically for temporal graphs in link prediction tasks.",
        "link": "https://dl.acm.org/doi/10.1145/3488560.3498428",
        "category": "Databases"
    },
    {
        "title": "A Novel GAN based User Desensitization Data Generation Algorithm",
        "authors": "['Congcong Shi', 'Pengfei Yu', 'Xiuli Huang']",
        "date": "October 2021",
        "source": "EITCE '21: Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
        "abstract": "In recent years, how to use user data rationally has become a hot topic of discussion. In order to solve the problem that existing user desensitised data generation methods lead to the unavailability of the user profile analysis function of applications, this paper proposes a generative adversarial network-based algorithm for generating user privacy desensitised data, which improves the usability of the data while ensuring data privacy. The algorithm is based on generative adversarial networks, using a differential autoencoder to extract user data features and migrate the features to the desensitised data; it also uses a multi-headed attention mechanism to intensively extract the main features of the user data; in order to ensure that the desensitised data can be used for the Android app's own user portrait analysis, its discriminator uses the app's own neural network learning model to adjust the output of the generator. Due to the stochastic nature of the differential auto-encoder learning process and the approximation process, the desensitised data are independent of the user's original data, thus protecting the user's private data. The desensitised data are guided by the discriminator with the characteristics of the original data, and the results of user profiling based on this desensitised data are approximately the same as the results of the original data analysis, which protects user privacy without compromising profiling.",
        "link": "https://dl.acm.org/doi/10.1145/3501409.3501599",
        "category": "Databases"
    },
    {
        "title": "Secure Multi-party Computation of Differentially Private Heavy Hitters",
        "authors": "['Jonas Böhler', 'Florian Kerschbaum']",
        "date": "November 2021",
        "source": "CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
        "abstract": "Private learning of top-k, i.e., the k most frequent values also called heavy hitters, is a common industry scenario: Companies want to privately learn, e.g., frequently typed new words to improve suggestions on mobile devices, often used browser settings, telemetry data of frequent crashes, heavily shared articles, etc. Real-world deployments often use local differential privacy, where distributed users share locally randomized data with an untrusted server. Central differential privacy, on the other hand, assumes access to the raw data and applies the randomization only once, on the aggregated result. These solutions either require large amounts of users for high accuracy (local model) or a trusted third party (central model).We present multi-party computation protocols HH and PEM of sketches (succinct data structures) to efficiently compute differentially private top-k: HH has running time linear in the size of the data and is applicable for very small data sets (hundreds of values), and PEM is sublinear in the data domain and provides better accuracy than HH for large data sizes. Our approaches are efficient (practical running time, requiring no output reconstruction as other sketches) and more accurate than local differential privacy even for a small number of users. In our experiments we were able to securely compute differentially private top-k in less than 10 minutes using 3 semi-honest computation parties distributed over the Internet with inputs from hundreds of users (HH) and input size that is independent of the user count (PEM, excluding count aggregation).",
        "link": "https://dl.acm.org/doi/10.1145/3460120.3484557",
        "category": "Databases"
    },
    {
        "title": "OptDebug: Fault-Inducing Operation Isolation for Dataflow Applications",
        "authors": "['Muhammad Ali Gulzar', 'Miryung Kim']",
        "date": "November 2021",
        "source": "SoCC '21: Proceedings of the ACM Symposium on Cloud Computing",
        "abstract": "Fault-isolation is extremely challenging in large scale data processing in cloud environments. Data provenance is a dominant existing approach to isolate data records responsible for a given output. However, data provenance concerns fault isolation only in the data-space, as opposed to fault isolation in the code-space---how can we precisely localize operations or APIs responsible for a given suspicious or incorrect result? We present OptDebug that identifies fault-inducing operations in a dataflow application using three insights. First, debugging is easier with a small-scale input than a large-scale input. So it uses data provenance to simplify the original input records to a smaller set leading to test failures and test successes. Second, keeping track of operation provenance is crucial for debugging. Thus, it leverages automated taint analysis to propagate the lineage of operations downstream with individual records. Lastly, each operation may contribute to test failures to a different degree. Thus OptDebug ranks each operation's spectra---the relative participation frequency in failing vs. passing tests. In our experiments, OptDebug achieves 100% recall and 86% precision in terms of detecting faulty operations and reduces the debugging time by 17x compared to a naïve approach. Overall, OptDebug shows great promise in improving developer productivity in today's complex data processing pipelines by obviating the need to re-execute the program repetitively with different inputs and manually examine program traces to isolate buggy code.",
        "link": "https://dl.acm.org/doi/10.1145/3472883.3487016",
        "category": "Databases"
    },
    {
        "title": "AnyOLAP: analytical processing of arbitrary data-intensive applications without ETL",
        "authors": "['Felix Schuhknecht', 'Aaron Priesterroth', 'Justus Henneberg', 'Reza Salkhordeh']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "The volume of data that is processed and produced by modern data-intensive applications is constantly increasing. Of course, along with the volume, the interest in analyzing and interpreting this data increases as well. As a consequence, more and more DBMSs and processing frameworks are specialized towards the efficient execution of long-running, read-only analytical queries. Unfortunately, to enable analysis, the data first has to be moved from the source application to the analytics tool via a lengthy ETL process, which increases the runtime and complexity of the analysis pipeline.In this work, we advocate to simply skip ETL altogether. With AnyOLAP, we can perform online analysis of data directly within the source application and while it is running. In the proposed demonstration, the audience will get the chance to put AnyOLAP to the test on a set of data-intensive applications that are supposed to be analyzed while they are up and running. As the entire analysis pipeline of AnyOLAP will be exposed to the audience in form of live and interactive visualizations, users will be able to experience the benefits of true online analysis firsthand.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476354",
        "category": "Databases"
    },
    {
        "title": "Almost Linear Semantic XML Keyword Search",
        "authors": "['Joe Tekli', 'Gilbert Tekli', 'Richard Chbeir']",
        "date": "November 2021",
        "source": "MEDES '21: Proceedings of the 13th International Conference on Management of Digital EcoSystems",
        "abstract": "Many efforts have been deployed by the IR community to extend free-text query processing toward semi-structured XML search. Most methods rely on the concept of Lowest Comment Ancestor (LCA) between two or multiple structural nodes to identify the most specific XML elements containing query keywords posted by the user. Yet, few of the existing approaches consider XML semantics, and the methods that process semantics generally rely on computationally expensive word sense disambiguation (WSD) techniques, or apply semantic analysis in one stage only: performing query relaxation/refinement over the bag of words retrieval model, to reduce processing time. In this paper, we describe the building blocks of a new approach for XML keyword search aiming to solve the limitations mentioned above. Our solution first transforms the XML document collection (offline) and the keyword query (on-the-fly) into meaningful semantic representations using context-based and global disambiguation methods, specially designed to allow almost linear computation efficiency. Consequently, the semantically augmented XML data tree is processed for structural node clustering, based on semantic query concepts (i.e., key-concepts), in order to identify and rank candidate answer sub-trees containing related occurrences of query key-concepts. Preliminary experiments highlight the quality and potential of our approach.",
        "link": "https://dl.acm.org/doi/10.1145/3444757.3485079",
        "category": "Databases"
    },
    {
        "title": "Towards an optimized GROUP by abstraction for large-scale machine learning",
        "authors": "['Side Li', 'Arun Kumar']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "Many applications that use large-scale machine learning (ML) increasingly prefer different models for subgroups (e.g., countries) to improve accuracy, fairness, or other desiderata. We call this emerging popular practice learning over groups, analogizing to GROUP BY in SQL, albeit for ML training instead of SQL aggregates. From the systems standpoint, this practice compounds the already data-intensive workload of ML model selection (e.g., hyperparameter tuning). Often, thousands of models may need to be trained, necessitating high-throughput parallel execution. Alas, most ML systems today focus on training one model at a time or at best, parallelizing hyperparameter tuning. This status quo leads to resource wastage, low throughput, and high runtimes. In this work, we take the first step towards enabling and optimizing learning over groups from the data systems standpoint for three popular classes of ML: linear models, neural networks, and gradient-boosted decision trees. Analytically and empirically, we compare standard approaches to execute this workload today: task-parallelism and data-parallelism. We find neither is universally dominant. We put forth a novel hybrid approach we call grouped learning that avoids redundancy in communications and I/O using a novel form of parallel gradient descent we call Gradient Accumulation Parallelism (GAP). We prototype our ideas into a system we call Kingpin built on top of existing ML tools and the flexible massively-parallel runtime Ray. An extensive empirical evaluation on large ML benchmark datasets shows that Kingpin matches or is 4x to 14x faster than state-of-the-art ML systems, including Ray's native execution and PyTorch DDP.",
        "link": "https://dl.acm.org/doi/10.14778/3476249.3476284",
        "category": "Databases"
    },
    {
        "title": "Dynamic Structural Clustering on Graphs",
        "authors": "['Boyu Ruan', 'Junhao Gan', 'Hao Wu', 'Anthony Wirth']",
        "date": "June 2021",
        "source": "SIGMOD '21: Proceedings of the 2021 International Conference on Management of Data",
        "abstract": "\\em Structural Clustering ($\\strclu$) is one of the most popular graph clustering paradigms. In this paper, we consider $\\strclu$ under Jaccard similarity on a dynamic graph, G = (V, E), subject to edge insertions and deletions (updates). The goal is to maintain certain information under updates, so that the strclu clustering result on~G can be retrieved in O(|V| + |E|)$ time, upon request. The state-of-the-art worst-case cost is~O(|V|) per update; we improve this update-time bound \\em significantly with the ρ-approximate notion. Specifically, for a specified failure probability, δ^*, and \\em every sequence of~M updates (no need to know M's value in advance), our algorithm, $\\dynelm$, achieves~O(?og^2 |V| + og |V| \\cdot ?og \\fracM ?^* )$ amortized cost for each update, \\em at all times in linear space. Moreover, $\\dynelm$ provides a provable \"sandwich'' guarantee on the clustering quality at all times after each update with probability at least 1 - ^*. We further develop dynelm into our ultimate algorithm, dynstr, which also supports \\em cluster-group-by queries. Given Q \\subseteq V, this puts the non-empty intersection of Q and each strclu cluster into a distinct group. dynstr not only achieves all the guarantees of dynelm, but also runs \\em cluster-group-by queries in~O(|Q|\\cdot og |V|) time. We demonstrate the performance of our algorithms via extensive experiments, on 15 real datasets. Experimental results confirm that our algorithms are up to three orders of magnitude more efficient than state-of-the-art competitors, and still provide quality structural clustering results.",
        "link": "https://dl.acm.org/doi/10.1145/3448016.3452828",
        "category": "Databases"
    },
    {
        "title": "FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities",
        "authors": "['Pavlos Fafalios', 'Kostas Petrakis', 'Georgios Samaritakis', 'Korina Doerr', 'Athina Kritsotaki', 'Yannis Tzitzikas', 'Martin Doerr']",
        "date": "None",
        "source": "Journal on Computing and Cultural Heritage ",
        "abstract": "Descriptive and empirical sciences, such as History, are the sciences that collect, observe and describe phenomena to explain them and draw interpretative conclusions about influences, driving forces and impacts under given circumstances. Spreadsheet software and relational database management systems are still the dominant tools for quantitative analysis and overall data management in these these sciences, allowing researchers to directly analyse the gathered data and perform scholarly interpretation. However, this current practice has a set of limitations, including the high dependency of the collected data on the initial research hypothesis, usually useless for other research, the lack of representation of the details from which the registered relations are inferred, and the difficulty to revisit the original data sources for verification, corrections or improvements. To cope with these problems, in this article we present FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar forms of empirical research. We describe the related challenges, the overall methodology we follow for supporting semantic interoperability, and discuss the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT, which examines economic, social and demographic impacts of the introduction of steamboats in the Mediterranean area between the 1850s and the 1920s.",
        "link": "https://dl.acm.org/doi/10.1145/3461460",
        "category": "Databases"
    },
    {
        "title": "Heuristics for Genome Rearrangement Distance With Replicated Genes",
        "authors": "['Gabriel Siqueira', 'Klairton Lima Brito', 'Ulisses Dias', 'Zanoni Dias']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "In comparative genomics, one goal is to find similarities between genomes of different organisms. Comparisons using genome features like genes, gene order, and regulatory sequences are carried out with this purpose in mind. Genome rearrangements are mutational events that affect large extensions of the genome. They are responsible for creating extant species with conserved genes in different positions across genomes. Close species &#x2014; from an evolutionary point of view &#x2014; tend to have the same set of genes or share most of them. When we consider gene order to compare two genomes, it is possible to use a parsimony criterion to estimate how close the species are. We are interested in the shortest sequence of genome rearrangements capable of transforming one genome into the other, which is named <italic>rearrangement distance</italic>. Reversal is one of the most studied genome rearrangements events. This event acts in a segment of the genome, inverting the position and the orientation of genes in it. Transposition is another widely studied event. This event swaps the position of two consecutive segments of the genome. When the genome has no gene repetition, a common approach is to map it as a permutation such that each element represents a conserved block. When genomes have replicated genes, this mapping is usually performed using strings. The number of replicas depends on the organisms being compared, but in many scenarios, it tends to be small. In this work, we study the rearrangement distance between genomes with replicated genes considering that the orientation of genes is unknown. We present four heuristics for the problem of genome rearrangement distance with replicated genes. We carry out experiments considering the exclusive use of the reversals or transpositions events, as well as the version in which both events are allowed. We developed a database of simulated genomes and compared our results with other algorithms from the literature. The experiments showed that our heuristics with more sophisticated rules presented a better performance than the known algorithms to estimate the evolutionary distance between genomes with replicated genes. In order to validate the application of our algorithms in real data, we construct a phylogenetic tree based on the distance provided by our algorithm and compare it with a know tree from the literature.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2021.3095021",
        "category": "Databases"
    },
    {
        "title": "Text Mining Approach for Identifying Research Trends",
        "authors": "['Snezhana Sulova']",
        "date": "June 2021",
        "source": "CompSysTech '21: Proceedings of the 22nd International Conference on Computer Systems and Technologies",
        "abstract": "With the increase of unstructured data, the issues connected with automatic text processing, the categorization of documents and the discovery of topics have become objects of growing interest. In order to improve the process of grouping and processing research publications, we would like to propose a method based upon natural language processing. It is based on text mining technologies which aim to identify key tendencies in documents. It processes the content of publications by clustering and identifies the topics of each identified group. This analysis helps by identifying key tendencies as well as discovering emerging new areas of research. Publications from the research literature database, Scopus, were used to test the approach. The topic of the publications is “the application of digital technologies in the logistics business”. The experiments were completed using the RapidMiner Studio software.",
        "link": "https://dl.acm.org/doi/10.1145/3472410.3472433",
        "category": "Databases"
    },
    {
        "title": "3D Visual Discomfort Prediction Based on Scene Structure and Depth Difference",
        "authors": "['Hongmei Liu', 'Huabiao Qin', 'Shixin Huang']",
        "date": "March 2021",
        "source": "IVSP '21: Proceedings of the 2021 3rd International Conference on Image, Video and Signal Processing",
        "abstract": "Feature engineering under the stereoscopic images is widely used for visual discomfort prediction. Due to the complexity of the human visual system, extensive feature representation face major challenges, including a lack of inconsistent representation between the response of visual perception and the stimulation of stereoscopic display. To solve this problem, a saliency contrast feature representation method is proposed. Specifically, inspired by human attention mechanism, the stereo image is divided into saliency region and no-saliency region to estimate human fixations from complex natural scenes. The relative distance between saliency region and no-saliency region is obtained to express scene structure features, which combines the visual characteristic and scene structure factor. Besides, depth difference features are represented by combining subjectively perceived depth with objective simulated depth of each region. To evaluate the effectiveness of the proposed method, typical regression models are used to capture the relationship of the extracted features to visual discomfort scores. The experiments conducted on the benchmark IEEE-SA and IVY LAB S3D database show that the superiority of the proposed method and get the optimal solution with random forests as the regression model.",
        "link": "https://dl.acm.org/doi/10.1145/3459212.3459220",
        "category": "Databases"
    },
    {
        "title": "PUGCQ: A Large Scale Dataset for Quality Assessment of Professional User-Generated Content",
        "authors": "['Guo Li', 'Baoliang Chen', 'Lingyu Zhu', 'Qinwen He', 'Hongfei Fan', 'Shiqi Wang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Recent years have witnessed a surge of professional user-generated content (PUGC) based video services, coinciding with the accelerated proliferation of video acquisition devices such as mobile phones, wearable cameras, and unmanned aerial vehicles. Different from traditional UGC videos by impromptu shooting, PUGC videos produced by professional users tend to be carefully designed and edited, receiving high popularity with a relatively satisfactory playing count. In this paper, we systematically conduct the comprehensive study on the perceptual quality of PUGC videos and introduce a database consisting of 10,000 PUGC videos with subjective ratings. In particular, during the subjective testing, we collect the human opinions based upon not only the MOS, but also the attributes that could potentially influence the visual quality including face, noise, blur, brightness, and color. We make the attempt to analyze the large-scale PUGC database with a series of video quality assessment (VQA) algorithms and a dedicated baseline model based on pretrained deep neural network is further presented. The cross-dataset experiments reveal a large domain gap between the PUGC and the traditional user-generated videos, which are critical in learning based VQA. These results shed light on developing next-generation PUGC quality assessment algorithms with desired properties including promising generalization capability, high accuracy, and effectiveness in perceptual optimization. The dataset and the codes are released at https://github.com/wlkdb/pugcq_create.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475183",
        "category": "Databases"
    },
    {
        "title": "Sporq: An Interactive Environment for Exploring Code using Query-by-Example",
        "authors": "['Aaditya Naik', 'Jonathan Mendelson', 'Nathaniel Sands', 'Yuepeng Wang', 'Mayur Naik', 'Mukund Raghothaman']",
        "date": "October 2021",
        "source": "UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology",
        "abstract": "There has been widespread adoption of IDEs and powerful tools for program analysis. However, programmers still find it difficult to conveniently analyze their code for custom patterns. Such systems either provide inflexible interfaces or require knowledge of complex query languages and compiler internals. In this paper, we present Sporq, a tool that allows developers to mine their codebases for a range of patterns, including bugs, code smells, and violations of coding standards. Sporq offers an interactive environment in which the user highlights program elements, and the system responds by identifying other parts of the codebase with similar patterns. The programmer can then provide feedback which enables the system to rapidly infer the programmer’s intent. Internally, our system is driven by high-fidelity relational program representations and algorithms to synthesize database queries from examples. Our experiments and user studies with a VS Code extension indicate that Sporq reduces the effort needed by programmers to write custom analyses and discover bugs in large codebases.",
        "link": "https://dl.acm.org/doi/10.1145/3472749.3474737",
        "category": "Databases"
    },
    {
        "title": "Gazelle: transcript abundance query against large-scale RNA-seq experiments",
        "authors": "['Xiaofei Zhang', 'Ye Yu', 'Chan Hee Mok', 'James N. MacLeod', 'Jinze Liu']",
        "date": "August 2021",
        "source": "BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "abstract": "The exponential growth of high throughput sequencing data has been witnessed in almost every sequencing data repository. To date, most of the exploratory analysis on these large datasets requires heavy lifting data processing pipelines that are both resource and labor intensive. Very recently, various algorithms have been developed to enable arbitrary sequence query over large collections of sequencing data. These algorithms were designed to support presence/absence query, i.e., screening for RNA-seq samples containing a given transcript sequence. Their utility is rather limited as they cannot retrieve abundance information of query sequence. Such abundance information is indeed critical in real applications in order to understand how the variation of transcript expression associates with different biological conditions or disease subtypes. In this paper, we present Gazelle, a sequence query engine that enables fast and quantified query against large-scale RNA-seq experiments. Gazelle exploits the advantages of two different types of hashing algorithms and seamlessly combines them into one integrated structure to support highly efficient and accurate sequence queries with abundance. We evaluated the performance of Gazelle on three datasets to benchmark its efficiency, accuracy as well as its utility in real-life applications. Our result shows that Gazelle achieves near-perfect k-mer query, supports on-demand sequence query against moderately large sequence database, and renders highly consistent abundance estimation with RT-qPCR as well as traditional transcript quantification method such as Kallisto.",
        "link": "https://dl.acm.org/doi/10.1145/3459930.3469548",
        "category": "Databases"
    },
    {
        "title": "Identifying SQL Misconceptions of Novices: Findings from a Think-Aloud Study",
        "authors": "['Daphne Miedema', 'Efthimia Aivaloglou', 'George Fletcher']",
        "date": "August 2021",
        "source": "ICER 2021: Proceedings of the 17th ACM Conference on International Computing Education Research",
        "abstract": "SQL is the most commonly taught database query language. While previous research has investigated the errors made by novices during SQL query formulation, the underlying causes for these errors have remained unexplored. Understanding the basic misconceptions held by novices which lead to these errors would help improve how we teach query languages to our students. In this paper we aim to identify the misconceptions that might be the causes of documented SQL errors that novices make. To this end, we conducted a qualitative think-aloud study to gather information on the thinking process of university students while solving query formulation problems. With the queries in hand, we analyzed the underlying causes for the errors made by our participants. In this paper we present the identified SQL misconceptions organized into four top-level categories: misconceptions based in previous course knowledge, generalization-based misconceptions, language-based misconceptions, and misconceptions due to an incomplete or incorrect mental model. A deep exploration of misconceptions can uncover gaps in instruction. By drawing attention to these, we aim to improve SQL education.",
        "link": "https://dl.acm.org/doi/10.1145/3446871.3469759",
        "category": "Databases"
    },
    {
        "title": "Online Additive Quantization",
        "authors": "['Qi Liu', 'Jin Zhang', 'Defu Lian', 'Yong Ge', 'Jianhui Ma', 'Enhong Chen']",
        "date": "August 2021",
        "source": "KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "abstract": "Approximate nearest neighbor search (ANNs) plays an important role in many applications ranging from information retrieval, recommender systems to machine translation. Several ANN indexes, such as hashing and quantization, have been designed to update for the evolving database, but there exists a remarkable performance gap between them and retrained indexes on the entire database. To close the gap, we propose an online additive quantization algorithm (online AQ) to dynamically update quantization codebooks with the incoming streaming data. Then we derive the regret bound to theoretically guarantee the performance of the online AQ algorithm. Moreover, to improve the learning efficiency, we develop a randomized block beam search algorithm for assigning each data to the codewords of the codebook. Finally, we extensively evaluate the proposed online AQ algorithm on four real-world datasets, showing that it remarkably outperforms the state-of-the-art baselines.",
        "link": "https://dl.acm.org/doi/10.1145/3447548.3467441",
        "category": "Databases"
    },
    {
        "title": "More Style, Less Work: Card-style Data Decrease Risk-limiting Audit Sample Sizes",
        "authors": "['Amanda K. Glazer', 'Jacob V. Spertus', 'Philip B. Stark']",
        "date": "None",
        "source": "Digital Threats: Research and Practice",
        "abstract": "U.S. elections rely heavily on computers such as voter registration databases, electronic pollbooks, voting machines, scanners, tabulators, and results reporting websites. These introduce digital threats to election outcomes. Risk-limiting audits (RLAs) mitigate threats to some of these systems by manually inspecting random samples of ballot cards. RLAs have a large chance of correcting wrong outcomes (by conducting a full manual tabulation of a trustworthy record of the votes), but can save labor when reported outcomes are correct. This efficiency is eroded when sampling cannot be targeted to ballot cards that contain the contest(s) under audit. If the sample is drawn from all cast cards, then RLA sample sizes scale like the reciprocal of the fraction of ballot cards that contain the contest(s) under audit. That fraction shrinks as the number of cards per ballot grows (i.e., when elections contain more contests) and as the fraction of ballots that contain the contest decreases (i.e., when a smaller percentage of voters are eligible to vote in the contest). States that conduct RLAs of contests on multi-card ballots or RLAs of small contests can dramatically reduce sample sizes by using information about which ballot cards contain which contests—by keeping track of card-style data (CSD). For instance, CSD reduce the expected number of draws needed to audit a single countywide contest on a 4-card ballot by 75%. Similarly, CSD reduce the expected number of draws by 95% or more for an audit of two contests with the same margin on a 4-card ballot if one contest is on every ballot and the other is on 10% of ballots. In realistic examples, the savings can be several orders of magnitude.",
        "link": "https://dl.acm.org/doi/10.1145/3457907",
        "category": "Databases"
    },
    {
        "title": "Design of Smart Travel Management System Based on Cloud Service",
        "authors": "['Gao Fang']",
        "date": "December 2021",
        "source": "EBIMCS '21: Proceedings of the 2021 4th International Conference on E-Business, Information Management and Computer Science",
        "abstract": "The design and implementation of the database is an important task in system development. Many information in the management of smart tourism needs to be saved and managed so that these data can be analyzed later. Therefore, in the process of designing the system, the original number of system requirements is analyzed, and the relationship between business entities is established. Combined with the relational database used by the system, E-R diagram drawing and data table implementation were performed.",
        "link": "https://dl.acm.org/doi/10.1145/3511716.3511764",
        "category": "Databases"
    },
    {
        "title": "Perception-Oriented Stereo Image Super-Resolution",
        "authors": "['Chenxi Ma', 'Bo Yan', 'Weimin Tan', 'Xuhao Jiang']",
        "date": "October 2021",
        "source": "MM '21: Proceedings of the 29th ACM International Conference on Multimedia",
        "abstract": "Recent studies of deep learning based stereo image super-resolution (StereoSR) have promoted the development of StereoSR. However, existing StereoSR models mainly concentrate on improving quantitative evaluation metrics and neglect the visual quality of super-resolved stereo images. To improve the perceptual performance, this paper proposes the first perception-oriented stereo image super-resolution approach by exploiting the feedback, provided by the evaluation on the perceptual quality of StereoSR results. To provide accurate guidance for the StereoSR model, we develop the first special stereo image super-resolution quality assessment (StereoSRQA) model, and further construct a StereoSRQA database. Extensive experiments demonstrate that our StereoSR approach significantly improves the perceptual quality and enhances the reliability of stereo images for disparity estimation.",
        "link": "https://dl.acm.org/doi/10.1145/3474085.3475408",
        "category": "Databases"
    },
    {
        "title": "Construction of Eriocheir sinensis Protein-protein Interaction Network and Extraction of Molting Sub-network",
        "authors": "['Tong Hao', 'Yifei Gou', 'Jingjing Li', 'Bin Wang', 'Yicheng Zhang', 'Jinsheng Sun']",
        "date": "January 2022",
        "source": "ICBBB '22: Proceedings of the 2022 12th International Conference on Bioscience, Biochemistry and Bioinformatics",
        "abstract": "Eriocheir sinensis is an important aquatic economic animal. Its molting process is closely related to its growth and development. The current research on molting mechanism of E. sinensis mainly focuses on the single gene or tissue. The systematic analysis is missing in the study on the process and mechanism of molting. Protein-protein interaction network (PIN) is an important tool for systematical study of some specific functions and their regulation process. In order to systematically explore the molting process of E. sinensis, a PIN of E. sinensis was constructed with homology alignment method based on its transcriptome data of multi tissues, combined with PINA and STRING database. The PIN contains 8,225 proteins and 148,524 protein-protein interactions (PPIs), corresponding to 31,507 unigenes in the transcriptome, which is currently the largest PIN of E. sinensis. Then the molting sub-network was extracted from the PIN according to GO annotation. The sub-network was composed of 40 proteins and 64 PPIs. These proteins are distributed in 14 molting related functions, mainly including the cuticle formation, cuticle development and sclerotization, cuticle attachment to epithelium, and the growth and regulation of hair, cuticle pigmentation and regulation. The sub-network reflected some regulation relationships among these molting related functions. The E. sinensis PIN provides an important tool for the systematic study of its cellular activities. The molting sub-network supplies important clues for the further experimental study of the molting mechanism in E. sinensis.",
        "link": "https://dl.acm.org/doi/10.1145/3510427.3510438",
        "category": "Databases"
    },
    {
        "title": "ParaFold: Paralleling AlphaFold for Large-Scale Predictions",
        "authors": "['Bozitao Zhong', 'Xiaoming Su', 'Minhua Wen', 'Sicheng Zuo', 'Liang Hong', 'James Lin']",
        "date": "January 2022",
        "source": "HPCAsia 2022 Workshop: International Conference on High Performance Computing in Asia-Pacific Region Workshops",
        "abstract": "AlphaFold developed by DeepMind predicts protein structures from the amino acid sequence at or near experimental resolution, solving the 50-year-old protein folding challenge, leading to progress by transforming large-scale genomics data into protein structures. AlphaFold will also greatly change the scientific research model from low-throughput to high-throughput manner. The overall AlphaFold prediction process consists of two stages: 1) MSA construction based on CPUs and 2) model inferences on GPUs. In the first stage, AlphaFold uses CPUs only, taking up to hours for MSA construction of a single protein due to the large database sizes and I/O bottlenecks. However, GPUs in this stage remain idle, resulting in low GPU utilization and restricting the capacity of large-scale structure predictions. Therefore, we proposed “ParaFold”, an open-source parallel version of AlphaFold for high throughput protein structure predictions. ParaFold separates the CPU and GPU parts to enable large-scale structure predictions and to improve GPU utilization. ParaFold also effectively reduces the CPU and GPU runtime with two optimizations without compromising the quality of prediction results: using multi-threaded parallelism on CPUs and using optimized JAX compilation on GPUs. We evaluated ParaFold with three datasets of different protein lengths. We showed the large-scale structure prediction capability by running model 1 inference of ∼ 20,000 small proteins in 5.4 hours on one NVIDIA DGX-2. With the CPU/GPU separation and JAX compile optimization, the total GPU runtime was reduced to 5.4 hours, compared with 1,352.6 hours when using AlphaFold, achieving a 99.7% GPU runtime reduction. ParaFold largely increased the protein structure prediction capacity of GPU per day, getting a 250X speedup over AlphaFold with this case (∼ 20,000 proteins of the same 50 residues). ParaFold offers an rapid and effective approach for high-throughput structure predictions, leveraging the predictive power by running on supercomputers, with shorter time and at a lower cost. The development of ParaFold will greatly speed up high-throughput studies and render the protein “structure-omics” feasible.",
        "link": "https://dl.acm.org/doi/10.1145/3503470.3503471",
        "category": "Databases"
    },
    {
        "title": "SCIFFS: Enabling Secure Third-Party Security Analytics using Serverless Computing",
        "authors": "['Isaac Polinsky', 'Pubali Datta', 'Adam Bates', 'William Enck']",
        "date": "June 2021",
        "source": "SACMAT '21: Proceedings of the 26th ACM Symposium on Access Control Models and Technologies",
        "abstract": "Third-party security analytics allow companies to outsource threat monitoring tasks to teams of experts and avoid the costs of in-house security operations centers. By analyzing telemetry data from many clients these services are able to offer enhanced insights, identifying global trends and spotting threats before they reach most customers. Unfortunately, the aggregation that drives these insights simultaneously risks exposing sensitive client data if it is not properly sanitized and tracked. In this work, we present SCIFFS, an automated information flow monitoring framework for preventing sensitive data exposure in third-party security analytics platforms. SCIFFS performs decentralized information flow control over customer data it in a serverless setting, leveraging the innate polyinstantiated nature of serverless functions to assure precise and lightweight tracking of data flows. Evaluating SCIFFS against a proof-of-concept security analytics framework on the widely-used OpenFaaS platform, we demonstrate that our solution supports common analyst workflows data ingestion, custom dashboards, threat hunting) while imposing just 3.87% runtime overhead on event ingestion and the overhead on aggregation queries grows linearly with the number of records in the database (e.g., 18.75% for 50,000 records and 104.27% for 500,000 records) as compared to an insecure baseline. Thus, SCIFFS not only establishes a privacy-respecting model for third-party security analytics, but also highlights the opportunities for security-sensitive applications in the serverless computing model.",
        "link": "https://dl.acm.org/doi/10.1145/3450569.3463567",
        "category": "Databases"
    },
    {
        "title": "The Title of the Paper: Research on insurance marketing application based on hash link-table improved association rule algorithm",
        "authors": "['Xianmei He', 'Shaohua Teng']",
        "date": "February 2022",
        "source": "ICMLC '22: Proceedings of the 2022 14th International Conference on Machine Learning and Computing",
        "abstract": "Aiming at the problem of slow processing efficiency of Apriori algorithm when the amount of data is large, the association rule algorithm is combined with hash link-table, and an improved association rule algorithm based on hash link-table is proposed to solve the disadvantage of long average time-consuming of traditional association rule algorithm in finding frequent itemsets. The improved association rule algorithm through hash link-table is applied to the insurance marketing scenario with large data set, analyzes the insurance purchase behavior of a large number of customers in the insurance database, finds out the insurance product sales association rules, and accurately recommends the products they are interested in to customers, so as to increase the success rate of marketing.",
        "link": "https://dl.acm.org/doi/10.1145/3529836.3529840",
        "category": "Databases"
    },
    {
        "title": "Succinct range filters",
        "authors": "['Huanchen Zhang', 'Hyeontaek Lim', 'Viktor Leis', 'David G. Andersen', 'Michael Kaminsky', 'Kimberly Keeton', 'Andrew Pavlo']",
        "date": "April 2021",
        "source": "Communications of the ACM",
        "abstract": "We present the Succinct Range Filter (SuRF), a fast and compact data structure for approximate membership tests. Unlike traditional Bloom filters, SuRF supports both single-key lookups and common range queries, such as range counts. SuRF is based on a new data structure called the Fast Succinct Trie (FST) that matches the performance of state-of-the-art order-preserving indexes, while consuming only 10 bits per trie node---a space close to the minimum required by information theory. Our experiments show that SuRF speeds up range queries in a widely used database storage engine by up to 5×.",
        "link": "https://dl.acm.org/doi/10.1145/3450262",
        "category": "Databases"
    },
    {
        "title": "Automated recognition of geographical named entities in titles of Ukiyo-e prints",
        "authors": "['Marita Chatzipanagiotou', 'Ewa Machotka', 'John Pavlopoulos']",
        "date": "December 2021",
        "source": "DHW 2021: Digital Humanities Workshop",
        "abstract": "This paper investigates the application of Natural Language Processing as a means to study the relationship between topography and its visual renderings in early modern Japanese ukiyo-e landscape prints. We introduce a new dataset with titles of landscape prints that have been annotated by an art historian for any included place-names. The prints are hosted by the digital database of the Art Research Center at the Ritsumeikan University, Kyoto, one of the hubs of Digital Humanities in Japan. By applying, calibrating and assessing a Named Entity Recognition (NER) tool, we argue that ‘distant viewing’ or macroanalysis of visual datasets can be facilitated, which is needed to assist art historical studies of this rich, complex and diverse research material. Experimental results indicated that the performance of NER can be improved by 30% and reach 50% precision, by using part of the introduced dataset.",
        "link": "https://dl.acm.org/doi/10.1145/3526242.3526254",
        "category": "Databases"
    },
    {
        "title": "Design of Xinjiang Uygur Sports Virtual Museum Management System Based on ASP.NET",
        "authors": "['Ping Wang']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "The traditional virtual museum management system design can only meet the management system of some areas, and also lack the function of exhibits. To this end, the paper puts forward a design of Xinjiang Uygur Sports Virtual Museum Management System based on ASP.NET. First of all, the ASP.NET technology is summarized, to analyze structure and function of the management system; and then a virtual museum is created, and the database is operated to achieve the exhibition release and user browsing function; finally, the virtual museum management system design is performed. Experiments show that it is not only conducive to the management and protection of the collection of goods, while also achieving a wider range of resource sharing.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3484029",
        "category": "Databases"
    },
    {
        "title": "SEMOUR: A Scripted Emotional Speech Repository for Urdu",
        "authors": "['Nimra Zaheer', 'Obaid Ullah Ahmad', 'Ammar Ahmed', 'Muhammad Shehryar Khan', 'Mudassir Shabbir']",
        "date": "May 2021",
        "source": "CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "abstract": "Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufficient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the first scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition System. Our gender-balanced dataset contains 15,040 unique instances recorded by eight professional actors eliciting a syntactically complex script. The dataset is phonetically balanced, and reliably exhibits a varied set of emotions as marked by the high agreement scores among human raters in experiments. We also provide various baseline speech emotion prediction scores on the database, which could be used for various applications like personalized robot assistants, diagnosis of psychological disorders, and getting feedback from a low-tech-enabled population, etc. On a random test sample, our model correctly predicts an emotion with a state-of-the-art 92% accuracy.",
        "link": "https://dl.acm.org/doi/10.1145/3411764.3445171",
        "category": "Databases"
    },
    {
        "title": "Playing Fetch with CAT: Composing Cache Partitioning and Prefetching for Task-based Query Processing",
        "authors": "['Qitian Zeng', 'Kyle C. Hale', 'Boris Glavic']",
        "date": "June 2021",
        "source": "DAMON '21: Proceedings of the 17th International Workshop on Data Management on New Hardware",
        "abstract": "Software prefetching and hardware-based cache allocation techniques (CAT) have been successfully applied in main-memory database engines to fetch data into cache before it is needed and to partition a shared last-level cache (LLC) to prevent concurrent tasks from evicting each others' data. We investigate the interaction of these techniques and demonstrate that while a single prefetching strategy is sufficient, the combination of both techniques is only effective if the cache partitioning strategy adapts the partitioning based on the types of tasks currently sharing an LLC. We present a simple, yet effective, scheme that uses prefetching and adapts cache partition allocations dynamically.",
        "link": "https://dl.acm.org/doi/10.1145/3465998.3466016",
        "category": "Databases"
    },
    {
        "title": "Development of Disease Analysis Module Based on Highway Bridge Inspection and Evaluation System",
        "authors": "['Qiang Xu', 'Lixin Liu', 'Jia Yu', 'Jianyong Song']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "In order to meet the needs of inspectors in applying the bridge inspection and evaluation system for disease cause analysis, firstly analyze and evaluate the functions of the existing highway bridge inspection and evaluation system, and on this basis, the technical route to realize the development of bridge disease cause analysis and maintenance countermeasures module is formulated. Secondly, based on the MySQL database technology, a knowledge base system for the analysis of the causes of bridge diseases and maintenance countermeasures was constructed, and the database of the existing highway bridge inspection and evaluation system was improved. Finally, a Web terminal bridge disease cause analysis and maintenance countermeasure module was developed, which provides an assistant for inspectors with insufficient knowledge and experience in analyzing the cause of bridge disease. The automatic report generation function is improved, and the work quality and efficiency are improved. Improving the automation level of bridge disease analysis, it has laid the foundation for the research of intelligent diagnosis of bridge disease.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495470",
        "category": "Databases"
    },
    {
        "title": "How Inclusive are We?",
        "authors": "['Angela Bonifati', 'Michael J. Mior', 'Felix Naumann', 'Nele Sina Noack']",
        "date": "December 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "ACM SIGMOD, VLDB and other database organizations have committed to fostering an inclusive and diverse community, as do many other scientific organizations. Recently, different measures have been taken to advance these goals, especially for underrepresented groups. One possible measure is double-blind reviewing, which aims to hide gender, ethnicity, and other properties of the authors.We report the preliminary results of a gender diversity analysis of publications of the database community across several peer-reviewed venues, and also compare women's authorship percentages in both single-blind and double-blind venues along the years. We also obtained a cross comparison of the obtained results in data management with other relevant areas in Computer Science.",
        "link": "https://dl.acm.org/doi/10.1145/3516431.3516438",
        "category": "Databases"
    },
    {
        "title": "MoreData: A Geospatial Data Enrichment Framework",
        "authors": "['Leonardo J. A. S. Figueiredo', 'Germano B. dos Santos', 'Raissa P. P. M. Souza', 'Fabrício A. Silva', 'Thais R. M. Braga Silva']",
        "date": "November 2021",
        "source": "SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems",
        "abstract": "In recent years, we are facing a significant increase in the collection and availability of geospatial data. This type of data is paramount to help decision makers in different contexts, such as smart cities, mobile social networks, and e-commerce. In addition, the behavior of mobile users can be extracted and exploited to improve the quality of the services offered by mobile providers. However, raw location data needs to be semantically enriched to be useful, which is an arduous task because it requires queries on different data sources of different formats and complex joins. To mitigate this difficulty, this work proposes MoreData, a flexible and expandable framework for the semantic enrichment of geospatial data. The framework accepts different input formats, provides connectors to different sources (APIs, Relational Database, Fast Search Engine Tool, and Open Street Map), and is easy to extend to new sources. To evaluate the framework's benefits, we use it to enrich a real database comprised of thousands of mobile users' locations with data. We observe that the adoption of the framework leads to less time and effort in the data enrichment process, saving time for the more important tasks, such as building and analyzing models.",
        "link": "https://dl.acm.org/doi/10.1145/3474717.3484210",
        "category": "Databases"
    },
    {
        "title": "Auto Insurance Knowledge Graph Construction and Its Application to Fraud Detection",
        "authors": "['Long Zhang', 'Tianxing Wu', 'Xiuqi Chen', 'Bingjie Lu', 'Chongning Na', 'Guilin Qi']",
        "date": "December 2021",
        "source": "IJCKG '21: Proceedings of the 10th International Joint Conference on Knowledge Graphs",
        "abstract": "In recent years, feature engineering based machine learning models have made great progress in auto insurance fraud detection. However, their performance on single fraud case detection has never reached to a high level, and they cannot detect gang frauds. One of the main causes is that such machine learning models directly neglect the associations between auto insurance cases. To resolve this problem, we propose to leverage knowledge graph techniques to discover associations between cases for fraud detection. We first construct an auto insurance knowledge graph (AIKG) with ontology building and knowledge extraction from relational database. We then apply AIKG to both gang fraud detection and single fraud case detection. We finally conduct comprehensive experiments on fraud detection, and our methods significantly outperform state-of-the-art baselines in different evaluation metrics. Our built auto insurance ontology which is the core part of AIKG has been published on the Web and can be open access.",
        "link": "https://dl.acm.org/doi/10.1145/3502223.3502231",
        "category": "Databases"
    },
    {
        "title": "Study on Acupuncture Rehabilitation Nursing Measures for Patients with Limb Dysfunction Based on Systematic Evaluation Results",
        "authors": "['Jingru Sun', 'Jiaxiang Zheng']",
        "date": "May 2021",
        "source": "CIPAE 2021: 2021 2nd International Conference on Computers, Information Processing and Advanced Education",
        "abstract": "Acupuncture belongs to the quintessence of Chinese medicine and has been applied for thousands of years. It is of positive significance to do rehabilitation nursing for patients with limb dysfunction. Search the medical journal database of wanfang data Medical Information System, CNKI, and academic dissertation database, etc., and obtain all random and semi-random controlled trials involving acupuncture and moxibustion for limb dysfunction, and then make statistical analysis on the included literatures. A total of 10 randomized controlled trials with 759 patients were included. Meta-analysis showed that there were significant differences in scores of simple mental state examination scale, Montreal cognitive assessment scale, activities of daily living scale, clock drawing test and total effective rate between the experimental group and the control group. The results of this meta-analysis suggest that acupuncture combined Meta cognitive rehabilitation training is better than cognitive rehabilitation training or drugs alone. However, due to the low quality of the original literature, high-quality, multicenter, large sample randomized blind controlled trials are needed to confirm it.",
        "link": "https://dl.acm.org/doi/10.1145/3456887.3459692",
        "category": "Databases"
    },
    {
        "title": "A Hypergraph-based Method for Pharmaceutical Data Similarity Retrieval",
        "authors": "['Mingyue Li', 'Lixin Du', 'Jiangying Xu', 'Chen Guo']",
        "date": "September 2021",
        "source": "ICBDT '21: Proceedings of the 4th International Conference on Big Data Technologies",
        "abstract": "Drug and compound similarity retrieval is very important for new drug research and development. Most pharmaceutical database provide keyword searching service. Because keyword search method cannot identify entity semantic similarity information, so the retrieval results often got poor drug or compound semantic similarity. In this paper, we propose an attribute semantic similarity retrieval method for pharmaceutical data based on hypergraph and natural language processing technology. Firstly, we use natural language processing technology to research and construct the drug attribute semantic similarity network. Then, we continue building a hypergraph based on drug attribute to get better retrieval efficiency. The experimental results show that, our method can provide similarity retrieval service for researchers.",
        "link": "https://dl.acm.org/doi/10.1145/3490322.3490344",
        "category": "Databases"
    },
    {
        "title": "Research on construction of intelligent information Service Platform based on educational function of university library",
        "authors": "['Bing Zhang', 'Lina Si', 'Yixin Chen']",
        "date": "December 2021",
        "source": "EBIMCS '21: Proceedings of the 2021 4th International Conference on E-Business, Information Management and Computer Science",
        "abstract": "With the continuous development of information-based big data era, intelligent information service has been widely used in various fields. University library service is also facing new opportunities and challenges in this process, and intelligent library is emerging at the historic moment, this paper bases its research subject on the literature concerning \"College library education and information service platform\" from 1982 to 2020 in CNKI database, deeply discusses the construction mode of intelligent information service platform of university libraries, and puts forward effective suggestions to provide a solid basis for improving the ability and level of intelligent teaching and learning of university libraries.",
        "link": "https://dl.acm.org/doi/10.1145/3511716.3511758",
        "category": "Databases"
    },
    {
        "title": "Deciphering Key Genes and miRNAs Associated With Hepatocellular Carcinoma via Network-Based Approach",
        "authors": "['Sachin Bhatt', 'Prithvi Singh', 'Archana Sharma', 'Arpita Rai', 'Ravins Dohare', 'Shweta Sankhwar', 'Akash Sharma', 'Mansoor Ali Syed']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Hepatocellular carcinoma (HCC)is a common type of liver cancer and has a high mortality world-widely. The diagnosis, prognoses, and therapeutics are very poor due to the unclear molecular mechanism of progression of the disease. To unveil the molecular mechanism of progression of HCC, we extract a large sample of mRNA expression levels from the GEO database where a total of 167 samples were used for study, and out of them, 115 samples were from HCC tumor tissue. This study aims to investigate the module of differentially expressed genes (DEGs)which are co-expressed only in HCC sample data but not in normal tissue samples. Thereafter, we identified the highly significant module of significant co-expressed genes and formed a PPI network for these genes. There were only six genes (namely, MSH3, DMC1, ALPP, IL10, ZNF223, and HSD17B7)obtained after analysis of the PPI network. Out of six only <italic>MSH3, DMC1, HSD17B7, and IL10 were</italic> found enriched in GO Term &#x0026; Pathway enrichment analysis and these candidate genes were mainly involved in cellular process, metabolic and catalytic activity, which promote the development &#x0026; progression of HCC. Lastly, the composite 3-node FFL reveals the driver miRNAs and TFs associated with our key genes.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2020.3016781",
        "category": "Databases"
    },
    {
        "title": "SPI: Automated Identification of Security Patches via Commits",
        "authors": "['Yaqin Zhou', 'Jing Kai Siow', 'Chenyu Wang', 'Shangqing Liu', 'Yang Liu']",
        "date": "None",
        "source": "ACM Transactions on Software Engineering and Methodology",
        "abstract": "Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.",
        "link": "https://dl.acm.org/doi/10.1145/3468854",
        "category": "Databases"
    },
    {
        "title": "Detection of COVID-19 in chest X-ray images using transfer learning with deep convolutional neural network",
        "authors": "['Luis Vogado', 'Pablo Vieira', 'Pedro Santos Neto', 'Lucas Lopes', 'Gleison Silva', 'Flávio Araújo', 'Rodrigo Veras']",
        "date": "March 2021",
        "source": "SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing",
        "abstract": "Over the years, Computer-Aided Diagnosis (CAD) systems have been proving their effectiveness in classifying many pathologies. With the advent of the COVID-19 pandemic, new systems were developed quickly. The chest radiography is one of the least expensive among the imaging exams that assist in the detection of COVID-19. Despite not having high sensitivity for pattern detection compared to other tests - such as ground-glass opacities in computed tomography - this test helps screen infected patients. Therefore, in this work, we propose a methodology for detecting COVID-19 in chest radiography considering three possible scenarios: the healthy, presence of COVID-19, and presence of other pathologies. We developed the methodology by evaluating transfer learning techniques in five well know pre-trained Convolutional Neural Networks (CNNs) architectures. For training CNNs, we used 1,932 healthy images, 3,651 of other pathologies, and 1,436 images related to the presence of COVID-19. We obtained an accuracy of 94.36% in the scenario COVID-19 vs. healthy, 99.80% for COVID-19 vs. others pathologies, and 95.01% differentiating in three classes. The results are considered promising when compared to state of the art since the database used in this work has the largest number of examples for the class COVID-19.",
        "link": "https://dl.acm.org/doi/10.1145/3412841.3442091",
        "category": "Databases"
    },
    {
        "title": "Based on the network pharmacology to explore the micro-mechanism of Maziren Pill in the treatment of senile functional constipation",
        "authors": "['Ya-qin Shen', 'Xiu-wen Yang', 'Yan-hua Wu', 'Li-fen Nie']",
        "date": "October 2021",
        "source": "ISAIMS '21: Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences",
        "abstract": "Mazieren pill is a classic prescription for the treatment of senile functional constipation, but its molecular mechanism is not clear. This study aims to explore the micro-mechanism of Maziren Pill in the treatment of senile functional constipation by network pharmacology. First, the active components and targets of Dahuang, Maziren, Xingren, Zhishi, Baishao, and Houpu were searched by TCMSP; The disease targets of senile functional constipation were collected by the GeneCards database, and the main component targets of Maziren Pill and disease targets were crossed to obtain the efficacy target point. Then the Cytoscape 3.8 software was used to construct the drug-active component-target interaction network of Maziren pill in the treatment of senile functional constipation, and PPI protein interaction network was constructed and topological analysis was performed carried on. And then GO and KEGG enrichment analysis was performed using R software. As a result, we screened 48 active ingredients and 201 targets of the Maziren pill, including 46 active ingredients and 97 targets of the Maziren pill in the treatment of senile functional constipation. Proteins in the PPI protein interaction core network were MMP2, MAPK1, AKT1, VEGFA, MAPK8, MMP9, EGFR, MYC, TP53, BCL2L1, MAPK3, CASP3, EGF, PTGS2, JUN, MAPK14. GO and KEGG pathway enrichment analysis showed that they were mainly involved in the reaction to toxic substances, the reaction to oxygen level, the process of oxidative stress, the metabolic process of organic hydroxyl compounds, the reaction of cells to nitrogen compounds, and the circadian process. The MAPK signaling pathway, AGE-RAGE signaling pathway, p53 signaling pathway, NF-kB signaling pathway, JAK-STAT signaling pathway, and bile release pathway were regulated in the complications of diabetes. This study preliminarily identified the main pathways and targets related to Maziren Pill in the treatment of senile functional constipation, and laid the foundation for further exploration of its pharmacological effects in the future.",
        "link": "https://dl.acm.org/doi/10.1145/3500931.3500984",
        "category": "Databases"
    },
    {
        "title": "Low-Cost Hiding of the Query Pattern",
        "authors": "['Maryam Sepehri', 'Florian Kerschbaum']",
        "date": "May 2021",
        "source": "ASIA CCS '21: Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
        "abstract": "Several attacks have shown that the leakage from the access pattern in searchable encryption is dangerous. Attacks exploiting leakage from the query pattern are as dangerous, but less explored. While there are known, lightweight countermeasures to hide information in the access pattern by padding the ciphertexts, the same is not true for the query pattern. Oblivious RAM hides the query patterns, but requires a logarithmic overhead in the size of the database and hence will become even slower as data grows. In this paper we present a query smoothing algorithm to hide the frequency information in the query pattern of searchable encryption schemes by introducing fake queries. Our method only introduces a constant overhead of 7 to 13 fake queries per real query in our experiments. Furthermore, we show that our query smoothing algorithm can also be applied to range-searchable encryption schemes and then prevents all recent plaintext recovery attacks.",
        "link": "https://dl.acm.org/doi/10.1145/3433210.3453103",
        "category": "Databases"
    },
    {
        "title": "The Aurora Single Level Store Operating System",
        "authors": "['Emil Tsalapatis', 'Ryan Hancock', 'Tavian Barnes', 'Ali José Mashtizadeh']",
        "date": "October 2021",
        "source": "SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles",
        "abstract": "Applications on modern operating systems manage their ephemeral state in memory and persistent state on disk. Ensuring consistency between them is a source of significant developer effort and application bugs. We present the Aurora single level store, an OS that eliminates the distinction between ephemeral and persistent application state. Aurora continuously persists entire applications with millisecond granularity to provide persistence as an OS service. Aurora revists the problem of application checkpointing through the lens of a single level store. Aurora supports transparent and customized applications. The RocksDB database using Aurora's APIs achieved a 75% throughput improvement while removing 40% of its code.",
        "link": "https://dl.acm.org/doi/10.1145/3477132.3483563",
        "category": "Databases"
    },
    {
        "title": "SymbolFinder: Brainstorming Diverse Symbols Using Local Semantic Networks",
        "authors": "['Savvas Petridis', 'Hijung Valentina Shin', 'Lydia B Chilton']",
        "date": "October 2021",
        "source": "UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology",
        "abstract": "Visual symbols are the building blocks for visual communication. They convey abstract concepts like reform and participation quickly and effectively. When creating graphics with symbols, novice designers often struggle to brainstorm multiple, diverse symbols because they fixate on a few associations instead of broadly exploring different aspects of the concept. We present SymbolFinder, an interactive tool for finding visual symbols for abstract concepts. SymbolFinder molds symbol-finding into a recognition rather than recall task by introducing the user to diverse clusters of words associated with the concept. Users can dive into these clusters to find related, concrete objects that symbolize the concept. We evaluate SymbolFinder with two studies: a comparative user study, demonstrating that SymbolFinder helps novices find more unique symbols for abstract concepts with significantly less effort than a popular image database and a case study demonstrating how SymbolFinder helped design students create visual metaphors for three cover illustrations of news articles.",
        "link": "https://dl.acm.org/doi/10.1145/3472749.3474757",
        "category": "Databases"
    },
    {
        "title": "A comparative study of vulnerability reporting by software composition analysis tools",
        "authors": "['Nasif Imtiaz', 'Seaver Thorn', 'Laurie Williams']",
        "date": "October 2021",
        "source": "ESEM '21: Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
        "abstract": "Background: Modern software uses many third-party libraries and frameworks as dependencies. Known vulnerabilities in these dependencies are a potential security risk. Software composition analysis (SCA) tools, therefore, are being increasingly adopted by practitioners to keep track of vulnerable dependencies. Aim: The goal of this study is to understand the difference in vulnerability reporting by various SCA tools. Understanding if and how existing SCA tools differ in their analysis may help security practitioners to choose the right tooling and identify future research needs. Method: We present an in-depth case study by comparing the analysis reports of 9 industry-leading SCA tools on a large web application, OpenMRS, composed of Maven (Java) and npm (JavaScript) projects. Results: We find that the tools vary in their vulnerability reporting. The count of reported vulnerable dependencies ranges from 17 to 332 for Maven and from 32 to 239 for npm projects across the studied tools. Similarly, the count of unique known vulnerabilities reported by the tools ranges from 36 to 313 for Maven and from 45 to 234 for npm projects. Our manual analysis of the tools' results suggest that accuracy of the vulnerability database is a key differentiator for SCA tools. Conclusion: We recommend that practitioners should not rely on any single tool at the present, as that can result in missing known vulnerabilities. We point out two research directions in the SCA space: i) establishing frameworks and metrics to identify false positives for dependency vulnerabilities; and ii) building automation technologies for continuous monitoring of vulnerability data from open source package ecosystems.",
        "link": "https://dl.acm.org/doi/10.1145/3475716.3475769",
        "category": "Databases"
    },
    {
        "title": "The Establishment of Machine Translation Bilingual Corpus Based on Artificial Intelligence and Big Data Technology",
        "authors": "['Hanhui Li']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "With the rapid development of artificial intelligence and big data technology, all areas of society have gradually begun to use big data technology. Machine translation is a very popular and challenging research content in the field of natural language processing. From the idea of machine translation to the integration of syntactic information into machine translation, scholars at home and abroad have designed many formal models and algorithms for machine translation. They have made positive contributions to the research of machine translation in natural language processing and have made them increasingly mature. This article uses artificial intelligence big data technology to establish a machine translation bilingual prediction database. The content of the experiment is to conduct translation research on Chinese and English bilinguals through the machine translation bilingual prediction database. The main test is to test the accuracy of the number of sentences under different retrieval structures. Perform experimental tests on the recall rate, and analyze the data based on the test, and draw relevant conclusions.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3483147",
        "category": "Databases"
    },
    {
        "title": "Performance of ESI Dominant Disciplines – a Case Study of 21 Scientific Research Institutions in Yunnan Province",
        "authors": "['Guoqing Li', 'Baiqiu Zhang']",
        "date": "May 2021",
        "source": "CIPAE 2021: 2021 2nd International Conference on Computers, Information Processing and Advanced Education",
        "abstract": "In the big data environment, in the face of the weak competitiveness of disciplines in Southwest China, this study uses ESI and incites database, combined with bibliometric analysis method to conduct multi-dimensional benchmarking analysis on the comprehensive performance of ESI dominant disciplines of 21 scientific research institutions in Yunnan Province, so as to strengthen the interdisciplinary cooperation of various scientific research institutions in Yunnan Province, and improve the global average level of comprehensive cited performance To promote the coordinated development among disciplines of various scientific research institutions in Yunnan Province, enhance the international competitiveness of advantageous disciplines, and promote the development potential of potential disciplines and weak disciplines.",
        "link": "https://dl.acm.org/doi/10.1145/3456887.3456944",
        "category": "Databases"
    },
    {
        "title": "Real-Time Face Detection and Recognition on Raspberry Pi using LBP and Deep Learning",
        "authors": "['Venkata Kranthi B', 'Surekha Borra']",
        "date": "August 2021",
        "source": "DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence",
        "abstract": "Advancement in low power embedded systems in recent years help us in building portable real-time algorithms and applications. These embedded systems can be integrated into the Cameras used in Surveillance Systems to provide additional mid-level computer vision tasks like face detection and face recognition in person identification applications. This paper proposes face detection using Local Binary Patterns (LBP) and Haar cascades-based face recognition using Convolutional Neural Networks (CNN) derived from Lenet architecture. A database is created covering all challenges involved in face identification like illumination, orientation, expressions, disguise, and age factors. Two CNN architectures are proposed and compared for face recognition. The tasks are performed on Raspberry Pi in real-time, and analysis has been carried out on how well LBP and Haar cascades work in terms of accuracy and Frames per second (FPS) in real-time. The realtime results achieved are acceptable with the frame rate of 7.04 FPS with accuracy above 94%. Frame reading and frame processing are handled in separate threads on CPU and frame skipping, while detection and recognition have improved the frame rates significantly.",
        "link": "https://dl.acm.org/doi/10.1145/3484824.3484903",
        "category": "Databases"
    },
    {
        "title": "Research and Progress on The Application of Blockchain Technology in Agricultural Product Traceability Systems",
        "authors": "['Chen Jie', 'Zeng Guixiang', 'Wu Junhui', 'Wu Yusheng', 'Lin Kaiyan', 'Si Huiping']",
        "date": "May 2021",
        "source": "ICBDC '21: Proceedings of the 6th International Conference on Big Data and Computing",
        "abstract": "In recent years, with the occurrence of a series of major food safety accidents, people's demand for food has changed from \"quantity\" to \"quality\". Although a batch of agricultural product traceability system has also been born, but this traditional centralized traceability model cannot ensure the reliability of data. Unfortunately, reduce the credibility of tracing and meet the needs of consumers difficultly. Thus, the society needs a better traceability system to guarantee the entire process of agricultural traceability transparent and credible urgently. As a hot research direction of current network technology, blockchain technology is essentially a decentralized database with characteristics of tamper-proof, openness, transparency and traceability, which brings new ideas to the design of agricultural traceability system. Therefore, this paper aims to discuss the application development of blockchain in agricultural product traceability and the improvement of performance. Finally, it provides new ideas for building agricultural product traceability system.",
        "link": "https://dl.acm.org/doi/10.1145/3469968.3470002",
        "category": "Databases"
    },
    {
        "title": "Efficient Exact Computation of Setwise Minimax Regret for Interactive Preference Elicitation",
        "authors": "['Federico Toffano', 'Paolo Viappiani', 'Nic Wilson']",
        "date": "May 2021",
        "source": "AAMAS '21: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems",
        "abstract": "A key issue in artificial intelligence methods for interactive preference elicitation is choosing at each stage an appropriate query to the user, in order to find a near-optimal solution as quickly as possible. A theoretically attractive method is to choose a query that minimises max setwise regret (which corresponds to the worst case loss response in terms of value of information). We focus here on the situation in which the choices are represented explicitly in a database, and with a model of user utility as a weighted sum of the criteria; in this case when the user makes a choice, an agent learns a linear constraint on the unknown vector of weights. We develop an algorithmic method for computing minimax setwise regret for this form of preference model, by making use of a SAT solver with cardinality constraints to prune the search space, and computing max setwise regret using an extreme points method. Our experimental results demonstrate the feasibility of the approach and the very substantial speed up over the state of the art.",
        "link": "https://dl.acm.org/doi/10.5555/3463952.3464105",
        "category": "Databases"
    },
    {
        "title": "Differential Privacy Image Publishing based on NMF and SVD",
        "authors": "['Hangui Wei', 'Lina Ge', 'Zhenkun Lu', 'Guifen Zhang', 'Donghong Qin']",
        "date": "May 2021",
        "source": "ICBBT '21: Proceedings of the 2021 13th International Conference on Bioinformatics and Biomedical Technology",
        "abstract": "Personal privacy has become one of the hot topics in the era of big data. Differential privacy protection technology has strong protection ability and relies on rigorous statistical model, which makes it widely studied and applied. For privacy protection in gray face image publishing, SRA and ESRA methods have been proposed, but they are not robust enough and prone to damage the image. Therefore, this paper proposes face image publishing algorithms NSRA and NESRA based on the combination of NMF and SVD matrix decomposition under heuristic and exponential mechanism. Based on the experimental comparison of single image data availability and real face database recognition accuracy, both NSRA and NESRA algorithms are better than SRA and ESRA algorithms, and the image stability is stronger. Compared with SRA method and ESRA method, this algorithm has better usability and robustness, and can be more effectively used in social platforms and medical systems.",
        "link": "https://dl.acm.org/doi/10.1145/3473258.3473266",
        "category": "Databases"
    },
    {
        "title": "Identifying Gene Signatures for Cancer Drug Repositioning Based on Sample Clustering",
        "authors": "['Fei Wang', 'Yulian Ding', 'Xiujuan Lei', 'Bo Liao', 'Fang-Xiang Wu']",
        "date": "None",
        "source": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "abstract": "Drug repositioning is an important approach for drug discovery. Computational drug repositioning approaches typically use a gene signature to represent a particular disease and connect the gene signature with drug perturbation profiles. Although disease samples, especially from cancer, may be heterogeneous, most existing methods consider them as a homogeneous set to identify differentially expressed genes (DEGs)for further determining a gene signature. As a result, some genes that should be in a gene signature may be averaged off. In this study, we propose a new framework to identify gene signatures for cancer drug repositioning based on sample clustering (GS4CDRSC). GS4CDRSC first groups samples into several clusters based on their gene expression profiles. Second, an existing method is applied to the samples in each cluster for generating a list of DEGs. Then a weighting approach is used to identify an intergrated gene signature from all the lists of DEGs. The integrated gene signature is used to connect with drug perturbation profiles in the Connectivity Map (CMap)database to generate a list of drug candidates. GS4CDRSC has been tested with several cancer datasets and existing methods. The computational results show that GS4CDRSC outperforms those methods without the sample clustering and weighting approaches in terms of both number and rate of predicted known drugs for specific cancers.",
        "link": "https://dl.acm.org/doi/10.1109/TCBB.2020.3019781",
        "category": "Databases"
    },
    {
        "title": "Machine learning in SQL by translation to TensorFlow",
        "authors": "['Nantia Makrynioti', 'Ruy Ley-Wild', 'Vasilis Vassalos']",
        "date": "June 2021",
        "source": "DEEM '21: Proceedings of the Fifth Workshop on Data Management for End-To-End Machine Learning",
        "abstract": "We present sql4ml, a framework for expressing machine learning (ML) algorithms in a relational database management system (RDBMS). The user writes the objective function of an ML model as a SQL query, then sql4ml translates the query into an equivalent TensorFlow (TF) graph, which can be automatically differentiated and optimized to learn the model weights. Sql4ml makes the database a unified programming environment for feature engineering, learning/inference, and evaluating models. The proposed approach is more expressive than using ready-made ML algorithms, but abstracts away the details of the training process. We present the architecture of sql4ml and describe the method for translating an objective function in SQL to a TensorFlow representation. We show how recent ideas from Factorized ML [7] can be leveraged to efficiently move data between a database and an ML framework. Finally, we present experimental results regarding both the proposed translation and the optimization techniques for data transfer. Our results show that translation time is negligible compared to time for data processing, and that the optimization techniques achieve up to 50% improvement in the export runtime and up to 85% decrease in the size of the exported data.",
        "link": "https://dl.acm.org/doi/10.1145/3462462.3468879",
        "category": "Databases"
    },
    {
        "title": "Industry practice of coverage-guided enterprise-level DBMS fuzzing",
        "authors": "['Mingzhe Wang', 'Zhiyong Wu', 'Xinyi Xu', 'Jie Liang', 'Chijin Zhou', 'Huafeng Zhang', 'Yu Jiang']",
        "date": "May 2021",
        "source": "ICSE-SEIP '21: Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice",
        "abstract": "As an infrastructure for data persistence and analysis, Database Management Systems (DBMSs) are the cornerstones of modern enterprise software. To improve their correctness, the industry has been applying blackbox fuzzing for decades. Recently, the research community achieved impressive fuzzing gains using coverage guidance. However, due to the complexity and distributed nature of enterprise-level DBMSs, seldom are these researches applied to the industry. In this paper, we apply coverage-guided fuzzing to enterpriselevel DBMSs from Huawei and Bloomberg LP. In our practice of testing GaussDB and Comdb2, we found major challenges in all three testing stages. The challenges are collecting precise coverage, optimizing fuzzing performance, and analyzing root causes. In search of a general method to overcome these challenges, we propose Ratel, a coverage-guided fuzzer for enterprise-level DBMSs. With its industry-oriented design, Ratel improves the feedback precision, enhances the robustness of input generation, and performs an on-line investigation on the root cause of bugs. As a result, Ratel outperformed other fuzzers in terms of coverage and bugs. Compared to industrial black box fuzzers SQLsmith and SQLancer, as well as coverage-guided academic fuzzer Squirrel, Ratel covered 38.38%, 106.14%, 583.05% more basic blocks than the best results of other three fuzzers in GaussDB, PostgreSQL, and Comdb2, respectively. More importantly, Ratel has discovered 32, 42, and 5 unknown bugs in GaussDB, Comdb2, and PostgreSQL.",
        "link": "https://dl.acm.org/doi/10.1109/ICSE-SEIP52600.2021.00042",
        "category": "Databases"
    },
    {
        "title": "PriSeT: efficient de novo primer discovery",
        "authors": "['Marie Hoffmann', 'Michael T. Monaghan', 'Knut Reinert']",
        "date": "August 2021",
        "source": "BCB '21: Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
        "abstract": "Motivation: DNA metabarcoding is commonly used to infer the species composition of environmental samples, whereby a short, homologous DNA sequence is amplified and sequenced from all members of the community. Samples can comprise hundreds of organisms that can be closely or very distantly related. DNA metabarcoding combines polymerase chain reaction (PCR) and next-generation sequencing (NGS), and sequences are taxonomically identified based on their match to a reference database. Ideally, each species of interest would have a unique DNA barcode. This short, variable sequence needs to be flanked by conserved regions that can be used as primer-binding sites. PCR primer pairs would amplify a variable barcode in a broad evolutionary range of taxa. To date, no tools exist that computationally search and analyze the effectiveness of new primer pairs for large unaligned sequence data sets. More specifically we solve the following problem: Given a set of reference sequences R = {R1, R2, ..., Rm}, find a primer set P that allows for a high taxonomic coverage. This goal can be achieved by filtering for frequent primers and ranking by coverage or variation, i.e. the number of unique barcodes for further analysis. Here we present the software PriSeT, an offline primer-discovery tool that is capable of processing large libraries and is robust against mislabeled or low-quality references. It avoids the construction of a multisequence alignment of R. Instead, PriSeT uses encodings of frequent k-mers that allow bit-parallel processing and other optimizations. Results: We first evaluated PriSeT on references (mostly 18S rRNA genes) from 19 clades covering eukaryotic organisms that are typical for freshwater plankton samples. PriSeT recovered several published primer sets as well as additional, more chemically suitable primer sets. For these new sets, we compared frequency, taxonomic coverage, and amplicon variation with published primer sets. For 11 clades we found de novo primer pairs that cover more taxa than the published ones, and for six clades de novo primers resulted in greater sequence (i.e., DNA barcode) variation. We also applied PriSeT to SARS-CoV-2 genomes and computed 114 new primer pairs with the additional constraint that the sequences have no co-occurrences in closely related taxa. These primer sets would be suitable for empirical testing. Availability: https://github.com/mariehoffmann/PriSeT Contact: [email protected]",
        "link": "https://dl.acm.org/doi/10.1145/3459930.3469546",
        "category": "Databases"
    },
    {
        "title": "An ICT governance analysis for the digital and smart transformation of Brazilian municipalities",
        "authors": "['Luiz Claudio Diogo Reis', 'Flavia Cristina Bernardini', 'Simone Bacellar Leal Ferreira', 'Claudia Cappelli']",
        "date": "June 2021",
        "source": "DG.O'21: DG.O2021: The 22nd Annual International Conference on Digital Government Research",
        "abstract": "Brazilian municipalities face challenges to provide efficient public services to their local community. One reason is that cities in the country are different in geography, development, economics, education, and technology environments. In the context of digital transformation, cities are promoting innovative trends by adopting smart cities initiatives, in which Information and Communication Technology (ICT) represents a transversal enabler. Studies and frameworks were applied worldwide to characterize cities' smartness, but few focused on ICT governance, especially in subnational governments. From this perspective, this research, based on the fundamentals of digital government, ICT governance, and smart cities, analyzed ICT governance requisites and practices in fifteen Brazilian municipalities best ranked in the Smart City Connected index. The methodology procedures encompassed cities' official websites and transparency portals searches and information requests based on Brazilian Access Information Law. The results evidenced that most Brazilian cities lack a smart city program approach, several cities did not implement ICT governance mechanisms and were not compliant with the Information Access Law, and ICT governance was not conceived as active transparency in some cities portals. Future studies should focus on developing an ICT governance catalog practices for Brazilian's smart cities and a methodology for ICT governance open database as a smart city approach.",
        "link": "https://dl.acm.org/doi/10.1145/3463677.3463729",
        "category": "Databases"
    },
    {
        "title": "Refactoring Java Monoliths into Executable Microservice-Based Applications",
        "authors": "['Francisco Freitas', 'André Ferreira', 'Jácome Cunha']",
        "date": "September 2021",
        "source": "SBLP '21: Proceedings of the 25th Brazilian Symposium on Programming Languages",
        "abstract": "In the last few years we have been seeing a drastic change in the way software is developed. Large-scale software projects are being assembled by a flexible composition of many (small) components possibly written in different programming languages and deployed anywhere in the cloud – the so-called microservice-based applications.  The dramatic growth in popularity of microservice-based applications has pushed several companies to apply major refactorings to their software systems. However, this is a challenging task that may take several months or even years.  We propose a methodology to automatically evolve a Java monolithic application into a microservice-based one. Our methodology receives the Java code and a proposition of microservices and refactors the original classes to make each microservice independent. Our methodology creates an API for each method call to classes that are in other services. The database entities are also refactored to be included in the corresponding service. The initial evaluation shows that our tool can successfully refactor 80% of the applications tested.",
        "link": "https://dl.acm.org/doi/10.1145/3475061.3475086",
        "category": "Databases"
    },
    {
        "title": "Attribute selection based on genetic and classification algorithms in the prediction of hospitalization need of COVID-19 patients",
        "authors": "['Miriam Pizzatto Colpo', 'Bruno Cascaes Alves', 'Kevin Soares Pereira', 'Anna Flávia Zimmermann Brandão', 'Marilton Sanchotene de Aguiar', 'Tiago Thompsen Primo']",
        "date": "June 2021",
        "source": "SBSI '21: Proceedings of the XVII Brazilian Symposium on Information Systems",
        "abstract": "The COVID-19 pandemic has been pressuring the whole society and overloading hospital systems. Machine learning models designed to predict hospitalizations, for example, can contribute to better targeting hospital resources. However, as the excess of information, often irrelevant or redundant, can impair the performance of predictive models, we propose in this work a hybrid approach to attribute selection. This method aims to find an optimal attribute subset through a genetic algorithm, which considers the results of a classification model in its evaluation function to improve the hospitalization need prediction of COVID-19 patients. We evaluated this approach in a database of more than 200 thousand COVID-19 patients from the State Health Secretariat of Rio Grande do Sul. We provided an increase of 18% in the classification precision for patients with hospitalization necessities. In a real-time application, this would also mean greater precision in targeting resources, as well as, consequently and mainly, improved service to the infected population.",
        "link": "https://dl.acm.org/doi/10.1145/3466933.3466935",
        "category": "Databases"
    },
    {
        "title": "Building Tapis v3 Streams API Support for Real-Time Streaming Data Event-Driven Workflows",
        "authors": "['Smruti Padhy', 'Anagha Jamthe', 'Sean B. Cleveland', 'Jack A. Smith', 'Joe Stubbs', 'Christian Garcia', 'Michael Packard', 'Steve Terry', 'Julia Looney', 'Richard Cardone', 'Maytal Dahan', 'Gwen A. Jacobs']",
        "date": "July 2021",
        "source": "PEARC '21: Practice and Experience in Advanced Research Computing",
        "abstract": "The Tapis framework, an NSF-funded project, is an open-source, scalable API platform that enables researchers to perform distributed computational experiments securely and achieve faster scientific results with increased reproducibility. Tapis Streams API focuses on supporting scientific use cases that require working with real-time sensor data. The Streams Service, built on the top of the CHORDS time-series data service, allows storing, processing, annotating, querying, and archiving time-series data. This paper focuses on the new Tapis Streams API functionality that enables researchers to design and execute real-time data-driven event workflow for their research. We describe the architecture and design choices towards achieving this new capability with Streams API. Specifically, we demonstrate the integration of Streams API with Kapacitor, a native data processing engine for time-series database InfluxDB, and Abaco, an NSF Funded project, web service, and distributed computing platform providing function-as-a-Service (FaaS). The Streams API, which includes a wrapper interface for the Kapacitor alerting system, can define and enable alerts. Finally, simulation results from the water-quality use case depict that Streams API’s new capabilities can support real-time streaming data event-driven workflows.",
        "link": "https://dl.acm.org/doi/10.1145/3437359.3465567",
        "category": "Databases"
    },
    {
        "title": "Research on the Application of Computer Statistics Technology in the Educational Information Management System of Colleges and Universities",
        "authors": "['Haifeng Li']",
        "date": "May 2021",
        "source": "CIPAE 2021: 2021 2nd International Conference on Computers, Information Processing and Advanced Education",
        "abstract": "Aiming at the construction and application of university educational administration management system, a lightweight technical architecture solution based on Struts + Spring + Hibernate is proposed. Using MVC architecture, the whole system is divided into presentation layer, business processing layer, data persistence layer and database layer, which realizes the separation of user interface, business processing and data access. At the same time, in order to further improve the management efficiency of colleges and universities, the article designs an educational administration management system with higher execution efficiency. By analysing the traditional Apriori computer statistical technology, it points out the defects of the data mining algorithm. In response to the above shortcomings, the use of hashing, transaction compression, partitioning and sampling techniques to improve the Apriori computer statistical technology to improve the efficiency and reliability of the algorithm, thereby effectively improving the management efficiency of the educational management system.",
        "link": "https://dl.acm.org/doi/10.1145/3456887.3457518",
        "category": "Databases"
    },
    {
        "title": "Design and Implementation of Intelligent Course Arrangement System in Colleges Based on JSP Technology: Design Intelligent Course Arrangement System",
        "authors": "['Yuyang Jing']",
        "date": "October 2021",
        "source": "CSSE '21: Proceedings of the 4th International Conference on Computer Science and Software Engineering",
        "abstract": "This college's intelligent lesson scheduling system is based on a B/S web program, JSP technology, and the SqlServer2008 database, and finally achieve the following functions: After logging in to the system homepage, the system can enter the personal password modification interface to modify the login password; essential teaching the resource entry part includes functional modules such as the entry of courses, majors, classes, students, teachers, and teaching venues, and can also be added or deleted according to the actual situation; in the timetable generation and management part, there are mainly timetable generation, printing, and export. The functional module also includes the corresponding functions such as course adjustment and course selection.",
        "link": "https://dl.acm.org/doi/10.1145/3494885.3494938",
        "category": "Databases"
    },
    {
        "title": "DBOS: a DBMS-oriented operating system",
        "authors": "['Athinagoras Skiadopoulos', 'Qian Li', 'Peter Kraft', 'Kostis Kaffes', 'Daniel Hong', 'Shana Mathew', 'David Bestor', 'Michael Cafarella', 'Vijay Gadepally', 'Goetz Graefe', 'Jeremy Kepner', 'Christos Kozyrakis', 'Tim Kraska', 'Michael Stonebraker', 'Lalith Suresh', 'Matei Zaharia']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "This paper lays out the rationale for building a completely new operating system (OS) stack. Rather than build on a single node OS together with separate cluster schedulers, distributed filesystems, and network managers, we argue that a distributed transactional DBMS should be the basis for a scalable cluster OS. We show herein that such a database OS (DBOS) can do scheduling, file management, and inter-process communication with competitive performance to existing systems. In addition, significantly better analytics can be provided as well as a dramatic reduction in code complexity through implementing OS services as standard database queries, while implementing low-latency transactions and high availability only once.",
        "link": "https://dl.acm.org/doi/10.14778/3485450.3485454",
        "category": "Databases"
    },
    {
        "title": "What Becomes a Senior Researcher",
        "authors": "['H. V. Jagadish']",
        "date": "September 2021",
        "source": "ACM SIGMOD Record",
        "abstract": "As we progress through life, our needs, desires, and even objectives, change. That certainly has been the case for me over the years. This article is my attempt at distilling what advice I can for someone making a transition from a junior to a senior researcher in the database field: think Associate Professor or someone 5 to 10 years post PhD.",
        "link": "https://dl.acm.org/doi/10.1145/3503780.3503784",
        "category": "Databases"
    },
    {
        "title": "A bunched logic for conditional independence",
        "authors": "['Jialu Bao', 'Simon Docherty', 'Justin Hsu', 'Alexandra Silva']",
        "date": "June 2021",
        "source": "LICS '21: Proceedings of the 36th Annual ACM/IEEE Symposium on Logic in Computer Science",
        "abstract": "Independence and conditional independence are fundamental concepts for reasoning about groups of random variables in probabilistic programs. Verification methods for independence are still nascent, and existing methods cannot handle conditional independence. We extend the logic of bunched implications (BI) with a non-commutative conjunction and provide a model based on Markov kernels; conditional independence can be directly captured as a logical formula in this model. Noting that Markov kernels are Kleisli arrows for the distribution monad, we then introduce a second model based on the powerset monad and show how it can capture join dependency, a non-probabilistic analogue of conditional independence from database theory. Finally, we develop a program logic for verifying conditional independence in probabilistic programs.",
        "link": "https://dl.acm.org/doi/10.1109/LICS52264.2021.9470712",
        "category": "Databases"
    },
    {
        "title": "An Effective Image Enhancement Method for Color Fundus Images",
        "authors": "['Zhan Wang', 'SongPo Tian', 'Xin Fu', 'JiaZhen He']",
        "date": "December 2021",
        "source": "ACAI '21: Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence",
        "abstract": "A method is proposed for adaptive fundus image enhancement so as to restore the color images that are with extremely low or nonuniform brightness. Our proposed method is capable of increasing the brightness, contrast of images without losing original color hue. It includes three steps: luminance enhancement, contrast improvement, and color restoration. All procedures are adapted and modified from conventional image processing algorithms, which are proved simple and less time-consuming. Two public and commonly used fundus image database were partially employed to evaluate the performance of our method. It is concluded that the brightness of images was enhanced by 86.35 % and the contrast was improved by 132.37 % without color distortion.",
        "link": "https://dl.acm.org/doi/10.1145/3508546.3508589",
        "category": "Databases"
    },
    {
        "title": "Towards instance-optimized data systems",
        "authors": "['Tim Kraska']",
        "date": "None",
        "source": "Proceedings of the VLDB Endowment",
        "abstract": "In recent years, we have seen increased interest in applying machine learning to system problems. For example, there has been work on applying machine learning to improve query optimization, indexing, storage layouts, scheduling, log-structured merge trees, sorting, compression, and sketches, among many other data management tasks. Arguably, the ideas behind these techniques are similar: machine learning is used to model the data and/or workload in order to derive a more efficient algorithm or data structure. Ultimately, these techniques will allow us to build \"instance-optimized\" systems: that is, systems that self-adjust to a given workload and data distribution to provide unprecedented performance without the need for tuning by an administrator. While many of these techniques promise orders-of-magnitude better performance in lab settings, there is still general skepticism about how practical the current techniques really are.The following is intended as a progress report on ML for Systems and its readiness for real-world deployments, with a focus on our projects done as part of the Data Systems and AI Lab (DSAIL) at MIT By no means is it a comprehensive overview of all existing work, which has been steadily growing over the past several years not only in the database community but also in the systems, networking, theory, PL, and many other adjacent communities.",
        "link": "https://dl.acm.org/doi/10.14778/3476311.3476392",
        "category": "Databases"
    },
    {
        "title": "A Query Language for Workflow Logs",
        "authors": "['Yan Tang', 'Weilong Cui', 'Jianwen Su']",
        "date": "None",
        "source": "ACM Transactions on Management Information Systems",
        "abstract": "A business process (workflow) is an assembly of tasks to accomplish a business goal. Real-world workflow models often demanded to change due to new laws and policies, changes in the environment, and so on. To understand the inner workings of a business process to facilitate changes, workflow logs have the potential to enable inspecting, monitoring, diagnosing, analyzing, and improving the design of a complex workflow. Querying workflow logs, however, is still mostly an ad hoc practice by workflow managers. In this article, we focus on the problem of querying workflow log concerning both control flow and dataflow properties. We develop a query language based on “incident patterns” to allow the user to directly query workflow logs instead of having to transform such queries into database operations. We provide the formal semantics and a query evaluation algorithm of our language. By deriving an accurate cost model, we develop an optimization mechanism to accelerate query evaluation. Our experiment results demonstrate the effectiveness of the optimization and achieves up to 50× speedup over an adaption of existing evaluation method.",
        "link": "https://dl.acm.org/doi/10.1145/3482968",
        "category": "Databases"
    },
    {
        "title": "DAIR: A Query-Efficient Decision-based Attack on Image Retrieval Systems",
        "authors": "['Mingyang Chen', 'Junda Lu', 'Yi Wang', 'Jianbin Qin', 'Wei Wang']",
        "date": "July 2021",
        "source": "SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "abstract": "There is an increasing interest in studying adversarial attacks on image retrieval systems. However, most of the existing attack methods are based on the white-box setting, where the attackers have access to all the model and database details, which is a strong assumption for practical attacks. The generic transfer-based attack also requires substantial resources yet the effect was shown to be unreliable. In this paper, we make the first attempt in proposing a query-efficient decision-based attack framework for the image retrieval (DAIR) to completely subvert the top-K retrieval results with human imperceptible perturbations. We propose an optimization-based method with a smoothed utility function to overcome the challenging discrete nature of the problem. To further improve the query efficiency, we propose a novel sampling method that can achieve the transferability between the surrogate and the target model efficiently. Our comprehensive experimental evaluation on the benchmark datasets shows that our DAIR method outperforms significantly the state-of-the-art decision-based methods. We also demonstrate that real image retrieval engines (Bing Visual Search and Face++ engines) can be attacked successfully with only several hundreds of queries.",
        "link": "https://dl.acm.org/doi/10.1145/3404835.3462887",
        "category": "Databases"
    },
    {
        "title": "Application of Big Data Technology in the Innovation of University Education Management Work",
        "authors": "['Wenjun Yu']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "With the development of higher education in China, the scale of colleges and universities is expanding rapidly and the information of teaching management is developing rapidly. Facing the growing management information, it is inevitable to carry out education management innovation to meet the needs of the times. The use of teachers' resources and the assessment of teachers' classroom teaching quality must be solved by modern means to achieve the rational use and optimal allocation of teaching resources and improve the effectiveness of teaching management. The purpose of this paper is to study the application of big data(BD) technology in the innovation of university education management. This paper uses case study and literature research methods, combines data warehousing and data mining technologies in BD technology for evaluating teaching data with data warehousing as an organizational tool, and analyzes the current situation and future development trends of database technology data application in information management of educational administration construction in universities. The experimental results show that the ID3 algorithm performs better when there are fewer number of things and is about 10 seconds faster than the Apriori algorithm. But when the number of things are much more, the Apriori algorithm is significantly better than the ID3 algorithm. Moreover, the Apriori algorithm is more suitable for mining the deep laws of ungrasped objective things, and reveals unknown dependencies between data for rule mining with fuzzy concepts. Therefore, through analysis of data warehouse, data mining techniques and mining algorithms, by data mining of a large amount of assessment data, this paper finds that combining decision trees and association rule algorithms in data mining algorithms with teaching systems, data mining techniques introduced in educational assessment systems for assessment, not only improves the scientific nature of educational management, but also improves the effectiveness of digital education construction.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3483067",
        "category": "Databases"
    },
    {
        "title": "RNA-Seq Analysis Reveals DPYSL2 and PNKD Promotes Cardiomyocytes Differentiation during Reprogramming",
        "authors": "['Yaofei Wu']",
        "date": "December 2021",
        "source": "ICCBB '21: Proceedings of the 2021 5th International Conference on Computational Biology and Bioinformatics",
        "abstract": "Cardiomyocytes are terminally differentiated cells in the heart, whose injuries leads to severe health issue including cardiac failure and even death. Traditional medical procedure failed to rescue damaged cardiomyocytes and hence cannot cure severe myocardial damage. This is probably caused by the irreplaceability of limited cardiomyocytes number in our heart. Fortunately, recent studies reveal induced pluripotent stem cells (iPSCs) can be trained to differentiating into cardiomyocytes, opening the possibility of replacing damaged cardiomyocytes by reprogrammed iPSCs, though many remains unknow during cardiomyocytes reprogramming. In this project, we analyzed multiple RNA-Seq data during cardiomyocytes differentiation and reprogramming. We found potential candidate genes DPYSL2 and PNKD as well as several other genes that might play an important role in regulating cardiomyocytes reprogramming. We also proposed the possible mechanisms for these candidate genes based on their genetic nature as well as multi-omic data from public database. Hopefully, these studies could provide new insights on how cardiomyocytes differentiation were regulated during reprogramming and hence improving current iPSCs therapy for myocardial damage.",
        "link": "https://dl.acm.org/doi/10.1145/3512452.3512458",
        "category": "Databases"
    },
    {
        "title": "Design and Research of Web Crawler Based on Distributed Architecture",
        "authors": "['Lili Wang', 'Haoliang Wang']",
        "date": "October 2021",
        "source": "AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
        "abstract": "Internet data is abundance in content and diverse in organization. In order to automatically complete the process of collecting, analyzing, and storing large amounts of data and information on the web, a web crawler technology based on Hadoop distributed clusters is proposed. Using Nutch crawler framework, Hadoop distributed technology, and Zookeeper distributed coordination service framework, through the construction of distributed clusters, and high-performance Key-Value database Redis to store data, it verifies the feasibility of distributed crawlers. Through the analysis of data collection experiments, the comparison of multiple sets of experimental data between the distributed crawler based on Nutch and the traditional crawler shows that the crawler design of the distributed architecture is superior to the traditional crawler in terms of collection speed and efficiency.",
        "link": "https://dl.acm.org/doi/10.1145/3495018.3495061",
        "category": "Databases"
    },
    {
        "title": "Design of English Teaching Interactive System Based on Artificial Intelligence",
        "authors": "['Huimin Zhao']",
        "date": "February 2022",
        "source": "ICBDE '22: Proceedings of the 5th International Conference on Big Data and Education",
        "abstract": "By exploring the characteristic of interaction of network teaching, we proposed to develop an English teaching interaction system based on artificial intelligence, which can solve the problems in current English teaching class. We analyzed the functional requirements of the system corresponding to different user. By buiding Ubuntu operating system as the computing platform and exploring the object-oriented dynamic teaching environment provided by artificial intelligence platform, we build the LAMP development environment with MySQL database and PHP script language. We constructed a three-tier separated architecture system by using B/S mode, and refined the design and implementation of each module function through the secondary development of platform module plug-ins. To exhibit the advantages of the artificial intelligence system, we take college English teaching as an example to illustrate its functions. The functions of curriculum creation, teaching resources and activity design can be realized in the system, which verifies the effectiveness of the artificial intelligence system in interactive teaching and learning.",
        "link": "https://dl.acm.org/doi/10.1145/3524383.3533247",
        "category": "Databases"
    },
    {
        "title": "Towards event-driven decentralized marketplaces on the blockchain",
        "authors": "['Akash Pateria', 'Kemafor Anyanwu']",
        "date": "June 2021",
        "source": "DEBS '21: Proceedings of the 15th ACM International Conference on Distributed and Event-based Systems",
        "abstract": "Blockchains have become a popular technology for lowering the trust-tax burden between transacting parties that cannot necessarily trust each other. They are used as substitutes for the centralized authorities typically incorporated in transactional workflows to perform verification tasks and have the advantage of being objective and incorruptible. For applications such as supply chain marketplaces, auxilliary functionalities beyond the core blockchain roles of recording and validating transactions such as event detection are important for enabling application participants be responsive to business conditions. Unfortunately, existing blockchain event frameworks are immature, syntactic, inflexible and not expressive enough for many application needs. In this paper, we propose an approach that involves an event model which \"semantifies blockchain transactions\" and an implementation architecture that integrates a open-source blockchain database BigChainDB with a semantic engine and publish-subscribe messaging platform. Finally, we model and simulate a use-case inspired from the manufacturing domain and present usability and preliminary performance results that demonstrate the discriminatory ability of semantics-enabled event model.",
        "link": "https://dl.acm.org/doi/10.1145/3465480.3466921",
        "category": "Databases"
    },
    {
        "title": "Design and Implementation of Tax Collection and Management Index Early Warning System Based on Data Mining",
        "authors": "['Jie Mao']",
        "date": "December 2021",
        "source": "ICASIT 2021: 2021 International Conference on Aviation Safety and Information Technology",
        "abstract": "With the continuous improvement of tax information level of tax authorities, tax authorities have formed a large number of tax business management data in their daily business. However, at present, only a simple historical query is provided for these historical data, which is far from the expected advanced applications such as analysis and prediction and decision support. This paper designs and implements a set of early warning system of tax collection and management indicators by using database technology, constructs indicators according to taxpayers' declaration data, and uses data warehouse technology, attribute-oriented summary method and association rule algorithm for data mining to provide data support for tax early warning. Setting up the corresponding early warning value system can reduce the risk of tax evasion, improve the quality of tax monitoring, and then improve the efficiency of law enforcement and management level.",
        "link": "https://dl.acm.org/doi/10.1145/3510858.3511348",
        "category": "Databases"
    },
    {
        "title": "Research on University Ideological Education Platform Based on Data Mining Technology",
        "authors": "['Zhi Li']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "Cloud platform can not only provide massive teaching resources, but also retain a large amount of data about teachers and students during online learning. Data mining is a hot topic in the field of database application in recent years. Many achievements have been made in both theory and application, and the technology tends to be more mature. As a widely used new discipline, data mining technology has a good application prospect in university education informationization, which provides a new and scientific analysis way for the management, construction and service process of universities. K-means algorithm is used to track learners' learning behavior, analyze learners' learning patterns and cognitive level, classify learners, and provide automatic push of learning resources, so that online learning platform can teach learners in accordance with their aptitude and provide personalized services. According to the mining results, some suggestions are put forward for personalized design of online learning platform.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3482685",
        "category": "Databases"
    },
    {
        "title": "A DNN-based Real-time Speech Recognition Control Game",
        "authors": "['Yahao Wu', 'Jingzhi Ruan']",
        "date": "September 2021",
        "source": "ICIST '21: Proceedings of the 3rd International Conference on Intelligent Science and Technology",
        "abstract": "A real-time speech recognition game based on DNN was designed in this paper. English numbers 1 and 2 from the English database were firstly extracted for training through DNN, and K-folds and committee votes were used to optimize the system to obtain a mature training model for distinguishing speech input. Speech recording function belonging to Python was used to realize real-time speech recording. The 8000 feature elements in the obtained speech were converted into 784 feature elements through STFT, and then the obtained training model is introduced for speech prediction. In this paper, a game based on binary control was designed, which used speech commands to control the movement of game objects to avoid obstacles, and to reflect the recognition accuracy through scoring. The real-time control of the game by speech was by designing a list and a multi-threaded control system. Finally, some ideas were put forward for optimizing this game, and also more application ideas based on speech control.",
        "link": "https://dl.acm.org/doi/10.1145/3507959.3507967",
        "category": "Databases"
    },
    {
        "title": "Differential Expression Analysis Basing on Bladder Cancer on Dataset GSE40355/GPL13497",
        "authors": "['Yi Tang', 'Jingyan Diao', 'Degang Zhu']",
        "date": "July 2021",
        "source": "BIBE2021: The Fifth International Conference on Biological Information and Biomedical Engineering",
        "abstract": "Objective: This study aims to identify hub genes and key pathways associated with bladder cancer. Methods: bladder cancer related dataset GSE40355/GPL13497 was obtained from the GEO database, regarding algorithms were utilized to filter differentially expressed genes. GO analysis, KEGG analysis were run on the differentially expressed genes found. Furthermore, Protein-protein Interaction Network was built and hub genes were found based on the result. Results: 2,530 differentially expressed genes were found with the constrained conditions of |log 2(FC)| > 2.5, adj.P.value < 0.01. GO analysis indicates that the molecular functions of the differentially expressed genes found mostly enriched on cell adhesion molecule binding, actin binding, DNA-binding transcription activator activity (RNA polymerase II-specific), while the KEGG analysis showed that the signaling pathways mainly enriched on MAPK signaling pathway, cAMP signaling pathway and Calcium signaling pathway. Moreover, 7 hub genes located were significantly up-regulated or down-regulated. Conclusion: Genes related closely to bladder cancer were found through bioinformatical analysis, and 7 hub genes were obtained, therefore paved the way for the further researches.",
        "link": "https://dl.acm.org/doi/10.1145/3469678.3469692",
        "category": "Databases"
    },
    {
        "title": "In Silico Analysis Used to Estimate Active Compounds of Phthalazinones and the Potential Pharmacological Mechanism in Alzheimer's Disease Treatment",
        "authors": "['HUIMIN YE', 'HUAIYUAN QI', 'ZHIHAI HUANG', 'JINGSONG YAN', 'ZHONG LI']",
        "date": "September 2021",
        "source": "ICBRA '21: Proceedings of the 8th International Conference on Bioinformatics Research and Applications",
        "abstract": "Alzheimer's disease (AD) is prevalent among the aged and new drugs for AD are always in necessary. In this study, network pharmacology and molecular docking were adopted to bioactivity screening as well as pharmacological mechanism investigation, of phthalazinones. Those phthalazinones with certain structures and physical properties were screened out by PubChem database. And molecular docking was used to explore the potential activity of these phthalazinones as drugs for AD treatment. Secondly, network pharmacology analysis of phthalazinones and the drugs approved for AD were employed, including targets prediction, gene enrichment analysis and network analysis. Five compounds (PubChem CID: 137700766, 69619707,137700767, 137700768, 9827600) were recognized and enjoyed lower binding energy. There were 57 targets recognized as common targets that adopted to network construction. Totally 15 approved drugs with clear indications for AD were figured out, with 57 associated targets. The Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis showed that phthalazinone derivatives and drugs for AD shared the same essential pathway (neuroactive ligand-receptor interaction) with associated targets. In silico analysis suggested that these phthalazinones are potential to be effective for AD treatment by regulating neuroactive ligand-receptor interaction pathway, calcium signaling pathway, and other pathways.",
        "link": "https://dl.acm.org/doi/10.1145/3487027.3489043",
        "category": "Databases"
    },
    {
        "title": "Expression and Prognosis of HER Family in Breast Cancer",
        "authors": "['You Huang', 'Xuan Chen', 'Qifang Wu', 'Huimin Zhang', 'Jun Wang', 'Chao Shen', 'Xinghua Liao']",
        "date": "May 2021",
        "source": "ICBBT '21: Proceedings of the 2021 13th International Conference on Bioinformatics and Biomedical Technology",
        "abstract": "Human epidermal growth factor receptor (HER) family belongs to the transmembrane protein receptor family of the tyrosine kinase I subfamily and has been determined to be closely related to the clinicopathological characteristics of various tumors and the poor prognosis of tumor patients. However, the different expression patterns of the four HER family genes and their correlation with cancer immune infiltrate have not been analyzed. In this article, we analyzed the expression level of the HER family in breast cancer patients through Oncomine and Interactive Analysis of Gene Expression Profiles (GEPIA). We found that the expression levels of ERBB2 and ERBB3 in invasive breast cancer (BCRA) tissues were higher than those in normal breast tissues, while the expression levels of EGFR in the former were lower than those in the latter. Using the Tumor Immunity Estimation Resource (TIMER) database to study the correlation between the HER family and breast cancer immune infiltrates shows that the expression of the HER family is significantly associated with tumor purity and the level of infiltration of various immune cells. This study laid the foundation for further exploration of the molecular mechanism of the HER family in breast cancer.",
        "link": "https://dl.acm.org/doi/10.1145/3473258.3473270",
        "category": "Databases"
    },
    {
        "title": "Government data exchange platform and algorithm implementation in cloud computing environment",
        "authors": "['Fangyao Shi']",
        "date": "September 2021",
        "source": "ICISCAE 2021: 2021 4th International Conference on Information Systems and Computer Aided Education",
        "abstract": "In recent years, local governments at all levels in China have successively established a variety of e-government information systems in the process of performing their functions and providing services to the public, and accumulated a large amount of data resources. However, the data development platform, database environment and data format used in the construction of these information systems are all different, thus forming an information island. Therefore, this paper puts forward the design of Enterprise Service Bus(ESB) data exchange architecture based on cloud computing, and builds loosely coupled interconnection through ESB as a bridge. Even if a system changes, it will not affect the whole system. Improve the security and stability of web service technology through API gateway design and data monitoring analysis. According to the semi-structured feature of Geography Mark-up Language(GML) spatial data, a data model of GML spatial data storage in HBase is proposed. The filling algorithm of e-government big data system is constructed on the cloud computing platform, so as to parallelize the data, classify the data by decision tree, and realize the parallelization of incremental update decision forest, which can enhance the data processing ability of the data center.",
        "link": "https://dl.acm.org/doi/10.1145/3482632.3487439",
        "category": "Databases"
    },
    {
        "title": "Algorithm Research and Design of College Student Management Platform Based on Web Technology",
        "authors": "['Zhang Zhao']",
        "date": "July 2021",
        "source": "ICIIP '21: Proceedings of the 6th International Conference on Intelligent Information Processing",
        "abstract": "The traditional educational administration management work content is complicated and the workload is large; the continuous improvement of work efficiency and intelligence requirements has made the shortcomings of the traditional educational administration management system increasingly prominent. Therefore, an intelligent educational administration management system is designed to improve the quality of education and teaching. The efficiency of education, teaching and office work has become the top priority for the informationization of teaching management. Web technology is the core of the design of the college student management platform. This article determines the technical core of the development through the analysis of the supply and demand of the student management system, then conducts the feasibility analysis and system architecture design of the system, and completes the database design. In the design process, the component design idea is used to compile and debug the system code. Through testing, it is found that the system meets the needs of intelligence in the educational administration management system.",
        "link": "https://dl.acm.org/doi/10.1145/3480571.3480582",
        "category": "Databases"
    }
]